{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Adventure in Camera Calibration\n",
    "> Let's learn how to use a set of known 2D and 3D point correspondances to calibrate a camera\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Computer Vision,Optimization,Linear Algebra]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's Febrary 1972, the [A300](https://en.wikipedia.org/wiki/Airbus_A300) airliner is being unviled in Toulouse, let's go on an adventure (In camera calibration!).\n",
    "\n",
    "\n",
    "![An image of the Airbus A300 Final Assembly line in Toulouse](Images/2020-2-23-An-Adventure-In-Camera-Calibration/A300.jpg)\n",
    "\n",
    "Let's keep things interesting, and pretend that we work for an aircraft manufacturer, Norton Aircraft, headquartered in Burbank, California. Let's say we have seen this photo published in a magazine, and we want to try and learn as much about the dimensions of Airbus's new aircraft as possible. In order to do so, we will need to mathematically reconstruct the camera used to take the photo, as well as the scene itself.\n",
    "\n",
    "Now, In this case, we are lucky, because we notice the hexagonal pattern on the floor. In particular, we notice that it's a tessellating hexagonal pattern, which can only happen if all the hexagons have identical dimensions.\n",
    "\n",
    "While we don't know the dimensions of the hexagon, we guess that each side is approximately 1.6m long, based on the high of the people in the photo. If we assume some point on the ground, say the center of a polygon is the point 0,0, we can work out the X & Y location of each other polygon vertex we can see. Furthermore, we could also assume that the factory floor is flat and level. Hence the Z coordinate of each point is 0.\n",
    "\n",
    "Let's spend Â±5 minutes annotating the image, using an annotation tool like label me. I've generated a file, which you can find attached here: \n",
    "\n",
    "![An annotated image of the Airbus A300 Final Assembly line in Toulouse](Images/2020-2-23-An-Adventure-In-Camera-Calibration/Hexagons.jpg)\n",
    "\n",
    "\n",
    "Firstly, lets load in all of the x and y points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "JSON = json.loads(open('Data/2020-2-23-An-Adventure-In-Camera-Calibration/A300.json','r').read())\n",
    "polygons = {}\n",
    "for shape in JSON['shapes']:\n",
    "    coords = shape['label'].split(',')\n",
    "    x,y = int(coords[0]),int(coords[1])\n",
    "    polygons[x,y] = shape['points']    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now doing some maths, and work out the locations of each vertex of our hexagons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "points = []\n",
    "keys = sorted(polygons.keys())\n",
    "\n",
    "for key in keys:\n",
    "    poly = polygons[key]    \n",
    "    (pts_x, pts_y) = zip(*poly)\n",
    "    \n",
    "    pts_x = list(pts_x)\n",
    "    pts_y = list(pts_y)\n",
    "    \n",
    "    #Magic analytic formula for working out the location of each point, based on which vertex, of which polygon it is.\n",
    "    x_vertex = 0.5 * np.array([1,2,1,-1,-2,-1])\n",
    "    y_vertex = 0.5 * np.array([np.sqrt(3),0,-np.sqrt(3),-np.sqrt(3),0,np.sqrt(3)])\n",
    "    \n",
    "    row,col = key\n",
    "    x = row * 1.5 + x_vertex\n",
    "    y = col * 0.5 * np.sqrt(3) + y_vertex\n",
    "    \n",
    "    #From before, we assume the sides of each polygon is 1.6m\n",
    "    x*=1.6 #meters\n",
    "    y*=1.6 #meters\n",
    "    \n",
    "    for idx in range(0,6):\n",
    "        point = []\n",
    "        i = pts_x[idx]\n",
    "        j = pts_y[idx]\n",
    "        X = x[idx]\n",
    "        Y = y[idx]\n",
    "        Z = 0.0\n",
    "        points.append([i,j,X,Y,Z])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are presented with a minor problem, in many cases, we have annotated the same point up to 3 times, where the vertices of the hexagons meet. So let's go and find points that are within 10 pixels, and then take their average. If we don't do this, then we effectively over-weight some points in the image, at the expense of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "points = np.asarray(points)\n",
    "\n",
    "tree = KDTree(points[:,0:2], leaf_size=5)\n",
    "\n",
    "merged_indicies = []\n",
    "unique_points = []\n",
    "for i in range(0,points.shape[0]):\n",
    "    if i not in merged_indicies:\n",
    "        dist, ind = tree.query(points[i,0:2].reshape(-1, 2), k=3)\n",
    "        \n",
    "        indicies_to_merge = []\n",
    "        for j in range(0,3):\n",
    "            if dist[0][j]<10:\n",
    "                indicies_to_merge.append(ind[0][j]) \n",
    "                merged_indicies.append(ind[0][j])\n",
    "\n",
    "        mean_points = np.mean(points[indicies_to_merge,:],axis=0)\n",
    "        unique_points.append(mean_points)\n",
    "        \n",
    "unique_points = np.asarray(unique_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we have a bunch of 3D points, and corresponding 2D points in the photo.\n",
    "\n",
    "Now it's time to turn to the real magic, bundle adjustment. Basically, our task at hand, is to find a camera, which best fits the data we have measured. \n",
    "\n",
    "Let's talk more about cameras.\n",
    "\n",
    "> Important: There are many correct ways to model a camera mathematically. This is one way.\n",
    "\n",
    "\n",
    "Mathematically, cameras are are composed of two types of parameters, *Intrinsic* and *Extrinsic*.\n",
    "The *Extrinsic* parameters define the position and rotation of the camera, with respect to the origin of the points it's observing.\n",
    "\n",
    "The *Intrinsic* parameters define the parameters of the camera itself, for example the Focal length, the location of the camera's radial center, as well as distortion induced by the lens.\n",
    "\n",
    "\n",
    "The *Extrinisic* parameters are comprised of 6 degrees of freedom, given our world is 3 dimensional, and there are 3 dimensions which to rotate around. \n",
    "\n",
    "The *Intrinsic* parameters are more complex. There are a number of great resources, for example *Multiple View Geometry in Computer Vision*, or the OpenCV documentation. However, In this case, I am assuming that the principal point, the focal length, and the radial parameters are unknown.\n",
    "\n",
    "> Note: To be clear, I'm building on the shoulders of giants, I've heavily adapted this example from this incredible demo by *Nikolay Mayorov* which you can find (here)[https://scipy-cookbook.readthedocs.io/items/bundle_adjustment.html]\n",
    "\n",
    "\n",
    "Firstly, let's go and import a bunch of stuff we will need later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyMC3",
   "language": "python",
   "name": "pymc3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
