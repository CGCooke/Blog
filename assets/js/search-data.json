{
  
    
        "post0": {
            "title": "Vanishing point",
            "content": "Why are we interested in finding vanishing points? Well they are actually pretty interesting, and they are a key feature in projective geometry. However in practice, they are useful, because they allow us to quickly and easily estimate key parameters about a scene.  . Important: Parallel lines in 3D space, intersect at a vanashing point in 2D space. . from PIL import Image import matplotlib.pyplot as plt import json import numpy as np plt.rcParams[&#39;figure.figsize&#39;] = [15, 15] . To keep things nice and simple, let&#39;s use a rendered image from a video game (Counter Strike). This means that we can, for the moment ignore some other factors like Radial Distortion. . img = Image.open(&#39;data/10-04-2020-Vanishing-Point/csgo-dust2-mid.jpeg&#39;) plt.imshow(img) plt.show() . To keep things even easier, I&#39;ve used an external program to manual annotate the straight lines in the image, that correspond to one of 3 different vanishing points. . Often, we use a manhattan world assumption, that is assuming that there are 3 different sets of orthogonal parallel lines present in the world. . . Now, let&#39;s use linear algebra and least mean squares (and the magic of [Stack Overflow])(https://stackoverflow.com/questions/52088966/nearest-intersection-point-to-many-lines-in-python) to find one of the vanishing points. . JSON = json.loads(open(&#39;data/10-04-2020-Vanishing-Point/csgo-dust2-mid.json&#39;,&#39;r&#39;).read()) . def intersect_multiple_lines(P0,P1): &quot;&quot;&quot;P0 and P1 are NxD arrays defining N lines. D is the dimension of the space. This function returns the least squares intersection of the N lines from the system given by eq. 13 in http://cal.cs.illinois.edu/~johannes/research/LS_line_intersect.pdf. &quot;&quot;&quot; # generate all line direction vectors n = (P1-P0)/np.linalg.norm(P1-P0,axis=1)[:,np.newaxis] # normalized # generate the array of all projectors projs = np.eye(n.shape[1]) - n[:,:,np.newaxis]*n[:,np.newaxis] # I - n*n.T # see fig. 1 # generate R matrix and q vector R = projs.sum(axis=0) q = (projs @ P0[:,:,np.newaxis]).sum(axis=0) # solve the least squares problem for the # intersection point p: Rp = q p = np.linalg.lstsq(R,q,rcond=None)[0] return p . P0 = [] P1 = [] for shape in JSON[&#39;shapes&#39;]: points = shape[&#39;points&#39;] if shape[&#39;label&#39;] ==&#39;VP1&#39;: P0.append(points[0]) P1.append(points[1]) plt.plot([points[0][0],points[1][0]],[points[0][1],points[1][1]]) P0 = np.array(P0,dtype=np.float64) P1 = np.array(P1,dtype=np.float64) p = intersect_multiple_lines(P0,P1) plt.scatter(p[0],p[1],color=&#39;k&#39;,label=&#39;Vanishing point&#39;) plt.legend() plt.xlim(0,2560) plt.ylim(1600,0) plt.show() . Ok, so this is a good start, but I&#39;m interested in how certain we are about the vanishing point. . For example, when I annotated the lines, I most likely made mistakes in the precise location of each point. We would expect that for shorter lines, this would have a bigger impact on the location of the vanishing point than for longer lines. . Let&#39;s do a monte carlo simulation, to find the distribution of possible vanashing points. . More Stack overflow magic: . vanishing_points = [] for i in range(0,1000): P0_stochastic = P0 + 2*np.random.randn(P0.shape[0],P0.shape[1]) P1_stochastic = P1 + 2*np.random.randn(P1.shape[0],P1.shape[1]) p = intersect_multiple_lines(P0_stochastic,P1_stochastic) vanishing_points.append(p) for p in vanishing_points: plt.scatter(p[0],p[1],color=&#39;k&#39;,alpha=0.1) plt.xlim(1300,1450) plt.ylim(700,550) plt.grid() plt.show() . For completeness, we can compute the standard deviation of the points in both the x &amp; y axis. . vanishing_points = np.asarray(vanishing_points) print(vanishing_points.std(axis=0).ravel()) . [6.92612301 4.04771691] . Voila, we have a standard deviation of ±7 pixels in the x direction, and ±4 pixels in the y direction. .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/optimisation/linear%20algebra/monte%20carlo%20simulation/2020/04/10/Vanishing-Point.html",
            "relUrl": "/computer%20vision/optimisation/linear%20algebra/monte%20carlo%20simulation/2020/04/10/Vanishing-Point.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Vanishing point",
            "content": "Why are we interested in finding vanishing points? Well they are actually pretty interesting, and they are a key feature in projective geometry. However in practice, they are useful, because they allow us to quickly and easily estimate key parameters about a scene.  . Important: Parallel lines in 3D space, intersect at a vanashing point in 2D space. . from PIL import Image import matplotlib.pyplot as plt import json import numpy as np import scipy.linalg plt.rcParams[&#39;figure.figsize&#39;] = [15, 15] . To keep things nice and simple, let&#39;s use a rendered image from a video game (Counter Strike). This means that we can, for the moment ignore some other factors like Radial Distortion. . img = Image.open(&#39;data/10-04-2020-Vanishing-Point/csgo-dust2-mid.jpeg&#39;) plt.imshow(img) plt.show() . JSON = json.loads(open(&#39;data/10-04-2020-Vanishing-Point/csgo-dust2-mid.json&#39;,&#39;r&#39;).read()) . def intersect_multiple_lines(P0,P1): &quot;&quot;&quot;P0 and P1 are NxD arrays defining N lines. D is the dimension of the space. This function returns the least squares intersection of the N lines from the system given by eq. 13 in http://cal.cs.illinois.edu/~johannes/research/LS_line_intersect.pdf. &quot;&quot;&quot; # generate all line direction vectors n = (P1-P0)/np.linalg.norm(P1-P0,axis=1)[:,np.newaxis] # normalized # generate the array of all projectors projs = np.eye(n.shape[1]) - n[:,:,np.newaxis]*n[:,np.newaxis] # I - n*n.T # see fig. 1 # generate R matrix and q vector R = projs.sum(axis=0) q = (projs @ P0[:,:,np.newaxis]).sum(axis=0) # solve the least squares problem for the # intersection point p: Rp = q p = np.linalg.lstsq(R,q,rcond=None)[0] return p . def find_vanishing_point(point_name): P0 = [] P1 = [] for shape in JSON[&#39;shapes&#39;]: points = shape[&#39;points&#39;] if shape[&#39;label&#39;] == point_name: P0.append(points[0]) P1.append(points[1]) P0 = np.array(P0,dtype=np.float64) P1 = np.array(P1,dtype=np.float64) p = intersect_multiple_lines(P0,P1).ravel() return(p) vanishing_points = {} for point_name in [&#39;VP1&#39;,&#39;VP2&#39;,&#39;VP3&#39;]: vanishing_points[point_name]= find_vanishing_point(point_name) print(vanishing_points) . {&#39;VP1&#39;: array([1371.89171088, 630.42051773]), &#39;VP2&#39;: array([-10651.53961582, 536.68080631]), &#39;VP3&#39;: array([1272.22463298, 7683.01978252])} . plt.imshow(img) for point_name in [&#39;VP1&#39;,&#39;VP2&#39;,&#39;VP3&#39;]: vp = vanishing_points[point_name] plt.scatter(vp[0],vp[1],label=point_name) plt.legend() plt.show() . Ok, so we now have 3 different vanishing points. Using these, let&#39;s try and use them to gain some insights the cameras&#39;s relationship with the scene. . Hartley &amp; Zisserman put it best, &quot;Vanishing points are images of points at infinity, and provide orientation (attitude) in- formation in a similar manner to that provided by the fixed stars.&quot; . Hartley &amp; Zisserman also provide an algorithm (Example 8.27, page 226) to extract the camera calibration matrix K from 3 mutually orthogonal vanishing points. . Let&#39;s go and implement it in practice. . $v^T_i omega v_j = 0$ . $ omega = begin{bmatrix}w_1 &amp; 0 &amp; w_2 0 &amp; w_1 &amp; w_3 w_2 &amp; w_3 &amp; w_4 end{bmatrix}$ . $ v_j = begin{bmatrix} x_1 y_1 1 end{bmatrix}$ . $v^T_i = begin{bmatrix} x_2 &amp; y_2 &amp; 1 end{bmatrix}$ . $ omega v_j = begin{bmatrix} w_1 x_1 + w_2 w_1 y_1 + w_3 w_2 x_1 + w_3 y_1 + w_4 end{bmatrix}$ . $ v^T_i omega v_j = x_2(w_1 x_1 + w_2) + y_2(w_1 y_1 + w_3) + w_2 x_1 + w_3 y_1 + w_4$ . $ v^T_i omega v_j= w_1(x_2 x_1 + y_2 y_1) + w_2(x_2 + x_1) + w_3(y_2 + y_1) + w_4$ . def compute_K_from_vanishing_points(vanishing_points): A = [] for (point_name_1, point_name_2) in [(&#39;VP1&#39;,&#39;VP2&#39;),(&#39;VP2&#39;,&#39;VP3&#39;),(&#39;VP1&#39;,&#39;VP3&#39;)]: vp1 = vanishing_points[point_name_1] vp2 = vanishing_points[point_name_2] x1,y1 = vp1 x2,y2 = vp2 w1 = x2*x1 + y2*y1 w2 = x2 + x1 w3 = y2 + y1 w4 = 1 A.append([w1,w2,w3,w4]) A = np.array(A) w = scipy.linalg.null_space(A).ravel() w1 = w[0] w2 = w[1] w3 = w[2] w4 = w[3] omega = np.array([[w1,0,w2],[0,w1,w3],[w2,w3,w4]]) K = np.linalg.inv(np.linalg.cholesky(omega)).T K/=K[2,2] return(K) K = compute_K_from_vanishing_points(vanishing_points) . print(K) plt.scatter(2560/2.0,1600/2.0,color=&#39;G&#39;) plt.scatter(K[0,2],K[1,2],color=&#39;R&#39;) plt.imshow(img) plt.show() . [[ 7.27651440e+02 0.00000000e+00 1.32662075e+03] [-0.00000000e+00 7.27651440e+02 7.05955736e+02] [-0.00000000e+00 0.00000000e+00 1.00000000e+00]] .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/optimisation/linear%20algebra/monte%20carlo%20simulation/2020/04/10/K-From-Vanishing-Points.html",
            "relUrl": "/computer%20vision/optimisation/linear%20algebra/monte%20carlo%20simulation/2020/04/10/K-From-Vanishing-Points.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Inverse Radial Distortion",
            "content": "What is radial distortion? . Real cameras, with real optics suffer from radial distortion. Radial distortion is a common type of lens distortion). It&#39;s a deviation away from a perfect pinhole camera, where straight lines in 3D space are mapped to straight lines in 2D space. Let&#39;s define this distortion mathematically. . Defining a forward function . Radial distortion functions map points according to their distance (r) from the optical center of the lens. . We can model the function using a Taylor series: . begin{equation*} x_d = x(1 + k_1  r + k_2  r^2 + k_3 r^3) end{equation*} begin{equation*} y_d = y(1 + k_1  r + k_2 r^2 + k_3 r^3) end{equation*}IE, the function is described using 3 numbers, k_1, k_2 and k_3. . Firstly, let&#39;s import some things we need. . import numpy as np import matplotlib.pyplot as plt from scipy.optimize import least_squares %matplotlib inline plt.rcParams[&quot;figure.figsize&quot;] = (10,10) . Next let&#39;s use some distortion coefficients from a real lens, and plot how it maps points based on their radius from the optical center of the lens. . k_1 = -0.04436 k_2 = -0.35894 k_3 = 0.14944 r = np.linspace(0,1,1000) r_distorted = r*(1 + k_1 * r + k_2 * r**2 + k_3 * r**3) plt.xlabel(&#39;Initial R&#39;) plt.ylabel(&#39;Distorted R&#39;) plt.plot(r,r_distorted) plt.show() . To help understand this mapping, we can visualise the impact on a grid of straight lines. Note how lines are mapped to curves. . def distort_line(x,y,k_1,k_2,k_3): r = np.sqrt(x**2 + y**2) x_distorted = x*(1 + k_1 * r + k_2 * r**2 + k_3 * r**3) y_distorted = y*(1 + k_1 * r + k_2 * r**2 + k_3 * r**3) return(x_distorted,y_distorted) for y in np.linspace(-1,1,10): x = np.linspace(-1,1,1000) x_distorted,y_distorted = distort_line(x,y,k_1,k_2,k_3) plt.plot(x_distorted,y_distorted,color=&#39;k&#39;,alpha=0.8) for x in np.linspace(-1,1,10): y = np.linspace(-1,1,1000) x_distorted,y_distorted = distort_line(x,y,k_1,k_2,k_3) plt.plot(x_distorted,y_distorted,color=&#39;k&#39;,alpha=0.8) plt.xlim(-1,1) plt.ylim(-1,1) plt.show() . Finding an Inverse Function . Now it&#39;s time to find an inverse function, a function that will allow us to take points that have been distorted, and map them back to where they would have been, had the lens been free from distortion. . Unfortunately we can&#39;t algebraically find and an inverse function based on the forward function. However, we can find an inverse function through a process of optimization. . def undistort_point(undistortion_params,r_distorted): undistorted = r_distorted*(1 + undistortion_params[0] * r_distorted + undistortion_params[1] * r_distorted**2 + undistortion_params[2] * r_distorted**3 + undistortion_params[3] * r_distorted**4 + undistortion_params[4] * r_distorted**5) return(undistorted) def fun(undistortion_params,r_distorted): #Compute residuals. undistorted = undistort_point(undistortion_params, r_distorted) return((undistorted - np.linspace(0,1,1000))).ravel() . x0 = np.zeros(5).ravel() res = least_squares(fun, x0, verbose=2, ftol=1e-12,loss=&#39;linear&#39;, args=([r_distorted])) . Iteration Total nfev Cost Cost reduction Step norm Optimality 0 1 5.6524e+00 3.02e+01 1 2 2.3481e-07 5.65e+00 9.82e-01 1.88e-08 2 3 2.3481e-07 1.62e-17 1.58e-06 1.04e-11 `gtol` termination condition is satisfied. Function evaluations 3, initial cost 5.6524e+00, final cost 2.3481e-07, first-order optimality 1.04e-11. . Basically the optimisation process tries to find a set of coefficients that allow us to map the output from the distortion function back to it&#39;s input. . undistorted = undistort_point(res.x,r_distorted) plt.plot(r_distorted,label=&#39;distorted&#39;,alpha=0.5) plt.plot(undistorted,label=&#39;un distorted&#39;,alpha=0.5) plt.plot(np.linspace(0,1,1000),label=&#39;target&#39;,alpha=0.5) plt.legend() plt.show() . print(res.x) . [ 0.04599498 0.32120247 0.22196835 -0.46283148 0.77191211] . Voila, we have found a coefficients of a taylor series that allow us to invert the distortion function. .",
            "url": "https://cgcooke.github.io/Blog/optimisation/computer%20vision/2020/04/05/Inverse-Radial-Distortion.pynb.html",
            "relUrl": "/optimisation/computer%20vision/2020/04/05/Inverse-Radial-Distortion.pynb.html",
            "date": " • Apr 5, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Exodus",
            "content": "The context . There are something like 500,000 international students in Australia, and in the words of Australian Prime minister, Scott Morrison, it&#39;s time for them to go home. . Some of these students are fortunate, and have secured employment in roles where they can continue to work, despite the economic and social impacts of COV19. Many are less fortunate. . Unfortunately for many of these students, returning home is easier said than done. Globally, airlines are grounding their fleets and countries are closing their borders. . The numbers . While the numbers change on a day to day basis, as I write, the earliest flight from Australia to India is on Sunday the 12th of April, in 9 days time. It costs $1,939 USD and is an epic 58 hours, providing a regional geo-political tour, taking you from Melbourne New Delhi, via Doha, Islamabad and Riyadh. (For geopolitical reasons, you can&#39;t fly from Doha to Riyadh, or from Islamabad to New Delhi). . While I&#39;m not certain of what the future holds, let&#39;s imagine a possible future, and consider what an exodus of Indian students would look like. Let&#39;s start with the numbers, there are 153,537 Indians on (X) type visas. . Let&#39;s think at a high level for how we can place these numbers in context. 372,000 Indians Indians visited Australia circa 2019. Let&#39;s assume that roughly the same number of Australians visit India. From this we can assume that there are approximately 700,000 seat&#39;s p/a available to/from India, or 350,000 seats available in each direction. This estimate isn&#39;t that far from the truth, with total bi-directional capacity of 676,000. . From this, we can see that under normal operating conditions, assuming that there were no other passengers, it would take ±3.5 months to completely evacuate 100,000 people from Australia to India using the available commercial capacity. . The airlift . Realistically, a government backed airlift would be required, in order for a significant number of citizens to be repatriated, in a reasonable amount of time. . Surprisingly, and exodus of this scale isn&#39;t without precedent in recent history. Take for example the airlift of Indians from Kuait, where 111,711 people were airlifted on 488 flights over 63 days from Amman, Jordan to Mumbai. It&#39;s important to note that this was over a distance of ±4,000km, as opposed to the ±10,000km between Sydney and Mumbai. Assuming that the aircraft utilised can spend a significant proportion of their time airborne, then this significantly decreases the rate at which the airlift can take place. . At a first approximation, it&#39;s clear that any meaningful airlift would involve hundreds of new flights, over a period of weeks or more likely months. In practice, this would realistically involve using the Indian Goveerment owned Air India. . Let&#39;s take a quick look at the fleet of aircraft that could be employed . Aircraft Type Number in service Available Seats . B787-8 | 27 | 256 | . B777-300ER | 15 | 342 | . B777-200LR | 3 | 238 | . B747-400 | 4 | 423 | . All up, we have 49 aircraft, with a weighted average of 296 seats. From this, we can make some high level assumptions. MIT found that wide body aircraft typically operated 12 hours (&quot;Block hours&quot;) a day on average. . Based on an average cruising speed of 500 Knots (925 Km/h), and a distance of 5,600 NM (10,400 km) between Sydney and New Delhi, we can approximate a flight time of 11 hours. From this, we can assume each return trip takes 2 days. . From this we find the total daily capacity, which is 7,256 seats. . begin{equation*} Total Capacity = {Average Seats times Fleet Size times Flights Per Day} end{equation*}In turn, it would take approximately 2 weeks to evacuate 100,000 people. In practice, an evacuation on this scale would be infeasible at this rate, and would likely take far longer.   . . We can also provide a first order approximation of the cost of the evacuation. For FY19, Air India recorded as Cost Per Available Seat Kilometre (CASK) of 5 Rupees ($0.065 USD). . From this, we can multiply the CASK by the total number of seats and the total number of seats, to find the total cost. . begin{equation*} Cost =  CASK times Seats times 2 times Distance end{equation*}Or a cool $229M AUD. .",
            "url": "https://cgcooke.github.io/Blog/aerospace/2020/04/03/An-Exodus.html",
            "relUrl": "/aerospace/2020/04/03/An-Exodus.html",
            "date": " • Apr 3, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "RQ Decomposition In Practice",
            "content": "Let&#39;s keep things short and sweet. . Given a camera projection matrix, $P$, we can decompose it into a $K$ (Camera Matrix), $R$ (Rotation Matrix) and $C$ (Camera Centroid) matrix. . IE, given we have $P = K[R|-RC]$, We want to find $K$, $R$ and $C$. . The method is simply described in Multiple View Geometry in Computer Vision (Second Edition), on page 163, however let&#39;s turn it into a practical Python implementation. . Let&#39;s follow along with an example from the book. . import numpy as np import scipy.linalg np.set_printoptions(precision=2) . P = np.array([[3.53553e+2, 3.39645e+2, 2.77744e+2, -1.44946e+6], [-1.03528e+2, 2.33212e+1, 4.59607e+2, -6.3252e+5], [7.07107e-1, -3.53553e-1, 6.12372e-1, -9.18559e+2]]) . So, we have: $P = [M | −MC]$ . M can be decomposed as $M=KR$ using the RQ decomposition. . M = P[0:3,0:3] K, R = linalg.rq(M) . So far, so good. . Now things get a little more complex. . We want to find a Camera matrix with a positive diagonal, giving positive focal lengths. . However, in case this doesn&#39;t happen, we can adjust the sign of the column of each column of both the $K$ and $R$ matrix, to &quot;Make it so&quot;. . T = np.diag(np.sign(np.diag(K))) if linalg.det(T) &lt; 0:     T[1,1] *= -1 K = np.dot(K,T) R = np.dot(T,R) .     Finally, we can find the Camera Center ($C$). . We have $P_4$, the 4th column of $P$. . $P_4 = −MC$ . From this, we can find $C = {-M}^{-1} P_4$ . def factorize(P): M = P[:,0:3] K,R = scipy.linalg.rq(M) T = np.diag(np.sign(np.diag(K))) if scipy.linalg.det(T) &lt; 0: T[1,1] *= -1 K = np.dot(K,T) R = np.dot(T,R) C = np.dot(scipy.linalg.inv(-M),P[:,3]) return(K,R,C) K,R,C = factorize(P) print(&#39;K&#39;) print(K) print(&#39;R&#39;) print(R) print(&#39;C&#39;) print(C) . K [[468.16 91.23 300. ] [ 0. 427.2 200. ] [ 0. 0. 1. ]] R [[ 0.41 0.91 0.05] [-0.57 0.22 0.79] [ 0.71 -0.35 0.61]] C [1000.01 2000. 1499.99] . Voila! . This presentation is a great read, and provides a good overview of the RQ and QR decompositions. .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/linear%20algebra/2020/03/13/RQ-Decomposition-In-Practice.html",
            "relUrl": "/computer%20vision/linear%20algebra/2020/03/13/RQ-Decomposition-In-Practice.html",
            "date": " • Mar 13, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Adventure in Camera Calibration",
            "content": "It&#39;s Febrary 1972, the A300 airliner is being unviled in Toulouse, let&#39;s go on an adventure (In camera calibration!). . . Let&#39;s keep things interesting, and pretend that we work for an aircraft manufacturer, Norton Aircraft, headquartered in Burbank, California. Let&#39;s say we have seen this photo published in a magazine, and we want to try and learn as much about the dimensions of Airbus&#39;s new aircraft as possible. In order to do so, we will need to mathematically reconstruct the camera used to take the photo, as well as the scene itself. . Now, In this case, we are lucky, because we notice the hexagonal pattern on the floor. In particular, we notice that it&#39;s a tessellating hexagonal pattern, which can only happen if all the hexagons have identical dimensions. . While we don&#39;t know the dimensions of the hexagon, we guess that each side is approximately 1.6m long, based on the high of the people in the photo. If we assume some point on the ground, say the center of a polygon is the point 0,0, we can work out the X &amp; Y location of each other polygon vertex we can see. Furthermore, we could also assume that the factory floor is flat and level. Hence the Z coordinate of each point is 0. . Let&#39;s spend ±5 minutes annotating the image, using an annotation tool like label me. I&#39;ve generated a file, which you can find attached here: . . Firstly, lets load in all of the x and y points: . import json import numpy as np JSON = json.loads(open(&#39;data/2020-02-23-An-Adventure-In-Camera-Calibration/A300.json&#39;,&#39;r&#39;).read()) polygons = {} for shape in JSON[&#39;shapes&#39;]: coords = shape[&#39;label&#39;].split(&#39;,&#39;) x,y = int(coords[0]),int(coords[1]) polygons[x,y] = shape[&#39;points&#39;] . Ok, now doing some maths, and work out the locations of each vertex of our hexagons. . from sklearn.neighbors import KDTree points = [] keys = sorted(polygons.keys()) for key in keys: poly = polygons[key] (pts_x, pts_y) = zip(*poly) pts_x = list(pts_x) pts_y = list(pts_y) #Magic analytic formula for working out the location of each point, based on which vertex, of which polygon it is. x_vertex = 0.5 * np.array([1,2,1,-1,-2,-1]) y_vertex = 0.5 * np.array([np.sqrt(3),0,-np.sqrt(3),-np.sqrt(3),0,np.sqrt(3)]) row,col = key x = row * 1.5 + x_vertex y = col * 0.5 * np.sqrt(3) + y_vertex #From before, we assume the sides of each polygon is 1.6m x*=1.6 #meters y*=1.6 #meters for idx in range(0,6): point = [] i = pts_x[idx] j = pts_y[idx] X = x[idx] Y = y[idx] Z = 0.0 points.append([i,j,X,Y,Z]) . Now we are presented with a minor problem, in many cases, we have annotated the same point up to 3 times, where the vertices of the hexagons meet. So let&#39;s go and find points that are within 10 pixels, and then take their average. If we don&#39;t do this, then we effectively over-weight some points in the image, at the expense of others. . points = np.asarray(points) np.savetxt(&quot;data/2020-02-23-An-Adventure-In-Camera-Calibration/points.csv&quot;, points) tree = KDTree(points[:,0:2], leaf_size=5) merged_indicies = [] unique_points = [] for i in range(0,points.shape[0]): if i not in merged_indicies: dist, ind = tree.query(points[i,0:2].reshape(-1, 2), k=3) indicies_to_merge = [] for j in range(0,3): if dist[0][j]&lt;10: indicies_to_merge.append(ind[0][j]) merged_indicies.append(ind[0][j]) mean_points = np.mean(points[indicies_to_merge,:],axis=0) unique_points.append(mean_points) unique_points = np.asarray(unique_points) . So, now we have a bunch of 3D points, and corresponding 2D points in the photo. . Now it&#39;s time to turn to the real magic, bundle adjustment. Basically, our task at hand, is to find a camera, which best fits the data we have measured. . Let&#39;s talk more about cameras. . Important: There are many correct ways to model a camera mathematically. This is one way. . Mathematically, cameras are are composed of two types of parameters, Intrinsic and Extrinsic. The Extrinsic parameters define the position and rotation of the camera, with respect to the origin of the points it&#39;s observing. . The Intrinsic parameters define the parameters of the camera itself, for example the Focal length, the location of the camera&#39;s radial center, as well as distortion induced by the lens. . The Extrinisic parameters are comprised of 6 degrees of freedom, given our world is 3 dimensional, and there are 3 dimensions which to rotate around. . The Intrinsic parameters are more complex. There are a number of great resources, for example Multiple View Geometry in Computer Vision, or the OpenCV documentation. However, In this case, I am assuming that the principal point, the focal length, and the radial parameters are unknown. . Note: To be clear, I&#8217;m building on the shoulders of giants, I&#8217;ve heavily adapted this example from this incredible demo by Nikolay Mayorov which you can find here . Firstly, let&#39;s go and import a bunch of stuff we will need later. . from __future__ import print_function . import numpy as np import matplotlib.pyplot as plt from scipy.optimize import least_squares from scipy.spatial.transform import Rotation as Rot %matplotlib inline plt.rcParams[&quot;figure.figsize&quot;] = (20,20) . points_2d = unique_points[:,0:2] points_3d = unique_points[:,2:5] print(&#39;We have {} unique points&#39;.format(points_2d.shape[0])) . We have 51 unique points . Let&#39;s start by providing some hints to the optimiser about what the solution could be like, by putting in some reasonable starting conditions. . We know both the image width and height, and we can assume that the principal point is in the center of the image. . I think the cameras is about 10 meters off the ground. . To make the task of optimization easier, lets rotate the camera so that it&#39;s facing directly down. This means that the points should be in front of/below it. . Let&#39;s also start off by assuming that the camera is centered above the points. It&#39;s obviously not correct, based on what we can see in the image, but it&#39;s not horrifically wrong. . image_width = 2251 image_height = 1508 estimated_focal_length_px = 2000 camera_params = np.zeros(12) r = Rot.from_euler(&#39;x&#39;, 180, degrees=True).as_rotvec() #Rotation matrix camera_params[0] = r[0] camera_params[1] = r[1] camera_params[2] = r[2] #C camera_params[3] = points_3d[:,0].mean() camera_params[4] = points_3d[:,1].mean() camera_params[5] = 10 #f,k1,k2, camera_params[6] = estimated_focal_length_px camera_params[7] = 0 camera_params[8] = 0 camera_params[9] = 0 #c_x,c_y camera_params[10] = image_width/2.0 camera_params[11] = image_height/2.0 . Now we come to the real magic. This function models the camera, taking points in 3D space, and converting them into points in 2D space. . There are lots of things going on here. . . Firstly, let&#39;s talk about the camera&#39;s intrinsic matrix. . Basically, it converts points from 3D space to 2D space. . begin{equation*} K = begin{bmatrix} f &amp; 0 &amp; c_{x} 0 &amp; f &amp; c_{y} 0 &amp; 0 &amp; 1 end{bmatrix} end{equation*}We have the focal length, $f$, and the camera optical center $c_x$ and $c_y$. . Now let&#39;s talk about the camera&#39;s extrinsic matrix. . These are the 6 degrees of freedom that describe it&#39;s position and orientation within the world. That&#39;s 3 degrees for the position, and 3 for the orientation. At its heart, what we are doing is simple, but confusing. . There are so many ways to represent our setup: . Coordinate systems: 2D and 3D. Left Handed or Right Handed?     | . | Rotations: . Quaternions? | Proper Euler angles (6 different ways)? | Tait–Bryan angles (6 different ways)? | Rodrigues rotation formula? | A rotation matrix? | . | The location of the camera in to the world. (2 Different ways). . | Today we are going to use two different ways to represent the rotations, Firstly a Rodrigues rotation vector representation, and a rotation matrix. The reason why we use two different representations is because it&#39;s easier to optimise when we have 3 degrees of freedom, rather than a naive rotation matrix which uses 9 numbers to represent 3 degrees of freedom. . R represents the orientation of the camera in the World Coordinate Frame (The frame which we use to describe our 3D points). . In python, we can use convert from the Rodrigues rotation vector to the Rotation matrix as follows: . from scipy.spatial.transform import Rotation as Rot rotation_vector = camera_params[:3] R = Rot.from_rotvec(rotation_vector).as_matrix() . begin{equation*} R = begin{bmatrix} R_1 &amp; R_2 &amp; R_3 R_4 &amp; R_5 &amp; R_6 R_7 &amp; R_8 &amp; R_9 end{bmatrix} end{equation*} Now, let&#39;s talk about the Project Matrix $P$ of the camera. This takes the points all the way from their location in 3D world coordinates, to pixel coordinates, assuming we have a camera without radial distortion. There are two main ways this could be formulated. . Firstly: begin{equation*} P = KR[I|−C] end{equation*} . Secondly: begin{equation*} P = K[R | t] end{equation*} . Where $t$ is: begin{equation*} t = −RC end{equation*} . Let&#39;s go with the first method, where C is : . begin{equation*} C = begin{bmatrix} -C_X -C_Y -C_Z end{bmatrix} end{equation*} However, there is one subtlety alluded to before, which is the impact of radial distortion. Simply, the camera&#39;s lens distorts the rays of light coming in, in a non-linear way. . We can model it using a Taylor series: . begin{equation*} x_c = x(1 + k_1 r + k_2 r^2 + k_3 r^3) end{equation*} begin{equation*} y_c = y(1 + k_1 r + k_2 r^2 + k_3 r^3) end{equation*} In python, we end up with: . r = np.sqrt(np.sum(points_proj**2, axis=1))) r = 1 + k1 times r**2 + k2 * r**4 + k3 * r**6 points_proj *= r[:, np.newaxis] . def project(points, camera_params): &quot;&quot;&quot;Convert 3-D points to 2-D by projecting onto images.&quot;&quot;&quot; #Rotation rotation_vector = camera_params[:3] R = Rot.from_rotvec(rotation_vector).as_matrix() #Camera Center C = camera_params[3:6].reshape(3,1) IC = np.hstack([np.eye(3),-C]) RIC = np.matmul(R,IC) #Make points Homogeneous points = np.hstack([points,np.ones((points.shape[0],1))]) #Perform Rotation and Translation #(n,k), (k,m) -&gt; (n,m) points_proj = np.matmul(points,RIC.T) #perspective divide points_proj = points_proj[:, :2] / points_proj[:, 2, np.newaxis] f = camera_params[6] k1 = camera_params[7] k2 = camera_params[8] k3 = camera_params[9] c_x = camera_params[10] c_y = camera_params[11] #Radial distortion r = np.sqrt(np.sum(points_proj**2, axis=1)) x = points_proj[:,0] y = points_proj[:,1] points_proj[:,0] = (1 + k1 * r + k2 * r**2 + k3 * r**3)*x points_proj[:,1] = (1 + k1 * r + k2 * r**2 + k3 * r**3)*y #Make points Homogeneous points_proj = np.hstack([points_proj, np.ones((points_proj.shape[0],1))]) K = np.asarray([[f, 0, c_x], [0, f, c_y], [0, 0, 1.0]]) points_proj = np.dot(points_proj,K.T) points_proj = points_proj[:,:2] return(points_proj) . This section below is really well explained by here. Basically, we are optimizing to minimise a geometric error. It&#39;s the distance between the 2D points we see, and the projection of their 3D counterparts. . Through a process of optimization, we aim to find parameters which result in low error, which means in turn they should represent the real parameters of the camera. . def fun(camera_params, points_2d, points_3d): #Compute residuals. points_proj = project(points_3d, camera_params) return(points_proj - points_2d).ravel() . x0 = camera_params.ravel() optimization_results = least_squares(fun, x0, verbose=1, x_scale=&#39;jac&#39;, ftol=1e-4, method=&#39;lm&#39;, loss=&#39;linear&#39;,args=(points_2d, points_3d)) . `ftol` termination condition is satisfied. Function evaluations 955, initial cost 3.7406e+07, final cost 1.7393e+02, first-order optimality 1.89e+03. . Now let&#39;s go and check out the results of our optimization process. . camera_params = optimization_results.x R_Rodrigues = camera_params[0:3] C = camera_params[3:6] r = Rot.from_rotvec(R_Rodrigues) R_matrix = r.as_matrix() r = Rot.from_matrix(R_matrix.T) R_Quaternion = r.as_quat() print(&#39;Quaternions: X: {:.3f} Y: {:.3f} Z: {:.3f} W: {:.3f} &#39;.format(R_Quaternion[0],R_Quaternion[1],R_Quaternion[2],R_Quaternion[3])) print(&#39;Camera position relative to the origin in (M): X: {:.2f}, Y: {:.2f}, Z: {:.2f}&#39;.format(C[0],C[1],C[2])) focal_length_px = camera_params[6] k1 = camera_params[7] k2 = camera_params[8] k3 = camera_params[9] c_x = camera_params[10] c_y = camera_params[11] print(&#39;Focal length (Pixels): {:.2f}&#39;.format(focal_length_px)) print(&#39;CX, CY: {:.2f} {:.2f}&#39;.format(c_x,c_y)) print(&#39;K_1, K_2, K_3 : {:.6f}, {:.6f}, {:.6f}&#39;.format(k1,k2,k3)) print(&#39;Mean error per point: {:.2f} pixels &#39;.format(optimization_results.cost/points_2d.shape[0])) . Quaternions: X: 0.894 Y: -0.408 Z: 0.083 W: -0.164 Camera position relative to the origin in (M): X: -6.84, Y: -12.92, Z: 2.73 Focal length (Pixels): 1023.46 CX, CY: 1038.42 2666.56 K_1, K_2, K_3 : -0.351113, 0.185768, -0.032289 Mean error per point: 3.41 pixels . Ok, first things first, the mean error per point is approximately 3.6 pixels, which is not great, not terrible. It&#39;s clear that we have found a decent solution, However there are some interesting things going on. . In particular, the principal point lies outside the image, which is curious to say the least. One possibility is that the image was cropped. . Now let&#39;s have a quick look at the errors for each point. . plt.hist(abs(optimization_results.fun),density=True) plt.title(&#39;Histogram of Residuals&#39;) plt.xlabel(&#39;Absolute Residual (Pixels)&#39;) plt.grid() plt.show() . So the histogram looks pretty good, apart from the one point with a high residual, which is probably due to sloppy labeling/annotation. . Now let&#39;s compare the points we annotated, with where they would be projected, using the camera parameters we found: . points_2d_proj = project(points_3d, optimization_results.x) img = plt.imread(&#39;data/2020-02-23-An-Adventure-In-Camera-Calibration/A300.jpg&#39;) plt.imshow(img) plt.scatter(points_2d[:,0],points_2d[:,1],label=&#39;Actual&#39;,c=&#39;r&#39;,alpha=0.5) plt.scatter(points_2d_proj[:,0],points_2d[:,1],label=&#39;Optimised&#39;,c=&#39;k&#39;,alpha=0.5) plt.show() . Again, this looks great. Finally, let&#39;s overlay the hexagons on the floor, to visually build confidence in our solution. . def plot_verticies(row,col): x_vertex = 0.5 * np.array([1,2,1,-1,-2,-1,1]) y_vertex = 0.5 * np.array([np.sqrt(3),0,-np.sqrt(3),-np.sqrt(3),0,np.sqrt(3),np.sqrt(3)]) x = row * 1.5 + x_vertex y = col * 0.5 * np.sqrt(3) + y_vertex x*=1.6 y*=1.6 points_3d = np.vstack([x,y,np.zeros(7)]).T points_2d_proj = project(points_3d, optimization_results.x) return(points_2d_proj) plt.imshow(img) for row in range(0,10,2): for col in range(0,10,2): points_2d_proj = plot_verticies(row,col) plt.plot(points_2d_proj[:,0],points_2d_proj[:,1],color=&#39;B&#39;,alpha=0.25) plt.text(np.mean(points_2d_proj[:,0]), np.mean(points_2d_proj[:,1]), str(row)+&#39;,&#39;+str(col), horizontalalignment=&#39;center&#39;,verticalalignment=&#39;center&#39;) for row in range(1,11,2): for col in range(1,11,2): points_2d_proj = plot_verticies(row,col) plt.plot(points_2d_proj[:,0],points_2d_proj[:,1],color=&#39;R&#39;,alpha=0.25) plt.text(np.mean(points_2d_proj[:,0]), np.mean(points_2d_proj[:,1]), str(row)+&#39;,&#39;+str(col), horizontalalignment=&#39;center&#39;,verticalalignment=&#39;center&#39;) plt.show() . /Users/cooke_c/.local/lib/python3.7/site-packages/IPython/core/pylabtools.py:128: MatplotlibDeprecationWarning: Support for uppercase single-letter colors is deprecated since Matplotlib 3.1 and will be removed in 3.3; please use lowercase instead. fig.canvas.print_figure(bytes_io, **kw) . Awesome, we can see visually that we have found a reasonable solution . Thanks for reading all the way to the end, . In the future, Let&#39;s look more about how we can extract useful information from this image, and understand how confident we can be in our solution. . Thanks to Nikolay Mayorov who created the awesome demo of optimization in Scipy that I built upon, please find the original code here. . Multiple View Geometry in Computer Vision is an incredible book, that I learn more from, each time I read it. in particular, for further information see: . Finite cameras. Page 153, Multiple View Geometry in Computer Vision (Second edition) | Note: Minimizing geometric error. Page 176, Multiple View Geometry in Computer Vision (Second edition) | .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/optimisation/linear%20algebra/2020/02/23/An-Adventure-In-Camera-Calibration.html",
            "relUrl": "/computer%20vision/optimisation/linear%20algebra/2020/02/23/An-Adventure-In-Camera-Calibration.html",
            "date": " • Feb 23, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Three Interesting Papers",
            "content": "Over the past couple of months, 3 incredibly exciting papers have come out, and I want to take the opportunity to share them with you. . The papers, in no particular order are MixMatch, Selfie and Unsupervised Data Augmentation, however let’s first discuss why they are exciting. . In my day to day work, I’m faced with an avalanche of data. Raw data may be cheap, but labelled data is precious, often relying on expensive experiments or busy experts. Even then, when labelled data is available there is an omnipresent, insatiable demand to do more with it. . Semi-supervised learning offers the opportunity to leverage the raw, unlabelled data to improve our models, reducing the barriers to building a model, and democratising AI. . I’m not going to discuss how the papers are actually implemented in detail, but I will say that the papers are very promising, and I hope they will be rapidly implemented and adapted as a standard part of deep learning workflows. . In a sentence, MixMatch uses MixUp, and label sharpening (Fancy way of saying “Artificially boosting your models confidence”) in order to effectively propagate labels. My first impression was “I can’t believe that works”, but then I saw that it decreases error rates 4x with when training with small numbers of samples on CIFAR-10. . Conversely, Selfie is inspired pertaining in BERT, and extends it to CNN’s. At a high level, the pre training task is analagous to removing pieces from a jigsaw puzzle, and asking “what piece should go in each hole?”. Given the power of transfer learning, this is hugely exciting for many problems where the data you want to train on is very different to what is found in ImageNet. . Finally, there is Unsupervised Data Augmentation (UDA), which prosecutes the thesis that “better data augmentation can lead to significantly better semi-supervised learning”. As with Selfie and MixMatch, the techniques used in this paper can be applied to image data. . Deep learning is built on a history of rapidly evolving best practice, including Xavier initialisation, Data Augmentation, One Cycle Policy and MixUp. I hope that adoptions of that MixMatch, Selfie and UDA will soon join this grab bag of best practice. .",
            "url": "https://cgcooke.github.io/Blog/deep%20learning/computer%20vision/2019/06/05/Three-Interesting-Papers.html",
            "relUrl": "/deep%20learning/computer%20vision/2019/06/05/Three-Interesting-Papers.html",
            "date": " • Jun 5, 2019"
        }
        
    
  
    
        ,"post7": {
            "title": "The Differentiable Airliner",
            "content": "It’s hard to conceive of something more carefully designed than an aircraft. Millions of parts, Hundreds of kilometers of wiring, all designed with by thousands of engineers. . What I find particularly fascinating however, is that the core design principles of an aircraft can be elicited through a relatively simple thought experiment, . At a steady state in cruise, we can make two assumptions. Firstly, the lift (L) generated by wings equals the weight (W) of the aircraft, secondly the thrust (T) generated by the engines equals the drag (D). Obviously these two cases are true, otherwise the aircraft would be accelerating either vertically or horizontally. . Thrust is generated by burning fuel in turbofan engine, and minimising fuel consumption is a key design goal. Reducing fuel consumption directly reduces the direct operating cost of an aircraft, while simultaneously increasing it’s potential range. . L=WL=WL=W . T=DT=DT=D . From this, we can also derive: . LD=WT frac{L}{D} = frac{W}{T}DL​=TW​ . T=WDLT = W frac{D}{L}T=WLD​ . So, from this we can see that we can reduce the thrust required, by . Reducing the aircraft weight. . | Reducing the drag. . | Improving the lift to drag ratio. . | Lets look at each of these aspects in a little more detail, . Reducing the aircraft weight . One of the current trends is the increasing use of composite materials in the design of aircraft. The higher the strength to weight ratio, allowing the same structural outcomes to be achieved, at a lower weight. . From this, we can see that fuel consumption is intricately linked to both the percentage of the aircraft which can be constructed from composite materials, as well as their strength. . One other important consideration is mission optimisation. An aircraft which is capable of long range flight needs to be engineered to structurally support the weight of the fuel required for the flight. If the aircraft operates a shorter route, then additional structure, and it’s associated weight increases fuel consumption, compared to an aircraft designed for shorter routes. . Reducing the drag . Fundamentally there are two sources of drag, Parasitic drag, and Lift induced drag. . Parasitic drag is a byproduct of the motion of the aircraft through the air, . Conversely, Lift induced drag is a byproduct of generating lift using the wings of the aircraft. . Together they sum to form total drag: . DTotal=DParasitic+DLiftInducedD_{Total} = D_{Parasitic } + D_{Lift Induced}DTotal​=DParasitic​+DLiftInduced​ . Minimising the Parasitic drag in turn reduces the total drag, minimising fuel burn. . Improving the lift to drag ratio. . Finally, maximising the lift to drag ratio, effectively means that aircraft wing can generate the required amount of lift, while minimising drag. . To provide a practical worked example, . Taking the case of an A330, weighing 200,000 kg, with an L/D ratio of approximately 18, then: . DLiftInduced=2×106×9.8118D_{Lift Induced} = frac{2 times10^{6} times 9.81}{18}DLiftInduced​=182×106×9.81​ . If the L/D ratio could be doubled, then the thrust, and fuel burn required to support the weight of the aircraft would halve. . Unconstrained Optimisation: The Virgin Atlantic GlobalFlyer . Optimising purely for minimum thrust, and thus fuel consumption, we end up with an aircraft with a fuselage made entirely out of composite materials, with minimal drag, and long slender wings, in order to maximise the L/D ratio. This is the design used by the Virgin Atlantic GlobalFlyer, with a L/D ratio of 37, which holds the FAI record for longest flight by an aircraft ever at 41,466 km. . Obviously the GlobalFlyer represents optimisation towards a singular goal, without consideration for other goals, like passenger comfort, however that doesn’t stop me loving it for it’s pure design. . .",
            "url": "https://cgcooke.github.io/Blog/aerospace/2019/03/30/The-Differentiable-Airliner.html",
            "relUrl": "/aerospace/2019/03/30/The-Differentiable-Airliner.html",
            "date": " • Mar 30, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "Kalman Filters",
            "content": "Kalman Filters are magic. While they take 5 minutes to explain at a basic level, you can work with them for a career and always be learning more. I think there is something philosophically satisfying about the way that they innately combine what we already believe and what we perceive in order to come to a new belief about the world. . While this sounds somewhat abstract, Kalman Filters provide a concrete mathematical formulation for fusing data from different sources, as well as physical models to provide (potentially) optimum estimates of the state of a system. . For less philosophy, and more maths, I strongly recommend stopping at this point and giving this incredible post a read. Afterwards, let’s talk about Kalman filters in a concrete way, . Creating a Kalman Filter in 7 easy steps: . One of the challenges with Kalman filters is that it’s easy to be initially overwhelmed by the mathematical background, and loose sight of what their implementation looks like in practice. In reality, it’s possible to break the implementation down into a series of discrete steps, which come together to fully describe the filter. . FilterPy is a fantastic Python library for creating Kalman filters, and has an accompanying book, which takes a deep dive into the mathematical theory of Kalman filters. Lets initially discuss the general process for defining a Kalman filter, before applying it to practical application. . x : Our filter state estimate, IE what we want to estimate. If we want to track an object moving an a video, this could be it’s pixel coordinates, as well as it’s velocity in pixels per second. [x,y,vx,vy] . | P : The covariance matrix. Encodes how certain the filter is about it’s estimates, evolves over time. In the object tracking example, how “confident” the filter is about the position of an object and it’s velocity. As the filter receives more measurements, the values in the covariance matrix are “washed out”, and so the the filter tends to be insensitive to the values used. . | Q : The process uncertainty. How large is the error associated with the system doing something unexpected between measurements? I find this the hardest to set, as it requires careful thought about the process. For example, if we are tracking the position and velocity of an object once a second, we would have more uncertainty if we were tracking the position of a fruit fly than an oil tanker. . | R : How uncertain each of our measurements are. This can be determined either through reading sensor datasheets or educated guesses. . | H : How to each measurement is related to the internal state of our system, in addition to scaling measurements. IE, If we have a GPS receiver, it tells us about our position, while an accelerometer tells us about our position. . | F : The state transition matrix. How the system evolves over time. IE, if we know the position and velocity of an object, then in the absence of any error or external influence we can predict it’s next position from it’s current position and velocity. . | B : The control matrix. This matrix allows us to tell the filter about how we expect any inputs we provide the system (u) to update the state of the system. In many cases, especially when we are taking measurements of a system we don’t control, the control matrix is not required. . | At this point, I think it’s worthwhile considering how all of these matrices are related to each other. Tim Babb of Bzarg, has a fantastic diagram, which sets out how information flows through all of the filters mentioned above. If you haven’t already, I strongly recommend you read his post on how Kalman filters work . Looking at the relationships between all of the matrices, . x and P are outputs of the filter, they tell what the filter believes the state of the system to be. . | H, F and B are matrices which control how the filter operates. . | Q, R are closely related, because they both denote uncertainty, in the process as well as the measurements. . | z and u denote inputs to the filter, in the case where we don’t control the system, then u is not required. . | A real world example: . Let’s look at a real world example. In computer vision, object tracking is the process of associating different detections of an object from different images/frames into a single “track”. Many algorithms have been developed for this task (Simple Online and Realtime Tracking)[https://arxiv.org/abs/1602.00763] is particularly elegant. In summary, SORT creates a Kalman filter for each object it wants to track, and then predicts the location and size of each object, in each frame using the filter. . Alex Bewley, one of the creators of SORT has developed a fantastic (implementation)[https://github.com/abewley/sort] of SORT, which uses Filterpy. . Let’s take a look at his implementation, through the lens of what I’ve discussed above: . Quickly defining some nomenclature, . u and v are the x and y pixel coordinates of the center of the bounding box around an object being tracked. . | s and r are tha scale and aspect ratio of the bounding box surrounding the object. . | u˙,v˙ dot u, dot vu˙,v˙ are the x and y velocity of the bounding box. . | s˙ dot ss˙ is the rate at which the scale of the bounding box is changing. . | . kf = KalmanFilter(dim_x=7, dim_z=4) . Our internal state is 7 dimensional: . [u,v,s,r,u˙,v˙,s˙][u, v, s, r, dot u, dot v , dot s][u,v,s,r,u˙,v˙,s˙] . While our input vector is 4 dimensional: . [u,v,s,r][u, v, s, r][u,v,s,r] . kf.F = np.array([[1,0,0,0,1,0,0], [0,1,0,0,0,1,0], [0,0,1,0,0,0,1], [0,0,0,1,0,0,0], [0,0,0,0,1,0,0], [0,0,0,0,0,1,0], [0,0,0,0,0,0,1]]) . The state transition matrix tells us that at each timestep, we update our state as follows: . u=u+u˙u = u + dot uu=u+u˙ . v=v+v˙v = v + dot vv=v+v˙ . s=s+s˙s = s + dot ss=s+s˙ . kf.H = np.array([[1,0,0,0,0,0,0], [0,1,0,0,0,0,0], [0,0,1,0,0,0,0], [0,0,0,1,0,0,0]]) . The sensor matrix tells us that we are directly measuring [u,v,s,r][u, v, s, r][u,v,s,r]. . kf.R = np.array([[ 1, 0, 0, 0], [ 0, 1, 0, 0,], [ 0, 0, 10, 0,], [ 0, 0, 0, 10,]]) . The sensor noise matrix tells us that we can measure uuu and vvv with a much higher degree of certainty than sss and rrr. . kf.P = np.array([[ 10, 0, 0, 0, 0, 0, 0], [ 0, 10, 0, 0, 0, 0, 0], [ 0, 0, 10, 0, 0, 0, 0], [ 0, 0, 0, 10, 0, 0, 0], [ 0, 0, 0, 0, 10000, 0, 0], [ 0, 0, 0, 0, 0, 10000, 0], [ 0, 0, 0, 0, 0, 0, 10000]]) . The covariance matrix tells us that the filter should have a very high initial uncertinty for each of the velocity components. . kf.Q = np.array([[1.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00] [0.e+00, 1.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00] [0.e+00, 0.e+00, 1.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00] [0.e+00, 0.e+00, 0.e+00, 1.e+00, 0.e+00, 0.e+00, 0.e+00] [0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-02, 0.e+00, 0.e+00] [0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-02, 0.e+00] [0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-04]]) . The Process uncertainty matrix tells us how much “uncertainty” there is in each component of the systems behaviour. . Filterpy has a function which can be very useful for generating Q. . filterpy.common.Q_discrete_white_noise . Further reading . Control theory is a broad an intellectually stimulating area, with broad applications. Brian Douglas has an incredible YouTube channel which I strongly recommend. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/2019/03/23/Kalman-Filters.html",
            "relUrl": "/bayesian/2019/03/23/Kalman-Filters.html",
            "date": " • Mar 23, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "Scattered Interpolation",
            "content": "Geospatial data often comes in the forms of samples taken at different points, when this occours a common next step is interpolation. An example of this is Airbourne LiDAR, where a laser range finder is scanned over the ground underneath the aircraft. The produces a cloud of scattered points, and often the next step is build a Digital Elevation Model (DEM) from these scattered points. There are a number of different options for interpolation in python, the correct choice of method is often task specific, so its good to have some options at your disposal. . import numpy as np import math import os from PIL import Image import gdal import matplotlib.pyplot as plt from scipy import interpolate . The Data . def downloadDEMFromCGIAR(lat,lon): &#39;&#39;&#39; Download a DEM from CGIAR FTP repository &#39;&#39;&#39; fileName = lonLatToFileName(lon,lat)+&#39;.zip&#39; &#39;&#39;&#39; Check to see if we have already downloaded the file &#39;&#39;&#39; if fileName not in os.listdir(&#39;.&#39;): os.system(&#39;&#39;&#39;wget --user=data_public --password=&#39;GDdci&#39; http://data.cgiar-csi.org/srtm/tiles/GeoTIFF/&#39;&#39;&#39;+fileName) os.system(&#39;unzip &#39;+fileName) def lonLatToFileName(lon,lat): &#39;&#39;&#39; Compute the input file name &#39;&#39;&#39; tileX = int(math.ceil((lon+180)/5.0)) tileY = -1*int(math.ceil((lat-65)/5.0)) inputFileName = &#39;srtm_&#39;+str(tileX).zfill(2)+&#39;_&#39;+str(tileY).zfill(2) return(inputFileName) lon,lat = -155.48647818,19.6662251 inputFileName = lonLatToFileName(lon,lat) #downloadDEMFromCGIAR(lat,lon) utmZone = int((math.floor((lon + 180)/6) % 60) + 1) &#39;&#39;&#39; Check to see if file is in northern or southern hemisphere &#39;&#39;&#39; if lat&lt;0: EPSGCode = &#39;EPSG:327&#39;+str(utmZone) else: EPSGCode = &#39;EPSG:326&#39;+str(utmZone) !! gdalwarp -q -te -155.75 19.25 -155.25 19.75 -srcnodata -32768 -dstnodata 0 srtm_05_09.tif subset.tif os.system(&#39;gdalwarp -q -t_srs &#39;+EPSGCode+&#39; -tr 100 -100 -r cubic subset.tif warped.tif&#39;) . data = np.asarray(Image.open(&#39;warped.tif&#39;)) plt.imshow(data) plt.colorbar() plt.show() y_range = data.shape[0] x_range = data.shape[1] num_samples = 10**5 xArray = np.zeros(num_samples) yArray = np.zeros(num_samples) heightArray = np.zeros(num_samples) for sample_num in range(0,num_samples): x_idx = np.random.randint(0,x_range) y_idx = np.random.randint(0,y_range) xArray[sample_num] = x_idx yArray[sample_num] = y_idx heightArray[sample_num] = data[y_idx,x_idx] . plt.scatter(xArray,yArray,c=heightArray,alpha=0.75) plt.show() . Interpolation . Ultimately, there is no universally correct choice of interpolation method to use. However it is useful to know the mechanics associated with going from scattered points to an interpolated array for a number of different methods. . numIndexes = 500 xi = np.linspace(np.min(xArray), np.max(xArray),numIndexes) yi = np.linspace(np.min(yArray), np.max(yArray),numIndexes) . One option is linear interpolation, which can be performed using scipy.interpolate. . XI, YI = np.meshgrid(xi, yi) points = np.vstack((xArray,yArray)).T values = np.asarray(heightArray) points = np.asarray(points) DEM = interpolate.griddata(points, values, (XI,YI), method=&#39;linear&#39;) . Contour plots . Contour plots are useful for visualzing Digital Elevation Models, Our first step is to create the contour lines, with a 25m contour interval as follows : We can map the array to an image using imshow, an alternative is to use plt.contourf, to “fill in” the contours. . levels = np.arange(0,4000,100) plt.contour(DEM, levels,linewidths=0.2,colors=&#39;k&#39;) plt.imshow(DEM,origin=&#39;lower&#39;) plt.colorbar() plt.show() .",
            "url": "https://cgcooke.github.io/Blog/remote%20sensing/2018/11/18/Scattered-Interpolation.html",
            "relUrl": "/remote%20sensing/2018/11/18/Scattered-Interpolation.html",
            "date": " • Nov 18, 2018"
        }
        
    
  
    
        ,"post10": {
            "title": "256 Shades Of Grey",
            "content": "On February 22, 2000, after 11 days of measurements, the most comprehensive map ever created of the earth’s topography was complete. The space shuttle Endeavor had just completed the Shuttle Radar Topography Mission, using a specialised radar to image the earths surface. . The Digital Elevation Map (DEM) produced by this mission is in the public domain and provides the measured terrain high at ~90 meter resolution. The mission mapped 99.98% of the area between 60 degrees North and 56 degrees South. . In this post, I will examine how to process the raw DEM so it is more intuitively interpreted, through the use of hillshading,slopeshading &amp; hypsometric tinting. . The process of transforming the raw GeoTIFF into the final imagery product is simple. Much of the grunt work being carried out by GDAL, the Geospatial Data Abstraction Library. . In order, we need to: . Download a DEM as a GeoTIFF | Extract a subsection of the GeoTIFF | Reproject the subsection | Make an image by hillshading | Make an image by coloring the subsection according to altitude | Make an image by coloring the subsection according to slope | Combine the 3 images into a final composite | DEM . Several different DEM’s have been created from the data collected on the SRTM mission, in this post I will use the CGIAR SRTM 90m Digital Elevation Database. Data is provided in 5x5 degree tiles, with each degree of latitude equal to approximately 111Km. . Our first task is to acquire a tile. Tiles can be downloaded from http://data.cgiar-csi.org/srtm/tiles/GeoTIFF/ using wget. . import os import math from PIL import Image, ImageChops, ImageEnhance from matplotlib import cm . def downloadDEMFromCGIAR(lat,lon): &#39;&#39;&#39; Download a DEM from CGIAR FTP repository &#39;&#39;&#39; fileName = lonLatToFileName(lon,lat)+&#39;.zip&#39; &#39;&#39;&#39; Check to see if we have already downloaded the file &#39;&#39;&#39; if fileName not in os.listdir(&#39;.&#39;): os.system(&#39;&#39;&#39;wget --user=data_public --password=&#39;GDdci&#39; http://data.cgiar-csi.org/srtm/tiles/GeoTIFF/&#39;&#39;&#39;+fileName) os.system(&#39;unzip &#39;+fileName) . def lonLatToFileName(lon,lat): &#39;&#39;&#39; Compute the input file name &#39;&#39;&#39; tileX = int(math.ceil((lon+180)/5.0)) tileY = -1*int(math.ceil((lat-65)/5.0)) inputFileName = &#39;srtm_&#39;+str(tileX).zfill(2)+&#39;_&#39;+str(tileY).zfill(2) return(inputFileName) . lon,lat = -123,49 inputFileName = lonLatToFileName(lon,lat) downloadDEMFromCGIAR(lat,lon) . Slicing . The area I have selected covers Washington State and British Columbia, with file name srtm_12_03.tif. . Let’s use GDAL to extract a subsection of the tile.The subsection covers Vancouver Island and the Pacific Ranges stretching from 125ºW - 122ºW &amp; 48ºN - 50ºN. Using gdalwarp: . !! gdalwarp -q -te -125 48 -122 50 -srcnodata -32768 -dstnodata 0 srtm_12_03.tif subset.tif . Our next step is to transform the subsection of the tile to a different projection. The of the points in the subsection are located on a grid 1/1200th of a degree apart. While degrees of latitude are always ~110Km in size, resulting in ~92.5M resolution, degrees of longitude decrease in size, from ~111Km at the equator to 0Km at the poles. A different scale exists between the latitude &amp; longitude axis and a longitude scale that depends on the latitude. . A solution is to project that points so that there is a consistent and equal scale in the X/Y plane. One choice is to use a family of projections called Universal Transverse Mercator. Each UTM projection can map points from longitude &amp; latitude to X &amp; Y coordinates in meters. The UTM projection is useful because it locally preserves both shapes and distances, over a distances of up to several hundred kilometres. . The tradeoff is that several different UTM projections are required for different points on earth, 120 to be precise. Fortunately it is relatively trivial to work out the required projection based on the longitude and latitude. Almost every conceivable projection has been assigned a code by the European Petroleum Survey Group (EPSG). This EPSG code can be used to unambiguously specify the projection being used. With UTM, each code starts with either 327 or 326, depending on the hemisphere of the projection. . utmZone = int((math.floor((lon + 180)/6) % 60) + 1) &#39;&#39;&#39; Check to see if file is in northern or southern hemisphere &#39;&#39;&#39; if lat&lt;0: EPSGCode = &#39;EPSG:327&#39;+str(utmZone) else: EPSGCode = &#39;EPSG:326&#39;+str(utmZone) . Once we have identified the correct EPSG code to use, the process of warping the subset to a new projection is relatively straightforward. . In the following system call to gdalwarp, t_srs denotes the target projection, and tr specifies the resolution in the X and Y plane. The Y resolution is negative because the in the GDAL file uses a row, column based coordinate system. . In this coordinate system, the origin is in the top left hand corner of the file. The row value increases as you move down the file, like an excel spreadsheet, however the UTM Y coordinate decreases. This results in the negative sign in the resolution. . os.system(&#39;gdalwarp -q -t_srs &#39;+EPSGCode+&#39; -tr 100 -100 -r cubic subset.tif warped.tif&#39;) . Hillshading . At this point we can begin to visualise the DEM. One highly effective method is hillshading, which models the way the surface of the DEM would be illuminated by light projected onto it. Shading of the slopes allows the DEM to be more intuitively interpreted than just coloring by height alone. . . !! gdaldem hillshade -q -az 45 -alt 45 warped.tif hillshade.tif . Hypsometric Tinting . Hillshading can also be combined with height information to aid interpretation of the topography. The technical name for the process of coloring a DEM based on height is hypsometric tinting. The process is simple, with GDAL mapping colors to cell heights, using a provided color scheme. . . def createColorMapLUT(minHeight,maxHeight,cmap = cm.YlGn_r,numSteps=256): &#39;&#39;&#39; Create a colormap for visualisation &#39;&#39;&#39; f =open(&#39;color_relief.txt&#39;,&#39;w&#39;) f.write(&#39;-0.1,135,206,250 n&#39;) f.write(&#39;0.1,135,206,250 n&#39;) for i in range(0,numSteps): r,g,b,a= cmap(i/float(numSteps)) height = minHeight + (maxHeight-minHeight)*(i/numSteps) f.write(str(height)+&#39;,&#39;+str(int(255*r))+&#39;,&#39;+str(int(255*g))+&#39;,&#39;+str(int(255*b))+&#39; n&#39;) f.write(str(-1)+&#39;,&#39;+str(int(255*r))+&#39;,&#39;+str(int(255*g))+&#39;,&#39;+str(int(255*b))+&#39; n&#39;) createColorMapLUT(minHeight=10,maxHeight=2658) . !! gdaldem color-relief -q warped.tif color_relief.txt color_relief.tif . Slope Shading . Another technique for visualizing terrain is slopeshading. While hypsometric tinting assigns colors to cells based on elevation, slope shading assigns colors to pixels based on the slope (0º to 90º). In this case, white (255,255,255) is assigned to slopes of 0º and black (0,0,0) is assigned to slopes of 90º, with varying shades of grey for slopes in-between. . . This color scheme is encoded in a txt file for gdaldem as follows: . f = open(&#39;color_slope.txt&#39;,&#39;w&#39;) f.write(&#39;0 255 255 255 n&#39;) f.write(&#39;90 0 0 0 n&#39;) f.close() . The computation of the slope shaded dem takes place over two steps. . The slope of each cell is computed | A shade of grey is assigned to each cell depending on the slope. | !! gdaldem slope -q warped.tif slope.tif !! gdaldem color-relief -q slope.tif color_slope.txt slopeshade.tif . Layer Merging . The final step in producing the final product is to merge the 3 different created images. The python Image Library (PIL) is a quick and dirty way to accomplish this task, with the 3 layers are merged using pixel by pixel multiplication. . One important detail to note is that the pixel by pixel multiplication occurs in the RGB space. From a theoretical perspective, it’s probably better that each pixel is first transformed to the Hue, Saturation, Value (HSV) color space, and the value is then multiplied by the hillshade and slope shade value, before being transformed back into the RGB color space. In practical terms however, the RGB space multiplication is a very reasonable approximation. . In one final tweak, the brightness of the output image is increased by 40%, to offset the average reduction in brightness caused by multiplying the layers together. . . &#39;&#39;&#39; Merge components using Python Image Lib &#39;&#39;&#39; slopeshade = Image.open(&quot;slopeshade.tif&quot;).convert(&#39;L&#39;) hillshade = Image.open(&quot;hillshade.tif&quot;) colorRelief = Image.open(&quot;color_relief.tif&quot;) #Lets just fill in any gaps in the hillshading ref = Image.new(&#39;L&#39;, slopeshade.size,180) hillshade = ImageChops.lighter(hillshade,ref) shading = ImageChops.multiply(slopeshade, hillshade).convert(&#39;RGB&#39;) merged = ImageChops.multiply(shading,colorRelief) &#39;&#39;&#39; Adjust the brightness to take into account the reduction caused by hillshading&#39;&#39;&#39; enhancer = ImageEnhance.Brightness(merged) img_enhanced = enhancer.enhance(1.4) img_enhanced.save(&#39;Merged.png&#39;) . Further reading . I found the following sources to be invaluable in compiling this post: . Creating color relief and slope shading | A workflow for creating beautiful relief shaded DEMs using gdal | Shaded relief map in python | Stamen Design | .",
            "url": "https://cgcooke.github.io/Blog/remote%20sensing/2018/11/18/256-Shades-of-Grey.html",
            "relUrl": "/remote%20sensing/2018/11/18/256-Shades-of-Grey.html",
            "date": " • Nov 18, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Cameron, . I’m passionate about AI (Computer Vision in particular), Robotics and the future of Aerospace. . I’m writing a blog because the first step to truly understanding something is being able to explain it to someone else. . Plus I get excited about the incredible world around us, and can’t wait to tell everyone about it! . Until next time. . Cameron . .",
          "url": "https://cgcooke.github.io/Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}