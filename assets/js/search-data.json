{
  
    
        "post0": {
            "title": "Modelling Afterpay's customer growth",
            "content": "The Context . Founded in 2014, Afterpay is a wildly successful Australian fintech startup. With a market capitalisation of circa 14 billion USD, it has rapidly grown to be a one of Australia&#39;s largest companies. In summary, Afterpay let&#39;s customers pay for products in 4 separate payments, and charges stores a margin for this. . I&#39;m interested in applying Bayesian analysis to understand more about Afterpay, based on the information that it has provided to the market, plus a little common sense. . Important: Obviously I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot. . Now, with that out of the way, let&#39;s get started. . The Model . First off, let&#39;s load in a bunch of libraries. . %matplotlib inline import pymc3 as pm import matplotlib.pyplot as plt import numpy as np from io import StringIO import pandas as pd import math plt.rcParams[&quot;figure.figsize&quot;] = (10,10) from warnings import filterwarnings filterwarnings(&#39;ignore&#39;) . While reading through Afterpay&#39;s releases to the markets, I came across this chart, which appears on page 3 of this release. . . csv_data = StringIO(&#39;&#39;&#39;millions_customers,date,month_count 0.0,30-sep-2015,0 0.0,31-dec-2015,3 0.0,31-mar-2016,6 0.1,30-jun-2016,9 0.2,30-sep-2016,12 0.4,31-dec-2016,15 0.6,31-mar-2017,18 0.8,30-jun-2017,21 1.1,30-sep-2017,24 1.5,31-dec-2017,27&#39;&#39;&#39;) df = pd.read_csv(csv_data, sep=&quot;,&quot;) plt.plot(df.month_count,df.millions_customers,color=&#39;r&#39;) plt.ylabel(&#39;Millions of customers&#39;) plt.xlabel(&#39;Months after launch&#39;) plt.grid() plt.show() . From this, we can see a rapid, accelerating growth in the number of customers over time. . One model we could use is the sigmoidal model. Also known as &quot;The S shaped curve&quot;, it&#39;s a model where growth starts off slow, accelerates, before slowing again. It&#39;s often used in technology adoption, or the introduction of a new product. . $$ frac{1}{1+e^{-x}}$$ . x = np.arange(-10,10,0.01) y = 1/(1+math.e**-x) plt.plot(x,y,color=&#39;k&#39;) plt.grid() plt.show() . The model can be modified, to modify the scale (L), how fast it grows (k), or when the fastest growth occurs (x0). . $$ frac{L}{1+e^{-k(x-x0)}}$$ . We can fit this model, to the data provided by Afterpay using PyMC3. . We can also utilise priors, to inject things we might know or suspect. . For example, while I am open minded about what proportion of Australians may choose to become customers of Afterpay, I limit the model to a maximum of 25M customers (population of Australia). . In general, this isn&#39;t great practice, because I explicitly exclude the possibility that there could be more than 25M customers, and no amount of data will be able to change my mind. . To quote Cromwell : I beseech you, in the bowels of Christ, think it possible that you may be mistaken. . Let&#39;s compose this as a Bayesian Regression problem. . with pm.Model() as model: millions_customers = df.millions_customers.values x = df.month_count.values.astype(np.float64) L = pm.Uniform(&#39;L&#39;, lower = 0, upper = 25) k = pm.Uniform(&#39;k&#39;, lower=0, upper=1) x0 = pm.Uniform(&#39;x0&#39;, lower=0, upper=100) customers_predicted = L/(1+math.e**(-k*(x-x0))) customers = pm.Normal(&#39;customers&#39;, mu = customers_predicted, sigma = 0.1, observed = millions_customers) . with model: trace = pm.sample(draws=10_000,tune=5_000) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [x0, k, L] Sampling 2 chains, 0 divergences: 100%|██████████| 30000/30000 [01:11&lt;00:00, 422.10draws/s] The number of effective samples is smaller than 25% for some parameters. . The Results . From the following plots, we can notice several things. Firstly, and most importantly, our model converged well. Secondly, we can see that according to our model, we expect the number of customers to most likely saturate below 5m people, but it&#39;s not impossible that we might have a lot more customers, even 25m. . Finally, we can see that there is a lot of uncertainty around the timing of the peak of the growth. This ranges from approximately 2 years to 4 years after the initial introduction of Afterpay. . pm.traceplot(trace); . pm.plot_posterior(trace); . pm.summary(trace) . mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . L 9.060 | 7.093 | 1.411 | 22.464 | 0.147 | 0.104 | 2331.0 | 2331.0 | 2034.0 | 3527.0 | 1.0 | . k 0.157 | 0.039 | 0.106 | 0.237 | 0.001 | 0.001 | 1992.0 | 1945.0 | 2380.0 | 2702.0 | 1.0 | . x0 34.935 | 9.071 | 20.408 | 48.923 | 0.196 | 0.139 | 2138.0 | 2138.0 | 2115.0 | 3482.0 | 1.0 | . Posterior Predictive Checks . Let&#39;s see if our model makes sense. By drawing samples from our model, we can further investigate the potential growth rates of Afterpay.  . for i in range(0,1_000): x = np.arange(0,30) plt.plot(x,trace[&#39;L&#39;][i]/(1+math.e**(-trace[&#39;k&#39;][i]*(x-trace[&#39;x0&#39;][i]))),color=&#39;k&#39;,alpha=0.01) plt.plot(df.month_count,df.millions_customers,color=&#39;r&#39;,label=&#39;Reported Customers&#39;) plt.legend() plt.ylabel(&#39;Millions of customers&#39;) plt.xlabel(&#39;Months after launch&#39;) plt.show() . Now let&#39;s start forcasting into the future. We can compare what we modelled to the actual customer numbers (3.1M) reported by Afterpay 54 months after they first launched. . for i in range(0,10_000): x = np.arange(0,55) plt.plot(x,trace[&#39;L&#39;][i]/(1+math.e**(-trace[&#39;k&#39;][i]*(x-trace[&#39;x0&#39;][i]))),color=&#39;k&#39;,alpha=0.01) plt.plot(df.month_count,df.millions_customers,color=&#39;r&#39;,label=&#39;Reported Customers&#39;) plt.scatter(54,3.1,label=&#39;Most recent report&#39;) plt.legend() plt.ylabel(&#39;Millions of customers&#39;) plt.xlabel(&#39;Months after launch&#39;) plt.show() . We can also generate a histogram, and compare the predictions of our model with the actual true number of 3.1M. . x = 54*np.ones(20000) y = trace[&#39;L&#39;]/(1+math.e**(-trace[&#39;k&#39;]*(x-trace[&#39;x0&#39;]))) plt.hist(y,bins=np.arange(0,25,0.5),alpha=0.5,density=True) plt.vlines(3.1,0,0.175,label=&#39;Most recent report&#39;) plt.xlabel(&#39;Millions of customers&#39;) plt.legend() plt.grid() plt.show() . Conclusion . In conclusion, Bayesian analysis is a powerful tool, because it allows us to understand how confident we are in our predictions. It&#39;s also powerful because we can feed in information that we might already believe. If you are 80% confident that no more than 20% of the Australian population will use Afterpay, then you can feed this in as a prior. . While this is a fairly rough and ready model, I&#39;m going to continue analysing Afterpay. My end-game is to try and gain an understanding of their customer&#39;s default rates. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/finance/2020/08/12/Modelling-Afterpay's-Customer-Growth.html",
            "relUrl": "/bayesian/finance/2020/08/12/Modelling-Afterpay's-Customer-Growth.html",
            "date": " • Aug 12, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Bayesian Camera Calibration",
            "content": "The Context . In a previous post, I attempted to reverse-engineer information about a camera, from a photo it had taken of a scene. While I found a solution, I wasn’t sure how confident I could be in the answer. I was also curious if I could improve the solution by injecting prior knowledge from other sources. . I had the idea to apply Bayesian analysis, and try to find a solution using Makov Chain Monte Carlo and PyMC3. After a bit of searching, I also found this paper, which told me that the idea wasn’t completely outlandish. . In this post, we will combine a prior belief (probability distributions) about some of the camera’s parameters, with measured 2D-3D scene correspondances. By combining these two sources of information, we can compute posterior distributions for each camera parameter. . Because we have a probability distribution, we can understand how certain we are about each parameter. . Let’s start by building a model. . Modelling . import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pymc3 as pm import pandas as pd from warnings import filterwarnings filterwarnings(&#39;ignore&#39;) plt.rcParams[&#39;figure.figsize&#39;] = [10,10] . df = pd.read_csv(&#39;data/2020-07-05-Bayesian-Camera-Calibration/points.csv&#39;,sep =&#39; &#39;) px = df.i.values py = df.j.values X_input = df.X.values Y_input = df.Y.values Z_input = df.Z.values number_points = px.shape[0] points3d = np.vstack([X_input,Y_input,Z_input]).T . Ok, so now that we have loaded in our 2D and 3D point correspondences, we can now turn to representing the camera itself. . Quaternions . For reasons of numerical stability, I’m going to use quaternions to represent the camera’s orientation/attitude in 3D space. . def create_rotation_matrix(Q0,Q1,Q2,Q3):     #Create a rotation matrix from a Quaternion representation of an angle.     R =[[Q0**2 + Q1**2 - Q2**2 - Q3**2, 2*(Q1*Q2 - Q0*Q3), 2*(Q0*Q2 + Q1*Q3)],         [2*(Q1*Q2 + Q0*Q3), Q0**2 - Q1**2 + Q2**2 - Q3**2, 2*(Q2*Q3 - Q0*Q1)],         [2*(Q1*Q3 - Q0*Q2), 2*(Q0*Q1 + Q2*Q3), (Q0**2 - Q1**2 - Q2**2 + Q3**2)]]     return(R) def normalize_quaternions(Q0,Q1,Q2,Q3):     norm = pm.math.sqrt(Q0**2 + Q1**2 + Q2**2 + Q3**2)     Q0 /= norm     Q1 /= norm     Q2 /= norm     Q3 /= norm     return(Q0,Q1,Q2,Q3) . In a previous post, I used sets of parallel lines in the image, to find the locations of vanishing points. . By using these vanishing points, I was able to determine both an estimate for the orientation of the camera, as well as of the intrinsic parameters. . Because we are using quaternions to represent the orientation of the camera, we have 4 different components (X,Y,Z,W). . A prior is a probability distribution on a parameter, and I’m using Student’s T to model this distribution. . Extrinsics . Q1 = pm.StudentT(&#39;Xq&#39;, nu = 1.824, mu = 0.706, sigma = 0.015) Q2 = pm.StudentT(&#39;Yq&#39;, nu = 1.694, mu = -0.298, sigma = 0.004) Q3 = pm.StudentT(&#39;Zq&#39;, nu = 2.015, mu = 0.272, sigma = 0.011) Q0 = pm.StudentT(&#39;Wq&#39;, nu = 0.970, mu = 0.590, sigma = 0.019) . To form a prior estimate for the location of the camera, I’m taking the results we found in this post, I’m taking the solution I generated as an initial estimate. However, I’m going to be open minded, and model the position using a normal distribution, with a standard deviation of 10 meters. . As I mentioned at the end of post, While I found a solution for the location of the camera, It was much lower that what I would have guessed. . I can imagine the camera being about 7-10 meters off the ground, so by using a broad prior, we are saying that this outcome wouldn’t be that surprising, if it was supported by the evidence. . # Define  translation priors X_translate = pm.Normal(&#39;X_translate&#39;, mu = -6.85, sigma = 10) Y_translate = pm.Normal(&#39;Y_translate&#39;, mu = -12.92, sigma = 10) Z_translate = pm.Normal(&#39;Z_translate&#39;, mu = 2.75, sigma = 10) . Now we have to Rotate and Translate the points, in 3D space, according to the attitude and the position of the camera in 3D space. . [R∣t][R | t][R∣t] . Where $t$ is: . t=−RCt = −RCt=−RC . In a previous post, I was able to elegantly use Numpy. . #Camera Center C = camera_params[3:6].reshape(3,1) IC = np.hstack([np.eye(3),-C]) RIC = np.matmul(R,IC) #Make points Homogeneous points = np.hstack([points,np.ones((points.shape[0],1))]) #Perform Rotation and Translation #(n,k), (k,m) -&gt; (n,m) points_proj = np.matmul(points,RIC.T) . However, we are a little constrained with PyMC3, so I explicitly (and inelegantly) carry out this as follows. . RIC_0_3 = R[0][0] * -X_translate + R[0][1] * -Y_translate + R[0][2] * -Z_translate RIC_1_3 = R[1][0] * -X_translate + R[1][1] * -Y_translate + R[1][2] * -Z_translate RIC_2_3 = R[2][0] * -X_translate + R[2][1] * -Y_translate + R[2][2] * -Z_translate X_out = X_est * R[0][0] + Y_est * R[0][1] + Z_est * R[0][2] + RIC_0_3 Y_out = X_est * R[1][0] + Y_est * R[1][1] + Z_est * R[1][2] + RIC_1_3 Z_out = X_est * R[2][0] + Y_est * R[2][1] + Z_est * R[2][2] + RIC_2_3 . Now let’s put it all together, . def Rotate_Translate(X_est, Y_est, Z_est):     #Define rotation priors     Q1 = pm.StudentT(&#39;Xq&#39;, nu = 1.824, mu = 0.706, sigma = 0.015)     Q2 = pm.StudentT(&#39;Yq&#39;, nu = 1.694, mu = -0.298, sigma = 0.004)     Q3 = pm.StudentT(&#39;Zq&#39;, nu = 2.015, mu = 0.272, sigma = 0.011)     Q0 = pm.StudentT(&#39;Wq&#39;, nu = 0.970, mu = 0.590, sigma = 0.019)         Q0,Q1,Q2,Q3 = normalize_quaternions(Q0,Q1,Q2,Q3)         R = create_rotation_matrix(Q0,Q1,Q2,Q3)         # Define  translation priors     X_translate = pm.Normal(&#39;X_translate&#39;, mu = -6.85, sigma = 10)     Y_translate = pm.Normal(&#39;Y_translate&#39;, mu = -12.92, sigma = 10)     Z_translate = pm.Normal(&#39;Z_translate&#39;, mu = 2.75, sigma = 10)         RIC_0_3 = R[0][0] * -X_translate + R[0][1] * -Y_translate + R[0][2] * -Z_translate     RIC_1_3 = R[1][0] * -X_translate + R[1][1] * -Y_translate + R[1][2] * -Z_translate     RIC_2_3 = R[2][0] * -X_translate + R[2][1] * -Y_translate + R[2][2] * -Z_translate         X_out = X_est * R[0][0] + Y_est * R[0][1] + Z_est * R[0][2] + RIC_0_3     Y_out = X_est * R[1][0] + Y_est * R[1][1] + Z_est * R[1][2] + RIC_1_3     Z_out = X_est * R[2][0] + Y_est * R[2][1] + Z_est * R[2][2] + RIC_2_3         return(X_out, Y_out, Z_out) . Intrinsics . For the intrinsics, I’m using a mixture of priors from the results of our optimisation, as well as what was identified in this post. . focal_length = pm.Normal(&#39;focal_length&#39;,mu = 2189.49, sigma = 11.74)       k1 = pm.Normal(&#39;k1&#39;, mu = -0.327041, sigma = 0.5 * 0.327041) k2 = pm.Normal(&#39;k2&#39;, mu = 0.175031,  sigma = 0.5 * 0.175031) k3 = pm.Normal(&#39;k3&#39;, mu = -0.030751, sigma = 0.5 * 0.030751) c_x = pm.Normal(&#39;c_x&#39;, mu = 2268/2.0, sigma = 1000) c_y = pm.Normal(&#39;c_y&#39;, mu = 1503/2.0, sigma = 1000) . with pm.Model() as model:     X, Y, Z = Rotate_Translate(points3d[:,0], points3d[:,1], points3d[:,2])         focal_length = pm.Normal(&#39;focal_length&#39;,mu = 2189.49, sigma = 11.74)           k1 = pm.Normal(&#39;k1&#39;, mu = -0.327041, sigma = 0.5 * 0.327041)     k2 = pm.Normal(&#39;k2&#39;, mu = 0.175031,  sigma = 0.5 * 0.175031)     k3 = pm.Normal(&#39;k3&#39;, mu = -0.030751, sigma = 0.5 * 0.030751)         c_x = pm.Normal(&#39;c_x&#39;, mu = 2268/2.0, sigma = 1000)     c_y = pm.Normal(&#39;c_y&#39;, mu = 1503/2.0, sigma = 1000)         px_est = X / Z     py_est = Y / Z         #Radial distortion     r = pm.math.sqrt(px_est**2 + py_est**2)         radial_distortion_factor = (1 + k1 * r + k2 * r**2 + k3 * r**3)     px_est *= radial_distortion_factor     py_est *= radial_distortion_factor         px_est *= focal_length     py_est *= focal_length     px_est += c_x     py_est += c_y         error_scale = 5 #px         delta = pm.math.sqrt((px - px_est)**2 + (py - py_est)**2)         # Define likelihood     likelihood = pm.Normal(&#39;rms_pixel_error&#39;, mu = delta, sigma = error_scale, observed=np.zeros(number_points)) . Finally we can use Markov Chain Monte Carlo (MCMC) to find the posterior distribution of the parameters. . with pm.Model() as model:     # Inference!     trace = pm.sample(draws=10_000, init=&#39;adapt_diag&#39;, cores=4, tune=5_000) . Results . Now that the MCMC sampling has finished, let’s look at the results: . pm.plot_posterior(trace); . . From this we can see two things. In the left hand column, we can see the distribution of potential values for each parameter. In the right hand column, we can see how the MCMC sampler moved through this space over time. . pm.summary(trace) . . A number of things stand out. . Firstly, the Z_centroid of the camera is now 7.44 meters off the ground, with a very small standard deviation (8.5cm). . Secondly, the y coordinate of the inferred principle point (c_y) is much closer to what we expect. Previously we found that it was located at ±2650 pixels, well outside the image. Now we find that it’s somewhere in the broad region of ±1000 pixels. . All things considered, we now have a solution where we understand how confident we are in each parameter, and is a solution that is more reasonable, than what we found in this post. . If we found more data, we could further refine our estimates, by using the posterior results as new priors. This is exciting, as it gives us the framework to evolve and update both our results, and how confident we are, over time. . Additional plots . pm.pairplot(trace, var_names=[&#39;X_translate&#39;,&#39;Y_translate&#39;,&#39;Z_translate&#39;], divergences=True); . . pm.pairplot(trace, var_names=[&#39;k1&#39;, &#39;k2&#39;, &#39;k3&#39;], divergences=True); . . pm.pairplot(trace, var_names=[&#39;c_x&#39;, &#39;c_y&#39;], divergences=True); . . pm.pairplot(trace, var_names=[&#39;Wq&#39;, &#39;Xq&#39;,&#39;Yq&#39;,&#39;Zq&#39;], divergences=True); . . sns.jointplot(trace[:][&#39;X_translate&#39;], trace[:][&#39;Y_translate&#39;], kind=&quot;hex&quot;); . . Conclusion . I’ve been planning to write this post for about a few months, but to get here was an interesting journey, as I needed to first put many of the building blocks in place. . I’m still on my Bayesian modelling journey, but I’ve been inspired by others along the way, in particular, this awesome post. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/pymc3/computer%20vision/2020/07/07/Bayesian-Camera-Calibration.html",
            "relUrl": "/bayesian/pymc3/computer%20vision/2020/07/07/Bayesian-Camera-Calibration.html",
            "date": " • Jul 7, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Vanishing points in practice",
            "content": "The Context . In a previous post, I used optimisation to try to calibrate a camera. Ultimately, while we arrived at a plausible solution, I was left with some questions. . In particular, I wasn&#39;t sure how much I could trust this solution. The principle point of the image, which is normally located near the center of the image, was located far outside it. . This in of itself isn&#39;t fatal. The image could have been a crop of a photo, however it&#39;s extremely unusual. . If we want a more accurate answer, we can break this down into a two step process, firstly, computing estimates of the attitude and the camera calibration matrix. . We can then combine these estimates with the points we have measured previously, to produce a new estimate of the position and attitude of the camera. I will cover this in the next post. . We can compute the attitude and camera matrix using the method shown in a couple of previous blog posts. . Finding vanishing points. . Firstly, let&#39;s start by finding the vanishing points in the image. I&#39;ve described how this happens in more detail in this [post] (https://cgcooke.github.io/Blog/computer%20vision/linear%20algebra/monte%20carlo%20simulation/2020/04/10/Finding-Vanishing-Points.html). . Let&#39;s start by loading in the libraries we will need. . from PIL import Image import matplotlib.pyplot as plt import json import numpy as np import scipy.linalg import seaborn as sns import scipy.stats from scipy.spatial.transform import Rotation as Rot plt.rcParams[&#39;figure.figsize&#39;] = [15, 15] np.random.seed(1) . img = Image.open(&#39;data/2020-02-23-An-Adventure-In-Camera-Calibration/A300.jpg&#39;) plt.imshow(img) plt.show() . Maximum Likelihood Estimate . Let&#39;s now load in the annotations I have made to this image. . JSON = json.loads(open(&#39;data/2020-07-06-Vanishing-Points-In-Practice/A300.json&#39;,&#39;r&#39;).read()) . def intersect_multiple_lines(P0,P1): &quot;&quot;&quot;P0 and P1 are NxD arrays defining N lines. D is the dimension of the space. This function returns the least squares intersection of the N lines from the system given by eq. 13 in http://cal.cs.illinois.edu/~johannes/research/LS_line_intersect.pdf. &quot;&quot;&quot; # generate all line direction vectors n = (P1-P0)/np.linalg.norm(P1-P0,axis=1)[:,np.newaxis] # normalized # generate the array of all projectors projs = np.eye(n.shape[1]) - n[:,:,np.newaxis]*n[:,np.newaxis] # I - n*n.T # generate R matrix and q vector R = projs.sum(axis=0) q = (projs @ P0[:,:,np.newaxis]).sum(axis=0) # solve the least squares problem for the # intersection point p: Rp = q p = np.linalg.lstsq(R,q,rcond=None)[0] return(p) . def load_line_data(point_name): P0 = [] P1 = [] for shape in JSON[&#39;shapes&#39;]: points = shape[&#39;points&#39;] if shape[&#39;label&#39;] == point_name: P0.append(points[0]) P1.append(points[1]) P0 = np.array(P0,dtype=np.float64) P1 = np.array(P1,dtype=np.float64) return(P0,P1) def find_vanishing_point(point_name): P0,P1 = load_line_data(point_name) p = intersect_multiple_lines(P0,P1).ravel() return(p) . Using the annotations I manually created of the parallel lines in the images, Let&#39;s compute 3 different vanishing points. In any image, there are an infinite set of parallel lines, and thus vanishing points, however we typically live in a manhattan world, of orthogonal 90 degree angles. Hence in most scenes, there are typically 3 different vanishing points.  . vanishing_points = {} for point_name in [&#39;VP1&#39;,&#39;VP2&#39;,&#39;VP3&#39;]: vanishing_points[point_name] = find_vanishing_point(point_name) plt.imshow(img) for point_name,color in [(&#39;VP1&#39;,&#39;g&#39;),(&#39;VP2&#39;,&#39;r&#39;),(&#39;VP3&#39;,&#39;b&#39;)]: vp = vanishing_points[point_name] p0,p1 = load_line_data(point_name) print(point_name,vp) plt.scatter(vp[0],vp[1],color=color,label=point_name) for i in range(0,p0.shape[0]): plt.plot([p0[i,0],p1[i,0]],[p0[i,1],p1[i,1]], color=color,alpha=0.5) plt.plot([vp[0],p1[i,0]],[vp[1],p1[i,1]], color=color,alpha=0.5) plt.legend() plt.ylim(1500,0) plt.show() . VP1 [2908.86609693 665.60570984] VP2 [-1634.06911245 479.56926814] VP3 [2.45111941e+01 2.60455689e+04] . Monte Carlo Simulation . Ok, so this is a good start, we can find the MLE estimate for the location of the vanishing points. However, I&#39;m also interested in the distribution of possible locations, given that I most likely made mistakes when annotating the points. . I&#39;m not sure of exactly how large the mistakes could be, but let&#39;s start by assuming that the standard deviation of the error was 1 pixel, and then perform a monte carlo simulation. . def monte_carlo_simulation(point_name,num_samples=1_000): P0,P1 = load_line_data(point_name) point_error_magnitude = 1 #px vanishing_points = [] for i in range(0,num_samples): P0_stochastic = P0 + point_error_magnitude * np.random.randn(P0.shape[0],P0.shape[1]) P1_stochastic = P1 + point_error_magnitude * np.random.randn(P1.shape[0],P1.shape[1]) p = intersect_multiple_lines(P0_stochastic,P1_stochastic) vanishing_points.append(p) vanishing_points = np.asarray(vanishing_points) return(vanishing_points) . plt.imshow(img) for point_name,color in [(&#39;VP1&#39;,&#39;g&#39;),(&#39;VP2&#39;,&#39;r&#39;),(&#39;VP3&#39;,&#39;b&#39;)]: vanishing_points = monte_carlo_simulation(point_name) for p in vanishing_points: plt.scatter(p[0],p[1],color=color,alpha=0.1) plt.ylim(1500,0) plt.show() . Finding the Intrinsic Matrix . def generate_A(vanishing_points): A = [] for (point_name_1, point_name_2) in [(&#39;VP1&#39;,&#39;VP2&#39;),(&#39;VP2&#39;,&#39;VP3&#39;),(&#39;VP3&#39;,&#39;VP1&#39;)]: vp1 = vanishing_points[point_name_1] vp2 = vanishing_points[point_name_2] x1,y1 = vp1 x2,y2 = vp2 w1 = x2*x1 + y2*y1 w2 = x2 + x1 w3 = y2 + y1 w4 = 1 A.append([w1,w2,w3,w4]) A = np.array(A) return(A) def compute_K(A): w = scipy.linalg.null_space(A).ravel() w1 = w[0] w2 = w[1] w3 = w[2] w4 = w[3] omega = np.array([[w1,0,w2], [0,w1,w3], [w2,w3,w4]]) K = np.linalg.inv(np.linalg.cholesky(omega)).T K/=K[2,2] return(K) . Finding the Rotation Matrix . def make_homogeneous(x): x_homogeneous = np.array([x[0],x[1],1]) return(x_homogeneous) def compute_R(K,vanishing_points): v_x_h = make_homogeneous(vanishing_points[&#39;VP1&#39;]) v_y_h = make_homogeneous(vanishing_points[&#39;VP2&#39;]) v_z_h = make_homogeneous(vanishing_points[&#39;VP3&#39;]) K_inv = np.linalg.inv(K) R_1 = np.dot(K_inv,v_x_h)/np.linalg.norm(np.dot(K_inv,v_x_h)).T R_2 = np.dot(K_inv,v_y_h)/np.linalg.norm(np.dot(K_inv,v_y_h)).T R_3 = np.cross(R_1,R_2) R = np.vstack([R_1,R_2,R_3]) return(R) . Ok, now let&#39;s visualize the results from 1,000 iterations of the monte carlo simulation. . from mpl_toolkits.mplot3d import Axes3D PTS1 = monte_carlo_simulation(point_name=&#39;VP1&#39;) PTS2 = monte_carlo_simulation(point_name=&#39;VP2&#39;) PTS3 = monte_carlo_simulation(point_name=&#39;VP3&#39;) fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) for i in range(0,1_000): vanishing_points = {} vanishing_points[&#39;VP1&#39;] = (PTS1[i,0,0],PTS1[i,1,0]) vanishing_points[&#39;VP2&#39;] = (PTS2[i,0,0],PTS2[i,1,0]) vanishing_points[&#39;VP3&#39;] = (PTS3[i,0,0],PTS3[i,1,0]) try: A = generate_A(vanishing_points) K = compute_K(A) R = compute_R(K,vanishing_points) ax.plot(xs=[0,R[0,0]], ys=[0,R[1,0]], zs = [0,R[2,0]],color=&#39;r&#39;,alpha=0.01) ax.plot(xs=[0,R[0,1]], ys=[0,R[1,1]], zs = [0,R[2,1]],color=&#39;g&#39;,alpha=0.01) ax.plot(xs=[0,R[0,2]], ys=[0,R[1,2]], zs = [0,R[2,2]],color=&#39;b&#39;,alpha=0.01) except: pass plt.show() . Parameter distributions . Quaternions . From the above visualisation, we can see that we have a distribution of attitudes. Let&#39;s make a subtle change, and find the quaternion representation of the angles. . Quaternions are an elegant way to represent a rotation in 3 dimensions, using 4 numbers. Without going into details, they have many useful properties. If you want to learn more, 3Blue1Brown has done some incredible videos explaining them in more details. . Let&#39;s do a Monte carlo simulation, and compute both the means, and variances of the attitudes. . quats = [] Ks = [] PTS1 = monte_carlo_simulation(point_name=&#39;VP1&#39;,num_samples = 10_000) PTS2 = monte_carlo_simulation(point_name=&#39;VP2&#39;,num_samples = 10_000) PTS3 = monte_carlo_simulation(point_name=&#39;VP3&#39;,num_samples = 10_000) for i in range(0,10_000): vanishing_points = {} vanishing_points[&#39;VP1&#39;] = (PTS1[i,0,0],PTS1[i,1,0]) vanishing_points[&#39;VP2&#39;] = (PTS2[i,0,0],PTS2[i,1,0]) vanishing_points[&#39;VP3&#39;] = (PTS3[i,0,0],PTS3[i,1,0]) try: A = generate_A(vanishing_points) K = compute_K(A) R = compute_R(K,vanishing_points) R = R.T r = Rot.from_matrix(R) Ks.append(K.ravel()) quats.append(r.as_quat()) except: pass Ks = np.vstack(Ks) quats = np.vstack(quats) . sns.distplot(quats[:,0],kde=False,fit=scipy.stats.t,norm_hist=True,bins=100,label=&#39;X&#39;); sns.distplot(quats[:,1],kde=False,fit=scipy.stats.t,norm_hist=True,bins=100,label=&#39;Y&#39;); sns.distplot(quats[:,2],kde=False,fit=scipy.stats.t,norm_hist=True,bins=100,label=&#39;Z&#39;); sns.distplot(quats[:,3],kde=False,fit=scipy.stats.t,norm_hist=True,bins=100,label=&#39;W&#39;); plt.legend() plt.show() . Ok, now let&#39;s fit Student&#39;s T distribution to each quaternion component: . component_names = [&#39;X&#39;,&#39;Y&#39;,&#39;Z&#39;,&#39;W&#39;] for i in range(0,4): nu, mu, sigma = scipy.stats.distributions.t.fit(quats[:,i]) print(&quot;{}: nu: {:.3f}, mu: {:.3f}, sigma: {:.3f}&quot;.format(component_names[i],nu,mu,sigma)) . X: nu: 1.824, mu: 0.706, sigma: 0.015 Y: nu: 1.694, mu: -0.298, sigma: 0.004 Z: nu: 2.015, mu: 0.272, sigma: 0.011 W: nu: 0.970, mu: 0.590, sigma: 0.019 . Focal Length . Now let&#39;s do the same for the focal length: . sns.distplot(Ks[:,0],kde=False,fit=scipy.stats.norm,label=&#39;Focal length&#39;); plt.legend() plt.show() mean, var = scipy.stats.distributions.norm.fit(Ks[:,0]) print(&quot;Mean: {:.2f}, Std: {:.2f}&quot;.format(mean,np.sqrt(var))) . Mean: 2189.49,Std: 11.74 . Principle Point . Finally, let&#39;s see what we think the principle point of the camera might be: . sns.distplot(Ks[:,2],kde=False,fit=scipy.stats.norm,label=&#39;cx&#39;); plt.legend() plt.show() mean, var = scipy.stats.distributions.norm.fit(Ks[:,2]) print(&quot;Mean (CX): {:.2f}, Std: {:.2f}&quot;.format(mean,np.sqrt(var))) sns.distplot(Ks[:,5],kde=False,fit=scipy.stats.norm,label=&#39;cy&#39;); plt.legend() plt.show() mean, var = scipy.stats.distributions.norm.fit(Ks[:,5]) print(&quot;Mean (CY): {:.2f}, Std: {:.2f}&quot;.format(mean,np.sqrt(var))) . Mean (CX): 845.77, Std: 9.73 . Mean (CY): 1032.71, Std: 17.25 . Now let&#39;s plot the joint distribution of th.e principle point in 2D space. . sns.jointplot(Ks[:,2],Ks[:,5], kind=&quot;hex&quot;); .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/linear%20algebra/monte%20carlo%20simulation/2020/07/06/Vanishing-Points-In-Practice.html",
            "relUrl": "/computer%20vision/linear%20algebra/monte%20carlo%20simulation/2020/07/06/Vanishing-Points-In-Practice.html",
            "date": " • Jul 6, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "On Target With PyMC3",
            "content": "The context . Last weekend I was relaxing in the countryside at a colleague&#39;s château, and we decided to take turns shooting at a target with an air rifle. . Unfortunately, we were all terrible shots, and the rifle was uncalibrated, so we didn&#39;t do a particularly good job of hitting the target. . In order to improve our aim, we need to adjust the rifle sights so that the pellet impacts are centered on the middle of the target. . The model . Let&#39;s start modelling this situation in Python. . import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pymc3 as pm from PIL import Image from warnings import filterwarnings filterwarnings(&#39;ignore&#39;) plt.rcParams[&#39;figure.figsize&#39;] = [20,20] np.random.seed(1337) . Ok, now let&#39;s pretend that our rifle is incorrectly sighted/calibrated such that, when we are aiming at exactly at the center of the target, our pellets will land 120 units to the right, and 80 pixels down. . Let&#39;s also pretend that they will be normally distributed around this point, with a standard deviation of 85 pixels in each axis due to the wind/humidity/our terrible aim. . This is our generative model, and we use it to generate some fake data. We will later use the same model construct, and try and infer the x_offset and y_offset. . Let&#39;s go ahead and randomly generate the results of 5 shots at the target. . x_offset = 120 #units/pixels y_offset = -80 #units/pixels standard_deviation = 85 #units/pixels num_samples = 5 x_observed = np.random.normal(x_offset,standard_deviation,5) y_observed = np.random.normal(y_offset,standard_deviation,5) . img = Image.open(&#39;data/2020-07-02-On-Target-With-PyMC3/1000px-10_m_Air_Rifle_target.svg.png&#39;) plt.imshow(img) plt.scatter(x_observed+500,500-y_observed,alpha=0.9,s = 1000) plt.grid() plt.show() . Now, given we have observed this target. What adjustments should we make in order to improve our aim? . As part of Bayesian analysis, we need to provide a prior distribution, which tells us what plausible values of x_offset and y_offset could be. . Based on the impact locations, PyMC3 will try to infer potential values x_offset and y_offset, which are the adjustments we need to make to our rifle. . The beauty of Baysian analysis is that we don&#39;t get a single value as a result, but a distribution of values. This allows us to understand how certain we can be about the results. . with pm.Model() as model: #Set up our model x_offset = pm.Normal(&#39;x_offset&#39;,mu = 0, sigma=250) y_offset = pm.Normal(&#39;y_offset&#39;,mu = 0, sigma=250) standard_deviation = pm.HalfNormal(&#39;standard_deviation&#39;,sigma=200) impact_x = pm.Normal(&#39;impact_x&#39;, mu = x_offset, sigma = standard_deviation, observed = x_observed) impact_y = pm.Normal(&#39;impact_y&#39;, mu = y_offset, sigma = standard_deviation, observed = y_observed) . Now that we have finished setting up our model, we can use Markov Chain Monte-Carlo (MCMC) to infer what x_offset and y_offset could be. . with pm.Model() as model: #The magic line that trace = pm.sample(draws=10_000, tune=1_000) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [standard_deviation, y_offset, x_offset] Sampling 2 chains, 1 divergences: 100%|██████████| 22000/22000 [00:09&lt;00:00, 2309.14draws/s] There was 1 divergence after tuning. Increase `target_accept` or reparameterize. . Results . Now it&#39;s time to look at the results, PyMC3 provides a number of options for understanding them. . Let&#39;s start with a numerical summary. From this we can see that mean value of x_offset is 65.6 and y_offset is -109.7. From this, we can see that our best guess of where we need to aim is 65.6 units to the left, and -109.7 units up. . pm.summary(trace) . mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . x_offset 65.645 | 50.662 | -36.013 | 156.687 | 0.420 | 0.327 | 14566.0 | 12024.0 | 15361.0 | 10949.0 | 1.0 | . y_offset -109.744 | 50.016 | -203.327 | -13.457 | 0.475 | 0.336 | 11107.0 | 11107.0 | 11512.0 | 9532.0 | 1.0 | . standard_deviation 109.028 | 31.722 | 59.562 | 165.361 | 0.324 | 0.235 | 9578.0 | 9085.0 | 10639.0 | 11167.0 | 1.0 | . A traceplot is a useful way to diagnose what is going on. I&#39;m yet to find a really good tutorial on how to interpret it, but here are a few key points that I&#39;ve picked up. . Left Hand Column: Curves should closely overlap. If they don&#39;t, then it means that you can&#39;t rely on the results. . | Right Hand Column: The chart should look like a &quot;Fuzzy Caterpillar&quot;. This means that you are effectively exploring the parameter space. . | . pm.traceplot(trace); . The posterior plot tells you what values your parameters are likely to have. For example, according to our model, there is a 94% chance that x_offset is between -36 and 157.  . pm.plot_posterior(trace); . Finally, because we have two variables, we can plot them together, and understand their joint distribution using Seaborn. . plot = sns.jointplot(trace[:][&#39;x_offset&#39;]+500, trace[:][&#39;y_offset&#39;]+500, kind=&quot;hex&quot;); plt.show() . Putting it all together, we can visualise the potential locations (in red) for the centroid of the actual location of where the pellets will land. With only 5 samples observed, there is a huge amount of uncertainty. As the number of samples increases, this uncertainty would decrease. . We can also see that the green point, which marks the true offset is within this distribution of red potential locations. . plt.imshow(img) plt.scatter(trace[:][&#39;x_offset&#39;]+500,500-trace[:][&#39;y_offset&#39;],alpha=0.5,s = 1,color=&#39;r&#39;) plt.scatter(x_observed+500,500-y_observed,alpha=0.9,s = 1000) plt.scatter(120+500,500--80,alpha=0.9,s = 1000,color=&#39;g&#39;) plt.grid() plt.show() . No handles with labels found to put in legend. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/pymc3/computer%20vision/2020/07/02/On-Target-With-PyMC3.html",
            "relUrl": "/bayesian/pymc3/computer%20vision/2020/07/02/On-Target-With-PyMC3.html",
            "date": " • Jul 2, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Visualising PUBG Deaths with Datashader",
            "content": "While browsing Kaggle, I cam across this interesting dataset, and I thought it would form the basis for some interesting blog posts. . The dataset contains 65M player deaths, from 720,000 different matches, from PlayerUnknown&#39;s Battlegrounds (PUBG), a wildly popular online game. . An Introduction to PUBG . Wikipedia sums up the aim of the game pretty well: . &quot;In the game, up to one hundred players parachute onto an island and scavenge for weapons and equipment to kill others while avoiding getting killed themselves. The available safe area of the game&#39;s map decreases in size over time, directing surviving players into tighter areas to force encounters. The last player or team standing wins the round.&quot; . But for something bit less dry, but just as accurate, there is this video on Youtube. . Data preprocessing . First off, let&#39;s load some of the libraries we will later need. . import glob import pandas as pd import datashader as ds import datashader.transfer_functions as tf import numpy as np import matplotlib.pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [15, 15] . Bad key &#34;text.kerning_factor&#34; on line 4 in /opt/anaconda3/envs/PyMC3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle. You probably need to get an updated matplotlibrc file from https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template or from the matplotlib source distribution . The dataset itself comes in a number of different .csv files, which we will load, and concatentate. . def load_deaths(): li = [] for filename in glob.glob(&quot;/Users/cooke_c/Documents/Blog_Staging/PUBG/9372_13466_bundle_archive/deaths/*.csv&quot;): df = pd.read_csv(filename) df = df.drop([&#39;match_id&#39;,&#39;victim_placement&#39;,&#39;killed_by&#39;,&#39;killer_name&#39;,&#39;killer_placement&#39;,&#39;killer_position_x&#39;,&#39;killer_position_y&#39;,&#39;victim_name&#39;],axis=&#39;columns&#39;) li.append(df) df = pd.concat(li, axis=0, ignore_index=True) return(df) . deaths_df = load_deaths() . Matches in PUBG are limited in time to approximately 32.5 minutes. Let&#39;s create a new categorical variable called &quot;phase&quot;. It will represent which of the following match phases a player died in: . Early (0-10m) (Lime Green points) | Mid Phase (10-25m) (Cyan points) | Late Phase (&gt;25m) (Purple points) | def create_phase_category(deaths_df): conditions = [ (1*60&lt;deaths_df.time) &amp; (deaths_df.time&lt;10*60), (10*60&lt;deaths_df.time) &amp; (deaths_df.time&lt;25*60), (25*60&lt;deaths_df.time)] choices = [&#39;early&#39;, &#39;mid&#39;, &#39;late&#39;] deaths_df[&#39;phase&#39;] = np.select(conditions, choices, default=&#39;very_early&#39;) deaths_df[&#39;phase&#39;] = deaths_df[&#39;phase&#39;].astype(&#39;category&#39;) return(deaths_df) . deaths_df = create_phase_category(deaths_df) . Datashader . Now, this is where the fun begins. . Datashader is a highly efficient Python library for visualising massive amount of data. . Taking Pandas dataframes as inputs, Datashader aggregates the data to form visualisations. . There are 3 key components that we use to generate our visualisation: .  Defining a canvas. It&#39;s going to be 4,000 by 4,000 pixels. The data range we want to visualise is 800,000 by 800,000. | cvs = ds.Canvas(plot_width=4_000, plot_height=4_000, x_range=[0,800_000],y_range=[0,800_000]) . We want to aggregate data from deaths_df, using the &#39;victim_position_x&#39; variable as the x coordinate and &#39;victim_position_y&#39; as the y coordinate. Effectively, we are computing a seperate 2D histogram for each category (game phase). . agg = cvs.points(deaths_df, &#39;victim_position_x&#39;, &#39;victim_position_y&#39;,ds.count_cat(&#39;phase&#39;)) . | We visualise our 2D histogram, coloring each bin/pixel according to our color map. We also use histogram equalization (how=&#39;eq_hist&#39;). . img = tf.shade(agg, color_key=color_key, how=&#39;eq_hist&#39;) . | This post is heavily inspired by this example, which is a lot more detailed about the pipeline involved. . def visualise_with_datashader(deaths_df): color_key = {&#39;very_early&#39;:&#39;black&#39;, &#39;early&#39;:&#39;lime&#39;, &#39;mid&#39;:&#39;aqua&#39;, &#39;late&#39;:&#39;fuchsia&#39;} cvs = ds.Canvas(plot_width=4_000, plot_height=4_000, x_range=[0,800_000],y_range=[0,800_000]) agg = cvs.points(deaths_df,&#39;victim_position_x&#39;,&#39;victim_position_y&#39;,ds.count_cat(&#39;phase&#39;)) img = tf.shade(agg, color_key=color_key, how=&#39;eq_hist&#39;) img = tf.set_background(img,&quot;black&quot;, name=&quot;Black bg&quot;) return(img) . One minor details, is that we need to invert the y coordinates we want to render, to match the coordinate system used for the game maps. . deaths_df.victim_position_y = 800_000 - deaths_df.victim_position_y . Erangel . Early (0-10m) (Lime Green points) | Mid Phase (10-25m) (Cyan points) | Late Phase (&gt;25m) (Purple points) | erangel_df = deaths_df[deaths_df.map==&#39;ERANGEL&#39;] num_points = erangel_df.shape[0] print(f&#39;Total points : {num_points}&#39;) img = visualise_with_datashader(erangel_df) ds.utils.export_image(img=img,filename=&#39;Erangel&#39;, fmt=&quot;.png&quot;); . Total points : 52964245 . . Miramar . Early (0-10m) (Lime Green points) | Mid Phase (10-25m) (Cyan points) | Late Phase (&gt;25m) (Purple points) | miramar_df = deaths_df[deaths_df.map==&#39;MIRAMAR&#39;] num_points = miramar_df.shape[0] print(f&#39;Total points : {num_points}&#39;) img = visualise_with_datashader(miramar_df) ds.utils.export_image(img=img,filename=&#39;Miramar&#39;, fmt=&quot;.png&quot;); . Total points : 11622838 . . Analysis . Let&#39;s finish by taking a closer look at the lower part of the Erangel map. . We can see 3 different phases of the game, the early phase in green, the mid phase in cyan, and the later phase in purple. . I will confess to have had playing a grand total of 2 games of PUBG, before deciding that playing virtual hide and seek wasn&#39;t that fun. Hence I&#39;m far from an expert, but we can see some fairly clear patterns. . In the early phases of the game, deaths are in and around buildings, as players search for supplies and weapons. . In the middle phase, the deaths appear to be more spread over the map, with concentrations on roads and natural chokepoints like bridges. . In the last phase of the game, the decreasing size of the &quot;safe zone&quot; forces the players into a concentrated area for a final stand. The results in the constellation of purple dots spread across the map. . Erangel subsection 1 . Erangel subsection 2 . Miramar subsection .",
            "url": "https://cgcooke.github.io/Blog/datashader/visualisation/pubg/2020/05/31/Visualising-PUBG-Deaths-With-Datashader.html",
            "relUrl": "/datashader/visualisation/pubg/2020/05/31/Visualising-PUBG-Deaths-With-Datashader.html",
            "date": " • May 31, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "How long is the Blacklist? (With PyMC3)",
            "content": "An Introduction to PyMC3 . Last post, I took a first principles approach to computationally solving a Bayesian analysis problem. In practice, problems can be far more complex. Fortunately, sophisticated software libraries already exist that can be leveraged in order to solve equally complex problems. While there are a number of options available, my current favourite is PyMC3. . If you want to be inspired by what&#39;s possible, I strongly suggest checking out the fantastic set of examples on the PyMC3 site.   . Putting it into practice . Let&#39;s start as we did last time, by taking a sample of the first 10 elements from the blacklist. . import numpy as np import matplotlib.pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [15, 15] # E: the data y = np.array([52, 145, 84, 161, 85, 152, 47, 109, 16, 16, 106, 101, 64, 73, 57, 83, 88, 135, 119,120,121, 122, 42, 8, 8, 104, 112, 89, 82, 74, 114, 22, 12, 21, 21, 67, 71, 93, 94, 75, 7, 97, 117, 62, 87, 55, 11, 38, 80, 72, 43, 50, 86, 31, 108, 24, 24, 95, 132, 103, 77, 113, 78, 32, 32, 41, 18, 14, 14, 79, 66, 65, 81, 105, 53, 98, 98, 111, 163, 102, 34, 107, 59, 10, 61, 29, 46, 4, 4, 30, 37, 76, 44, 54, 90, 48, 13, 118, 100, 56, 63, 51, 68, 19, 25, 23, 13, 110, 26, 17, 33, 20, 124, 146, 147, 131, 91, 116, 58, 99, 160, 20, 20, 9, 6, 115, 69, 136, 92, 128, 60, 15, 27, 27, 151, 138, 130, 125, 162, 159, 3, 137, 155, 144, 126, 158, 149, 150]) . y_sample = y[0:10] plt.stem(np.sort(y_sample), use_line_collection=True) plt.show() . Now this is when things start getting intesting. Firstly going to lay out all the code we need to solve this problem, all 7 lines. . import pymc3 as pm model = pm.Model() with model: # prior - P(N): N ~ uniform(max(y), 500) N = pm.DiscreteUniform(&quot;N&quot;, lower=y.max(), upper=500) # likelihood - P(D|N): y ~ uniform(0, N) y_obs = pm.DiscreteUniform(&quot;y_obs&quot;, lower=0, upper=N, observed=y) trace = pm.sample(10_000, start={&quot;N&quot;: y.max()}) pm.plots.plot_posterior(trace) . Multiprocess sampling (2 chains in 2 jobs) Metropolis: [N] Sampling 2 chains, 0 divergences: 100%|██████████| 21000/21000 [00:02&lt;00:00, 9637.66draws/s] The number of effective samples is smaller than 10% for some parameters. . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1121d6f10&gt;], dtype=object) . Ok, so from this we are 94% confident that the answer (the length of the Blacklist) is in the range 163 to 166. . Line by Line. . Let&#39;s step through now line by line, to understand what&#39;s going on. . Import PyMC3. . import pymc3 as pm . | Create a PyMC3 model. . model = pm.Model() . | Create a context manager, so that magic can happen behind the scenes . with model: . | Create a distribution for $N$, our prior belief of the length of the Blacklist ($P(H)$). In layman&#39;s terms, we are saying that $N$ is equally liklely to any integer between the highest index we observe in our sample of data, and 500. The number 500 is arbitrary, ultimately the data we observe will &quot;wash out&quot; any assumptions we have made. . | N = pm.DiscreteUniform(&quot;N&quot;, lower=y.max(), upper=500) . Create our likelyhood function, $P(E|H)$. IE, given the data we have observed, how likely is any value of N? Again, we are using a discrete uniform distribution to model the probability of seeing any observed data point. | y_obs = pm.DiscreteUniform(&quot;y_obs&quot;, lower=0, upper=N, observed=y) . Sample the model. We are computationally finding $P(H|E)$. This is where the real magic occurs. In this case, we generate 10,000 samples, and store them in &#39;trace&#39;. | trace = pm.sample(10_000, start={&quot;N&quot;: y.max()}) . Plot the distribution of the parameters. This is our posterior, ($P(H|E)$).pm.plots.plot_posterior(trace) . | Conclusion . In conclusion, PyMC3 can be used to quickly and efficiently conduct Bayesian analysis. I hope to do some more examples in future posts, looking at other &#39;real world&#39; problems. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/pymc3/2020/05/20/How-Long-Is-The-Blacklist-With-PyMC3.html",
            "relUrl": "/bayesian/pymc3/2020/05/20/How-Long-Is-The-Blacklist-With-PyMC3.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "How long is the Blacklist?",
            "content": "Overview . I&#39;m in the process of learning French, and as part of my language learning journey, I&#39;m slowly working my way through The Blacklist on Netflix. . The premise of the show involves an antihero played by James Spader working his way through a list of nefarious characters, taking them down one at a time. . Out of idle intellectual curiosity, I was curious about how many names are on the list, based on the sample of names that are given. It&#39;s actually an interesting problem we can solve using Bayesian analysis, more commonly known as the &quot;German Tank Problem&quot;, or the &quot;Locomotive Problem&quot;. . It&#39;s also an interesting into into Bayesian analysis. We are trying to infer an unknown quantity (The number of people on the list), while only being able to observe samples from this distribution. . Bayes in Brief . There are many other, far better blog posts which cover this topic, in particular this one. . Now, with that out of the way, let&#39;s talk about the problem at hand, and how we can solve it with Bayesian analysis. . We have some evidence, $E$, which is what we actually obsever. In this case, it&#39;s the index in the black list that we find out each episode. IE, in first episode, it&#39;s entry number $52$. . Then we have what we want to know, an estimate of the length of the Blacklist, given the evidence we have observed. This is the probability of a hypothesis, given the evidence $P(H|E)$. It&#39;s a probability, because we don&#39;t know for sure how long the blacklist is, given what we have observed, but we can work out how confident we can be about each potential option. . Now, given that $P(H|E)$ is what we want, we can use Baye&#39;s Formula to find it. . $P(H|E) = frac{P(E|H) P(H)}{P(E)}$ . We need three more pieces of the puzzle, $P(E|H)$, $P(H)$ and $P(E)$. . Now, $P(E|H)$ is the probability of the Evidence, given the Hypothesis. This term is also known as the Likelyhood, and is the term which I found most confusing while initially learning about Bayesian analysis. In this case, it&#39;s almost confusingly simple. . $P(H)$ is our prior belief about what the possible hypothesis could be. In this case, it&#39;s long long we plausibly think the Blacklist could be. In practice, we know it has to be as long as any of the entries we have observed. Much of the criticism of Baysian analysis comes from the fact we can inject our own beliefs into the process. In this case, I don&#39;t have a strong belief about the length of the Blacklist, but it&#39;s reasonable to assume that it&#39;s probably less than 200 entries long. . Let&#39;s assume that the Blacklist is of length $N$. Let&#39;s also assume that any given entry is as likely to be the subject of each episode as any other. Then the probability of drawing any given number, is $ frac{1}{N}$. This is the likelihood. Also note the subtle point that obviously that N has to be at least as large as the number we have drawn. . Finally, we have $P(E)$, which is the probability of the evidence. This is just a normalisation factor, which is often ignored. . Let&#39;s get down to business, fortunately Wikipedia has all the data we need. . No. overall No. in season Title Blacklist guide Directed by Written by Original air date US viewers(millions) . 1 | 1 | &quot;Pilot&quot; | No. 52 | Joe Carnahan | Jon Bokenkamp | September 23, 2013 | 12.58[10] | . 2 | 2 | &quot;The Freelancer&quot; | No. 145 | Jace Alexander | Jon Bokenkamp | September 30, 2013 | 11.35[11] | . 3 | 3 | &quot;Wujing&quot; | No. 84 | Michael Watkins | Lukas Reiter | October 7, 2013 | 11.18[12] | . 4 | 4 | &quot;The Stewmaker&quot; | No. 161 | Vince Misiano | Patrick Massett &amp; John Zinman | October 14, 2013 | 10.93[13] | . 5 | 5 | &quot;The Courier&quot; | No. 85 | Nick Gomez | John C. Kelley | October 21, 2013 | 10.44[14] | . 6 | 6 | &quot;Gina Zanetakos&quot; | No. 152 | Adam Arkin | Wendy West | October 28, 2013 | 10.51[15] | . 7 | 7 | &quot;Frederick Barnes&quot; | No. 47 | Michael Watkins | J. R. Orci | November 4, 2013 | 10.34[16] | . 8 | 8 | &quot;General Ludd&quot; | No. 109 | Stephen Surjik | Amanda Kate Shuman | November 11, 2013 | 10.69[17] | . 9 | 9 | &quot;Anslo Garrick&quot; | No. 16 | Joe Carnahan | Story by : Joe Carnahan &amp; Jason George Teleplay by : Joe Carnahan | November 25, 2013 | 10.96[18] | . 10 | 10 | &quot;Anslo Garrick Conclusion&quot; | No. 16 | Michael Watkins | Lukas Reiter &amp; J. R. Orci | December 2, 2013 | 11.67[19] | . 11 | 11 | &quot;The Good Samaritan&quot; | No. 106 | Dan Lerner | Brandon Margolis &amp; Brandon Sonnier | January 13, 2014 | 9.35[20] | . 12 | 12 | &quot;The Alchemist&quot; | No. 101 | Vince Misiano | Anthony Sparks | January 20, 2014 | 8.83[21] | . 13 | 13 | &quot;The Cyprus Agency&quot; | No. 64 | Michael Watkins | Lukas Reiter | January 27, 2014 | 10.17[22] | . 14 | 14 | &quot;Madeline Pratt&quot; | No. 73 | Michael Zinberg | Jim Campolongo | February 24, 2014 | 11.18[23] | . 15 | 15 | &quot;The Judge&quot; | No. 57 | Peter Werner | Jonathan Shapiro &amp; Lukas Reiter | March 3, 2014 | 11.01[24] | . 16 | 16 | &quot;Mako Tanida&quot; | No. 83 | Michael Watkins | Story by : Joe Carnahan Teleplay by : John Eisendrath &amp; Jon Bokenkamp &amp; Patrick Massett &amp; John Zinman | March 17, 2014 | 10.97[25] | . 17 | 17 | &quot;Ivan&quot; | No. 88 | Randy Zisk | J.R. Orci &amp; Amanda Kate Shuman | March 24, 2014 | 10.80[26] | . 18 | 18 | &quot;Milton Bobbit&quot; | No. 135 | Steven A. Adelson | Daniel Voll | March 31, 2014 | 11.39[27] | . 19 | 19 | &quot;The Pavlovich Brothers&quot; | Nos. 119-122 | Paul Edwards | Elizabeth Benjamin | April 21, 2014 | 11.24[28] | . 20 | 20 | &quot;The Kingmaker&quot; | No. 42 | Karen Gaviola | J. R. Orci &amp; Lukas Reiter | April 28, 2014 | 10.85[29] | . 21 | 21 | &quot;Berlin&quot; | No. 8 | Michael Zinberg | John Eisendrath &amp; Jon Bokenkamp | May 5, 2014 | 10.47[30] | . 22 | 22 | &quot;Berlin Conclusion&quot; | No. 8 | Michael Watkins | Story by : Richard D&#39;Ovidio Teleplay by : John Eisendrath &amp; Jon Bokenkamp &amp; Lukas Reiter &amp; J. R. Orci | May 12, 2014 | 10.44[31] | . Putting it into practice . Now, let&#39;s look at all the episodes, from the seasons that have been aired: . import numpy as np import matplotlib.pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [15, 15] # E: the data y = np.array([52, 145, 84, 161, 85, 152, 47, 109, 16, 16, 106, 101, 64, 73, 57, 83, 88, 135, 119,120,121, 122, 42, 8, 8, 104, 112, 89, 82, 74, 114, 22, 12, 21, 21, 67, 71, 93, 94, 75, 7, 97, 117, 62, 87, 55, 11, 38, 80, 72, 43, 50, 86, 31, 108, 24, 24, 95, 132, 103, 77, 113, 78, 32, 32, 41, 18, 14, 14, 79, 66, 65, 81, 105, 53, 98, 98, 111, 163, 102, 34, 107, 59, 10, 61, 29, 46, 4, 4, 30, 37, 76, 44, 54, 90, 48, 13, 118, 100, 56, 63, 51, 68, 19, 25, 23, 13, 110, 26, 17, 33, 20, 124, 146, 147, 131, 91, 116, 58, 99, 160, 20, 20, 9, 6, 115, 69, 136, 92, 128, 60, 15, 27, 27, 151, 138, 130, 125, 162, 159, 3, 137, 155, 144, 126, 158, 149, 150]) . Now, potting the episodes in order. If we had to guess now, we would likely guess that the length of the Blacklist, is 163 episodes, or a few more. This is because we have seen entries on the list up to 163, and It looks like nearly every entry on the list has been crossed off. . plt.stem(np.sort(y), use_line_collection=True) plt.show() . Now let&#39;s say that we have a sample of only the first 10 episodes, what can we infer from this sample? . y_sample = y[0:10] plt.stem(np.sort(y_sample), use_line_collection=True) plt.show() . Let&#39;s start by defining our likelyhood function, $P(E|H)$. . def compute_likelyhood(i,observed): if i &lt; observed: likelyhood = 0 else: likelyhood = 1.0/i return(likelyhood) . Now let&#39;s define the prior belif $P(H)$ of what we think the length of the Blacklist could be. . prior = np.ones(200) prior /= prior.sum() . This prior is quite broad, and it&#39;s also uniform. We aren&#39;t really imputing any of our human judgement about how long the list is. . Now, let&#39;s by start by computing the likelyhood for each possilble list length, after we have watched the first episode, and seen person #52 crossed off the list. . posterior = np.zeros(200) observed = 52 for i in range(0,200): posterior[i] = prior[i] * compute_likelyhood(i,observed) posterior /= posterior.sum() plt.stem(posterior) plt.xlabel(&#39;List length&#39;) plt.ylabel(&#39;Confidence&#39;) #plt.xlim(150,175) plt.show() . /Users/cooke_c/.local/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the &#34;use_line_collection&#34; keyword argument to True. import sys . Now we have have have this belief state, having seen the first episode, we can recursively use it as a prior, and update it each time we see an episode: . import matplotlib.pylab as pl posterior = np.zeros(200) episode_number = 1 for observed in y_sample: for i in range(1,200): posterior[i] = prior[i] * compute_likelyhood(i,observed) posterior /= posterior.sum() plt.plot(posterior,alpha=0.35,label=&#39;Episode: &#39;+str(episode_number ), color=pl.cm.jet(episode_number/10.0)) episode_number+=1 prior=posterior plt.legend() plt.xlabel(&#39;List length&#39;) plt.ylabel(&#39;Confidence&#39;) plt.xlim(155,175) plt.show() . From the above, we can see that as we see more data, our confidence in the length of the list increases. Let&#39;s look at the cumulative probability for different list lengths: . plt.plot(np.cumsum(posterior)) plt.grid() plt.xlim(155,185) plt.xlabel(&#39;List length&#39;) plt.ylabel(&#39;Cumulative Confidence&#39;) plt.show() . From this, we can see that we can be approximately 50% confident that the length of Blacklist is between 160 and 170 entries long, after we have seen the first 10 episodes. . Conclusion . We have taken a lighting tour of how Bayesian analysis can allow us to infer from what we can observe, a quantity we are interested in, when there is a random process involved. . In practice, Bayesian analysis isn&#39;t conducted from first principles, but instead uses a purpose built library. Next time, I want to go and demonstrate how to do this analysis using PyMC3. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/2020/04/29/How-Long-Is-The-Blacklist.html",
            "relUrl": "/bayesian/2020/04/29/How-Long-Is-The-Blacklist.html",
            "date": " • Apr 29, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "R From Vanishing points",
            "content": "Ok, so we now have 3 different vanishing points. Using these, let&#39;s try and use them to gain some insights into the camera&#39;s relationship with the scene. . In this post, I&#39;m going to focus on how to determine the camera&#39;s rotation with respect to the scene, using 3 orthogonal vanishing points present in the scene. . Note: Hartley &amp; Zisserman put it best, &quot;Vanishing points are images of points at infinity, and provide orientation (attitude) in- formation in a similar manner to that provided by the fixed stars.&quot; This is a great video to watch for another perspective on this problem. . Ok, now let&#39;s import some useful libraries, and visualise the scene. . from PIL import Image import matplotlib.pyplot as plt import numpy as np np.set_printoptions(precision=3) plt.rcParams[&#39;figure.figsize&#39;] = [15, 15] . img = Image.open(&#39;data/2020-04-10-Finding-Vanishing-Points/csgo-dust2-mid.jpeg&#39;) plt.imshow(img) plt.show() . Now is a good time to pause for a second, and talk about homogeneous coordinates. . Let&#39;s assume we are comfortable with the Euclidean coordinate system (X,Y,Z). Let&#39;s pretend that we have a point, $p$ at: . $X = 5$ | $Y = 7$ | $Z = 4$ | . I.e. . $p = begin{bmatrix} 5 7 4 end{bmatrix}$ . Let&#39;s just go out on a limb. Let&#39;s take $p$, and multiply each component by some amount ($k$). In addition, let&#39;s append $k$ as an extra dimension. . $p = begin{bmatrix} 5k 7k 4k k end{bmatrix}$ . This is the homogeneous representation of $p$. . Let&#39;s now take a point on the $Z$ axis, perhaps $Z=1$ for example: . $p = begin{bmatrix} 0 0 1 end{bmatrix}$ . or . $p = begin{bmatrix} 0k 0k 1k k end{bmatrix}$ . or . If k is 0, then if $Z$ is any other positive value (apart from 0), then it will be a point infinitely far away on the Z axis. This is also the Z vanishing point. . $p = begin{bmatrix} 0 0 1 0 end{bmatrix}$ . From my other post, we can see that we can map from the points in 3D space ($X$) to points in 2D space ($x$) using the matrix $P$. . begin{equation*} x = PX end{equation*}The matrix $P$ in turn, consists of 3 parts. . A intrinsic camera matrix $K$ | A Rotation matrix $R$ | A translation matrix $t$ | begin{equation*} P = K[R | t] end{equation*}Assuming that the camera is free from radial distortion, then the Z vanishing point can be found as follows. . $v_z = K begin{bmatrix} R_1 &amp; R_2 &amp; R_3 &amp; | &amp; t end{bmatrix} begin{bmatrix} 0 0 1 0 end{bmatrix} $ . Looking carefully, we realise that the we can knock out everything but the column $R_3$ of the rotation matrix $R$. . In the previous post, we already found the matrix $K$. . $K = begin{bmatrix} 728 &amp; 0 &amp; 1327 0 &amp; 728 &amp; 706 0 &amp; 0 &amp; 1 end{bmatrix}$ . Now, we can find $R_3$ as follows: . $R_3 = frac{K^{-1} v_z}{|K^{-1} v_z|} $ . Let&#39;s put this into practice. . Now, let&#39;s make some assumptions. . World X axis pointing right (Red arrow) | World Y axis pointing into the scene (Green arrow) | World Z axis pointing up (Blue arrow) | . . . vanishing_points = {&#39;VP1&#39;: [1371.892, 630.421], &#39;VP2&#39;: [-10651.54, 536.681], &#39;VP3&#39;: [1272.225, 7683.02 ]} K = np.array([[7.276e+02,0.000e+00,1.327e+03], [6.236e-14,7.276e+02,7.060e+02], [1.218e-16,0.000e+00,1.000e+00]]) def make_homogeneous(x): x_homogeneous = np.array([x[0],x[1],1]) return(x_homogeneous) v_x_h = make_homogeneous(vanishing_points[&#39;VP2&#39;]) v_y_h = make_homogeneous(vanishing_points[&#39;VP1&#39;]) v_z_h = make_homogeneous(vanishing_points[&#39;VP3&#39;]) K_inv = np.linalg.inv(K) R_2 = np.dot(K_inv,v_y_h)/np.linalg.norm(np.dot(K_inv,v_y_h)).T R_3 = -1 * np.dot(K_inv,v_z_h)/np.linalg.norm(np.dot(K_inv,v_z_h)).T R_1 = np.cross(R_2,R_3) R = np.vstack([R_1,R_2,R_3]) print(R) . [[ 0.998 0.014 -0.06 ] [ 0.061 -0.103 0.993] [ 0.008 -0.995 -0.104]] . This correlates well with what we were expecting. The rotation matrix takes us from the world coordinate system to the camera&#39;s coordinate system. In particular, we can see that the matrix $R$ maps (approximately): . World X axis to Camera X axis | World Y axis to Camera Negative Z axis | World Z axis to Camera Y axis | from mpl_toolkits.mplot3d import Axes3D fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.plot(xs=[0,1], ys=[0,0], zs = [0,0],color=&#39;r&#39;,label=&#39;X World&#39;) ax.plot(xs=[0,0], ys=[0,1], zs = [0,0],color=&#39;g&#39;,label=&#39;Y World&#39;) ax.plot(xs=[0,0], ys=[0,0], zs = [0,1],color=&#39;b&#39;,label=&#39;Z World&#39;) plt.legend() plt.show() fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.plot(xs=[0,R[0,0]], ys=[0,R[1,0]], zs = [0,R[2,0]],color=&#39;r&#39;,label=&#39;X Camera&#39;) ax.plot(xs=[0,R[0,1]], ys=[0,R[1,1]], zs = [0,R[2,1]],color=&#39;g&#39;,label=&#39;Y Camera&#39;) ax.plot(xs=[0,R[0,2]], ys=[0,R[1,2]], zs = [0,R[2,2]],color=&#39;b&#39;,label=&#39;Z Camera&#39;) plt.legend() plt.show() .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/linear%20algebra/2020/04/12/R-From-Vanishing-Points.html",
            "relUrl": "/computer%20vision/linear%20algebra/2020/04/12/R-From-Vanishing-Points.html",
            "date": " • Apr 12, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "K From Vanishing points",
            "content": "from PIL import Image import matplotlib.pyplot as plt import numpy as np import scipy.linalg np.set_printoptions(precision=3) plt.rcParams[&#39;figure.figsize&#39;] = [15, 15] . img = Image.open(&#39;data/2020-04-10-Finding-Vanishing-Points/csgo-dust2-mid.jpeg&#39;) plt.imshow(img) plt.show() . vanishing_points = {&#39;VP1&#39;: [1371.892, 630.421], &#39;VP2&#39;: [-10651.54 , 536.681], &#39;VP3&#39;: [1272.225, 7683.02 ]} . plt.imshow(img) for point_name in [&#39;VP1&#39;,&#39;VP2&#39;,&#39;VP3&#39;]: vp = vanishing_points[point_name] plt.scatter(vp[0],vp[1],label=point_name) plt.legend() plt.show() . Ok, so we now have 3 different vanishing points. Using these, let&#39;s try and use them to gain some insights the cameras&#39;s relationship with the scene. . Hartley &amp; Zisserman put it best, &quot;Vanishing points are images of points at infinity, and provide orientation (attitude) in- formation in a similar manner to that provided by the fixed stars.&quot; . Hartley &amp; Zisserman also provide an algorithm (Example 8.27, page 226) to extract the camera calibration matrix K from 3 mutually orthogonal vanishing points. . Let&#39;s go and implement it in practice. . Some algebra . H&amp;Z propose a matrix $ omega$ (omega) which captures the following relationship between the different vanishing points. . $v^T_i omega v_j = 0$ . Where: . $ omega = begin{bmatrix} w_1 &amp; 0 &amp; w_2 0 &amp; w_1 &amp; w_3 w_2 &amp; w_3 &amp; w_4 end{bmatrix}$ . And: . $ v_j = begin{bmatrix} x_1 y_1 1 end{bmatrix}$ . $v_i = begin{bmatrix} x_2 y_2 1 end{bmatrix}$ . If we can find this matrix $ omega$, then we can find the camera calibration matrix, if we make some assumptions: . Zero Skew | Square Pixels | From H&amp;Z, we have: &quot;K is obtained from $ omega$ by Cholesky factorization of omega, followed by inversion.&quot; . For good practice, we can also normalize the matrix, so that lower right value $K_{22}$ is 1. In python: . K = np.linalg.inv(np.linalg.cholesky(omega)).T K/=K[2,2] . Working backwards, we are faced with the task of finding $ omega$ . Multiplying through, we find that: . $ v^T_i omega v_j = x_2(w_1 x_1 + w_2) + y_2(w_1 y_1 + w_3) + w_2 x_1 + w_3 y_1 + w_4$ . Factorising: . $ v^T_i omega v_j= w_1(x_2 x_1 + y_2 y_1) + w_2(x_2 + x_1) + w_3(y_2 + y_1) + w_4$ . Great, we can now find all the coefficients we need for our matrix, from each pair of vanishing points. . We have 3 pairs of vanishing points: . 1 &amp; 2 | 2 &amp; 3 | 3 &amp; 1 | From each pair we can find a new set of values for $w_1$ to $w_4$. . Stacking them all on top each other, we end up with the matrix $A$. . $A = begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13} &amp; w_{14} w_{21} &amp; w_{22} &amp; w_{23} &amp; w_{24} w_{31} &amp; w_{32} &amp; w_{33} &amp; w_{34} end{bmatrix}$ . In Python: . def generate_A(vanishing_points): A = [] for (point_name_1, point_name_2) in [(&#39;VP1&#39;,&#39;VP2&#39;),(&#39;VP2&#39;,&#39;VP3&#39;),(&#39;VP3&#39;,&#39;VP1&#39;)]: vp1 = vanishing_points[point_name_1] vp2 = vanishing_points[point_name_2] x1,y1 = vp1 x2,y2 = vp2 w1 = x2*x1 + y2*y1 w2 = x2 + x1 w3 = y2 + y1 w4 = 1 A.append([w1,w2,w3,w4]) A = np.array(A) return(A) . def generate_A(vanishing_points): A = [] for (point_name_1, point_name_2) in [(&#39;VP1&#39;,&#39;VP2&#39;),(&#39;VP2&#39;,&#39;VP3&#39;),(&#39;VP3&#39;,&#39;VP1&#39;)]: vp1 = vanishing_points[point_name_1] vp2 = vanishing_points[point_name_2] x1,y1 = vp1 x2,y2 = vp2 w1 = x2*x1 + y2*y1 w2 = x2 + x1 w3 = y2 + y1 w4 = 1 A.append([w1,w2,w3,w4]) A = np.array(A) return(A) def compute_K(A): w = scipy.linalg.null_space(A).ravel() w1 = w[0] w2 = w[1] w3 = w[2] w4 = w[3] omega = np.array([[w1,0,w2], [0,w1,w3], [w2,w3,w4]]) K = np.linalg.inv(np.linalg.cholesky(omega)).T K/=K[2,2] return(K) A = generate_A(vanishing_points) K = compute_K(A) . print(K) . [[7.276e+02 0.000e+00 1.327e+03] [6.236e-14 7.276e+02 7.060e+02] [1.218e-16 0.000e+00 1.000e+00]] . ### The Calibration Matrix . So now we have the calibration matrix $K$, which gives us 3 seprate pieces of information. . The Focal length in pixels: $K_{11}$ or $K_{22}$ (728) | The x coordinate of the camera optical center: $K_{13}$ (1327) | The y coordinate of the camera optical center $K_{23}$ (706) | $K = begin{bmatrix} 728 &amp; 0 &amp; 1327 0 &amp; 728 &amp; 706 0 &amp; 0 &amp; 1 end{bmatrix}$ . plt.scatter(2560/2.0,1600/2.0,color=&#39;G&#39;) plt.scatter(K[0,2],K[1,2],color=&#39;R&#39;) plt.imshow(img) plt.show() .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/linear%20algebra/2020/04/11/K-From-Vanishing-Points.html",
            "relUrl": "/computer%20vision/linear%20algebra/2020/04/11/K-From-Vanishing-Points.html",
            "date": " • Apr 11, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Finding Vanishing Points",
            "content": "Why are we interested in finding vanishing points? Because they allow us to quickly and easily estimate key parameters about a camera. For example, it&#39;s focal length, optical center and rotation in 3D space. . However, the first step is to be able to identify the location of the vanishing points in an image. . Important: The projection of parallel lines in 3D space, intersect at a vanishing point in 2D space. Sometimes it can be easy to find the location of vanishing point in an image, for example when two objects in the real world are very long and quite close together, for example train tracks. Often it&#39;s a lot more challenging. . Let&#39;s use a semi-realistic image, I&#39;ve chosen a rendered image from a video game (Counter Strike). This means that we can, for the moment ignore some other factors like Radial Distortion. . Let&#39;s start, as always by importing what we will need later. . from PIL import Image import matplotlib.pyplot as plt import json import numpy as np plt.rcParams[&#39;figure.figsize&#39;] = [15, 15] . Now let&#39;s visualise the secene. . img = Image.open(&#39;data/2020-04-10-Finding-Vanishing-Points/csgo-dust2-mid.jpeg&#39;) plt.imshow(img) plt.show() . Maximum Likelihood Estimate . Straight lines in 3D space are mapped to straight lines in 2D space by an ideal camera. We can automate the detection of straight lines using algorithms like a hough transform. However I&#39;ve used an external program to manual annotate the straight lines in the image. I&#39;ve annotated 3 sets of lines that correspond to one of 3 different vanishing points. . Often, we use a manhattan world assumption, that is assuming that there are 3 different sets of orthogonal parallel lines present in the world. We assume that there are 2 orthogonal lines that form the ground plane. In an image, each set of parallel lines that lies on this plane online, which in practice is the horizon. . . Now, let&#39;s use linear algebra and least mean squares (and the magic of Stack Overflow) to find one of the vanishing points.  . JSON = json.loads(open(&#39;data/2020-04-10-Finding-Vanishing-Points/csgo-dust2-mid.json&#39;,&#39;r&#39;).read()) . def intersect_multiple_lines(P0,P1): &quot;&quot;&quot;P0 and P1 are NxD arrays defining N lines. D is the dimension of the space. This function returns the least squares intersection of the N lines from the system given by eq. 13 in http://cal.cs.illinois.edu/~johannes/research/LS_line_intersect.pdf. &quot;&quot;&quot; # generate all line direction vectors n = (P1-P0)/np.linalg.norm(P1-P0,axis=1)[:,np.newaxis] # normalized # generate the array of all projectors projs = np.eye(n.shape[1]) - n[:,:,np.newaxis]*n[:,np.newaxis] # I - n*n.T # generate R matrix and q vector R = projs.sum(axis=0) q = (projs @ P0[:,:,np.newaxis]).sum(axis=0) # solve the least squares problem for the # intersection point p: Rp = q p = np.linalg.lstsq(R,q,rcond=None)[0] return(p) . def load_line_data(point_name): P0 = [] P1 = [] for shape in JSON[&#39;shapes&#39;]: points = shape[&#39;points&#39;] if shape[&#39;label&#39;] == point_name: P0.append(points[0]) P1.append(points[1]) P0 = np.array(P0,dtype=np.float64) P1 = np.array(P1,dtype=np.float64) return(P0,P1) def find_vanishing_point(point_name): P0,P1 = load_line_data(point_name) p = intersect_multiple_lines(P0,P1).ravel() return(p) p = find_vanishing_point(point_name=&#39;VP1&#39;) . Now let&#39;s visualise the location of vanishing point 1. . plt.imshow(img) plt.scatter(p[0],p[1],color=&#39;r&#39;,label=&#39;Vanishing point #1&#39;) plt.legend() plt.xlim(0,2560) plt.ylim(1600,0) plt.show() . Monte Carlo Simulation . Ok, so this is a good start, but I&#39;m interested in how certain we are about the vanishing point. . For example, when I annotated the lines, I most likely made mistakes in the precise location of each point. We would expect that for shorter lines, this would have a bigger impact on the location of the vanishing point than for longer lines. . Let&#39;s do a monte carlo simulation, to find the distribution of possible vanishing points. . def monte_carlo_simulation(point_name): P0,P1 = load_line_data(point_name) point_error_magnitude = 1 vanishing_points = [] for i in range(0,1000): P0_stochastic = P0 + point_error_magnitude*np.random.randn(P0.shape[0],P0.shape[1]) P1_stochastic = P1 + point_error_magnitude*np.random.randn(P1.shape[0],P1.shape[1]) p = intersect_multiple_lines(P0_stochastic,P1_stochastic) vanishing_points.append(p) vanishing_points = np.asarray(vanishing_points) return(vanishing_points) point_name = &#39;VP1&#39; vanishing_points = monte_carlo_simulation(point_name) . Now let&#39;s visualise the distribution of points: . for p in vanishing_points: plt.scatter(p[0],p[1],color=&#39;k&#39;,alpha=0.1) plt.xlim(1350,1390) plt.ylim(640,620) plt.grid() plt.show() . For completeness, we can compute the standard deviation of the points in both the x &amp; y axis. . print(vanishing_points.std(axis=0).ravel()) . [3.5399997 2.03944019] . Voila, we have a standard deviation of ±3.5 pixels in the x direction, and ±2 pixels in the y direction. . Apendix . For the sake of completeness, let&#39;s compute the location of the other 3 vanishing points. . vanishing_points = {} for point_name in [&#39;VP1&#39;,&#39;VP2&#39;,&#39;VP3&#39;]: vanishing_points[point_name]= find_vanishing_point(point_name) plt.imshow(img) for point_name,color in [(&#39;VP1&#39;,&#39;g&#39;),(&#39;VP2&#39;,&#39;r&#39;),(&#39;VP3&#39;,&#39;b&#39;)]: vp = vanishing_points[point_name] print(point_name,vp) plt.scatter(vp[0],vp[1],color=color,label=point_name) plt.legend() plt.show() . VP1 [1371.89171088 630.42051773] VP2 [-10651.53961582 536.68080631] VP3 [1272.22463298 7683.01978252] . plt.imshow(img) for point_name,color in [(&#39;VP1&#39;,&#39;g&#39;),(&#39;VP2&#39;,&#39;r&#39;),(&#39;VP3&#39;,&#39;b&#39;)]: vanishing_points = monte_carlo_simulation(point_name) print(point_name, vanishing_points.std(axis=0).ravel()) for p in vanishing_points: plt.scatter(p[0],p[1],color=color,alpha=0.1) plt.show() . VP1 [3.29072559 2.01278422] VP2 [1706.16058495 32.11479688] VP3 [ 20.31361393 217.79742705] . We can see that there is a significant amount of uncertainty in the x component (±1728) of the 2nd vanishing point. This is because of the lack of good, long parallel lines running left/right across the image. .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/linear%20algebra/monte%20carlo%20simulation/2020/04/10/Finding-Vanishing-Points.html",
            "relUrl": "/computer%20vision/linear%20algebra/monte%20carlo%20simulation/2020/04/10/Finding-Vanishing-Points.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Inverse Radial Distortion",
            "content": "What is radial distortion? . Real cameras, with real optics suffer from radial distortion. Radial distortion is a common type of lens distortion). It&#39;s a deviation away from a perfect pinhole camera, where straight lines in 3D space are mapped to straight lines in 2D space. Let&#39;s define this distortion mathematically. . Defining a forward function . Radial distortion functions map points according to their distance (r) from the optical center of the lens. . We can model the function using a Taylor series: . begin{equation*} x_d = x(1 + k_1  r + k_2  r^2 + k_3 r^3) end{equation*} begin{equation*} y_d = y(1 + k_1  r + k_2 r^2 + k_3 r^3) end{equation*}IE, the function is described using 3 numbers, k_1, k_2 and k_3. . Firstly, let&#39;s import some things we need. . import numpy as np import matplotlib.pyplot as plt from scipy.optimize import least_squares %matplotlib inline plt.rcParams[&quot;figure.figsize&quot;] = (10,10) . Next let&#39;s use some distortion coefficients from a real lens, and plot how it maps points based on their radius from the optical center of the lens. . k_1 = -0.04436 k_2 = -0.35894 k_3 = 0.14944 r = np.linspace(0,1,1000) r_distorted = r*(1 + k_1 * r + k_2 * r**2 + k_3 * r**3) plt.xlabel(&#39;Initial R&#39;) plt.ylabel(&#39;Distorted R&#39;) plt.plot(r,r_distorted) plt.show() . To help understand this mapping, we can visualise the impact on a grid of straight lines. Note how lines are mapped to curves. . def distort_line(x,y,k_1,k_2,k_3): r = np.sqrt(x**2 + y**2) x_distorted = x*(1 + k_1 * r + k_2 * r**2 + k_3 * r**3) y_distorted = y*(1 + k_1 * r + k_2 * r**2 + k_3 * r**3) return(x_distorted,y_distorted) for y in np.linspace(-1,1,10): x = np.linspace(-1,1,1000) x_distorted,y_distorted = distort_line(x,y,k_1,k_2,k_3) plt.plot(x_distorted,y_distorted,color=&#39;k&#39;,alpha=0.8) for x in np.linspace(-1,1,10): y = np.linspace(-1,1,1000) x_distorted,y_distorted = distort_line(x,y,k_1,k_2,k_3) plt.plot(x_distorted,y_distorted,color=&#39;k&#39;,alpha=0.8) plt.xlim(-1,1) plt.ylim(-1,1) plt.show() . Finding an Inverse Function . Now it&#39;s time to find an inverse function, a function that will allow us to take points that have been distorted, and map them back to where they would have been, had the lens been free from distortion. . Unfortunately we can&#39;t algebraically find and an inverse function based on the forward function. However, we can find an inverse function through a process of optimization. . def undistort_point(undistortion_params,r_distorted): undistorted = r_distorted*(1 + undistortion_params[0] * r_distorted + undistortion_params[1] * r_distorted**2 + undistortion_params[2] * r_distorted**3 + undistortion_params[3] * r_distorted**4 + undistortion_params[4] * r_distorted**5) return(undistorted) def fun(undistortion_params,r_distorted): #Compute residuals. undistorted = undistort_point(undistortion_params, r_distorted) return((undistorted - np.linspace(0,1,1000))).ravel() . x0 = np.zeros(5).ravel() res = least_squares(fun, x0, verbose=2, ftol=1e-12,loss=&#39;linear&#39;, args=([r_distorted])) . Iteration Total nfev Cost Cost reduction Step norm Optimality 0 1 5.6524e+00 3.02e+01 1 2 2.3481e-07 5.65e+00 9.82e-01 1.88e-08 2 3 2.3481e-07 1.62e-17 1.58e-06 1.04e-11 `gtol` termination condition is satisfied. Function evaluations 3, initial cost 5.6524e+00, final cost 2.3481e-07, first-order optimality 1.04e-11. . Basically the optimisation process tries to find a set of coefficients that allow us to map the output from the distortion function back to it&#39;s input. . undistorted = undistort_point(res.x,r_distorted) plt.plot(r_distorted,label=&#39;distorted&#39;,alpha=0.5) plt.plot(undistorted,label=&#39;un distorted&#39;,alpha=0.5) plt.plot(np.linspace(0,1,1000),label=&#39;target&#39;,alpha=0.5) plt.legend() plt.show() . print(res.x) . [ 0.04599498 0.32120247 0.22196835 -0.46283148 0.77191211] . Voila, we have found a coefficients of a taylor series that allow us to invert the distortion function. .",
            "url": "https://cgcooke.github.io/Blog/optimisation/computer%20vision/2020/04/05/Inverse-Radial-Distortion.pynb.html",
            "relUrl": "/optimisation/computer%20vision/2020/04/05/Inverse-Radial-Distortion.pynb.html",
            "date": " • Apr 5, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "RQ Decomposition In Practice",
            "content": "Let&#39;s keep things short and sweet. . Given a camera projection matrix, $P$, we can decompose it into a $K$ (Camera Matrix), $R$ (Rotation Matrix) and $C$ (Camera Centroid) matrix. . IE, given we have $P = K[R|-RC]$, We want to find $K$, $R$ and $C$. . The method is simply described in Multiple View Geometry in Computer Vision (Second Edition), on page 163, however let&#39;s turn it into a practical Python implementation. . Let&#39;s follow along with an example from the book. . import numpy as np import scipy.linalg np.set_printoptions(precision=2) . P = np.array([[3.53553e+2, 3.39645e+2, 2.77744e+2, -1.44946e+6], [-1.03528e+2, 2.33212e+1, 4.59607e+2, -6.3252e+5], [7.07107e-1, -3.53553e-1, 6.12372e-1, -9.18559e+2]]) . So, we have: $P = [M | −MC]$ . M can be decomposed as $M=KR$ using the RQ decomposition. . M = P[0:3,0:3] K, R = linalg.rq(M) . So far, so good. . Now things get a little more complex. . We want to find a Camera matrix with a positive diagonal, giving positive focal lengths. . However, in case this doesn&#39;t happen, we can adjust the sign of the column of each column of both the $K$ and $R$ matrix, to &quot;Make it so&quot;. . T = np.diag(np.sign(np.diag(K))) if linalg.det(T) &lt; 0:     T[1,1] *= -1 K = np.dot(K,T) R = np.dot(T,R) .     Finally, we can find the Camera Center ($C$). . We have $P_4$, the 4th column of $P$. . $P_4 = −MC$ . From this, we can find $C = {-M}^{-1} P_4$ . def factorize(P): M = P[:,0:3] K,R = scipy.linalg.rq(M) T = np.diag(np.sign(np.diag(K))) if scipy.linalg.det(T) &lt; 0: T[1,1] *= -1 K = np.dot(K,T) R = np.dot(T,R) C = np.dot(scipy.linalg.inv(-M),P[:,3]) return(K,R,C) K,R,C = factorize(P) print(&#39;K&#39;) print(K) print(&#39;R&#39;) print(R) print(&#39;C&#39;) print(C) . K [[468.16 91.23 300. ] [ 0. 427.2 200. ] [ 0. 0. 1. ]] R [[ 0.41 0.91 0.05] [-0.57 0.22 0.79] [ 0.71 -0.35 0.61]] C [1000.01 2000. 1499.99] . Voila! . This presentation is a great read, and provides a good overview of the RQ and QR decompositions. .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/linear%20algebra/2020/03/13/RQ-Decomposition-In-Practice.html",
            "relUrl": "/computer%20vision/linear%20algebra/2020/03/13/RQ-Decomposition-In-Practice.html",
            "date": " • Mar 13, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "An Adventure in Camera Calibration",
            "content": "The Context . It&#39;s February 1972, the A300 airliner is being unveiled in Toulouse, let&#39;s go on an adventure (In camera calibration!). . . Let&#39;s say we have seen this photo published in a magazine, and we want to try and learn as much about the dimensions of Airbus&#39;s new aircraft as possible. In order to do so, we will need to mathematically reconstruct the camera used to take the photo, as well as the scene itself. . Control points . Now, In this case, we are lucky, because we notice the hexagonal pattern on the floor. In particular, we notice that it&#39;s a tessellating hexagonal pattern, which can only happen if all the hexagons have identical dimensions. . While we don&#39;t know the dimensions of the hexagon, we guess that each side is approximately 1.6m long, based on the height of the people in the photo. If we assume some point on the ground, say the center of a polygon is the point 0,0, we can work out the X &amp; Y location of each other polygon vertex we can see. Furthermore, we could also assume that the factory floor is flat and level. Hence the Z coordinate of each point is 0. . Let&#39;s spend ±5 minutes annotating the image, using an annotation tool like label me. I&#39;ve generated a file, which you can find attached here: . . Firstly, lets load in all of the x and y points: . import json import numpy as np JSON = json.loads(open(&#39;data/2020-02-23-An-Adventure-In-Camera-Calibration/A300.json&#39;,&#39;r&#39;).read()) polygons = {} for shape in JSON[&#39;shapes&#39;]: coords = shape[&#39;label&#39;].split(&#39;,&#39;) x,y = int(coords[0]),int(coords[1]) polygons[x,y] = shape[&#39;points&#39;] . Ok, now doing some maths, and work out the locations of each vertex of our hexagons. . from sklearn.neighbors import KDTree points = [] keys = sorted(polygons.keys()) for key in keys: poly = polygons[key] (pts_x, pts_y) = zip(*poly) pts_x = list(pts_x) pts_y = list(pts_y) #Magic analytic formula for working out the location of each point, based on which vertex, of which polygon it is. x_vertex = 0.5 * np.array([1,2,1,-1,-2,-1]) y_vertex = 0.5 * np.array([np.sqrt(3),0,-np.sqrt(3),-np.sqrt(3),0,np.sqrt(3)]) row,col = key x = row * 1.5 + x_vertex y = col * 0.5 * np.sqrt(3) + y_vertex #From before, we assume the sides of each polygon is 1.6m x*=1.6 #meters y*=1.6 #meters for idx in range(0,6): point = [] i = pts_x[idx] j = pts_y[idx] X = x[idx] Y = y[idx] Z = 0.0 points.append([i,j,X,Y,Z]) . Now we are presented with a minor problem, in many cases, we have annotated the same point up to 3 times, where the vertices of the hexagons meet. So let&#39;s go and find points that are within 10 pixels, and then take their average. If we don&#39;t do this, then we effectively over-weight some points in the image, at the expense of others. . points = np.asarray(points) np.savetxt(&quot;data/2020-02-23-An-Adventure-In-Camera-Calibration/points.csv&quot;, points) tree = KDTree(points[:,0:2], leaf_size=5) merged_indicies = [] unique_points = [] for i in range(0,points.shape[0]): if i not in merged_indicies: dist, ind = tree.query(points[i,0:2].reshape(-1, 2), k=3) indicies_to_merge = [] for j in range(0,3): if dist[0][j]&lt;10: indicies_to_merge.append(ind[0][j]) merged_indicies.append(ind[0][j]) mean_points = np.mean(points[indicies_to_merge,:],axis=0) unique_points.append(mean_points) unique_points = np.asarray(unique_points) . Camera Parameters . So, now we have a bunch of 3D points, and corresponding 2D points in the photo. . Now it&#39;s time to turn to the real magic, bundle adjustment. Basically, our task at hand, is to find a camera, which best fits the data we have measured. . Let&#39;s talk more about cameras. . Important: There are many correct ways to model a camera mathematically. This is one way. . Mathematically, cameras are are composed of two types of parameters, Intrinsic and Extrinsic. The Extrinsic parameters define the position and rotation of the camera, with respect to the origin of the points it&#39;s observing. . The Intrinsic parameters define the parameters of the camera itself, for example the Focal length, the location of the camera&#39;s radial center, as well as distortion induced by the lens. . The Extrinisic parameters are comprised of 6 degrees of freedom, given our world is 3 dimensional, and there are 3 dimensions which to rotate around. . The Intrinsic parameters are more complex. There are a number of great resources, for example Multiple View Geometry in Computer Vision, or the OpenCV documentation. However, In this case, I am assuming that the principal point, the focal length, and the radial parameters are unknown. . Note: To be clear, I&#8217;m building on the shoulders of giants, I&#8217;ve heavily adapted this example from this incredible demo by Nikolay Mayorov which you can find here . Firstly, let&#39;s go and import a bunch of stuff we will need later. . from __future__ import print_function from warnings import filterwarnings filterwarnings(&#39;ignore&#39;) . import numpy as np import matplotlib.pyplot as plt from scipy.optimize import least_squares from scipy.spatial.transform import Rotation as Rot %matplotlib inline plt.rcParams[&quot;figure.figsize&quot;] = (20,20) . points_2d = unique_points[:,0:2] points_3d = unique_points[:,2:5] print(&#39;We have {} unique points&#39;.format(points_2d.shape[0])) . We have 51 unique points . Modelling the Camera . Intrinsic Matrix . Now we come to the real magic. . begin{equation*} x = PX end{equation*}This function models the camera, taking points in 3D space, and converting them into points in 2D space. . There are lots of things going on here. . . Firstly, let&#39;s talk about the camera&#39;s intrinsic matrix. . Basically, it converts points from 3D space to 2D space. . begin{equation*} K = begin{bmatrix} f &amp; 0 &amp; c_{x} 0 &amp; f &amp; c_{y} 0 &amp; 0 &amp; 1 end{bmatrix} end{equation*}We have the focal length, $f$, and the camera optical center $c_x$ and $c_y$. . Extrinsic Matrix . Now let&#39;s talk about the camera&#39;s extrinsic matrix. . These are the 6 degrees of freedom that describe it&#39;s position and orientation within the world. That&#39;s 3 degrees for the position, and 3 for the orientation. At its heart, what we are doing is simple, but confusing. . There are so many ways to represent our setup: . Coordinate systems: 2D and 3D. Left Handed or Right Handed?     | . | Rotations: . Quaternions? | Proper Euler angles (6 different ways)? | Tait–Bryan angles (6 different ways)? | Rodrigues rotation formula? | A rotation matrix? | . | The location of the camera in to the world. (2 Different ways). . | Today we are going to use two different ways to represent the rotations, Firstly a Rodrigues rotation vector representation, and a rotation matrix. The reason why we use two different representations is because it&#39;s easier to optimise when we have 3 degrees of freedom, rather than a naive rotation matrix which uses 9 numbers to represent 3 degrees of freedom. . R represents the orientation of the camera in the World Coordinate Frame (The frame which we use to describe our 3D points). . In python, we can use convert from the Rodrigues rotation vector to the Rotation matrix as follows: . from scipy.spatial.transform import Rotation as Rot rotation_vector = camera_params[:3] R = Rot.from_rotvec(rotation_vector).as_matrix() . begin{equation*} R = begin{bmatrix} R_1 &amp; R_2 &amp; R_3 R_4 &amp; R_5 &amp; R_6 R_7 &amp; R_8 &amp; R_9 end{bmatrix} end{equation*} Now, let&#39;s talk about the Project Matrix $P$ of the camera. This takes the points all the way from their location in 3D world coordinates, to pixel coordinates, assuming we have a camera without radial distortion. There are two main ways this could be formulated. . Firstly: begin{equation*} P = KR[I|−C] end{equation*} . Secondly: begin{equation*} P = K[R | t] end{equation*} . Where $t$ is: begin{equation*} t = −RC end{equation*} . Let&#39;s go with the first method, where C is : . begin{equation*} C = begin{bmatrix} -C_X -C_Y -C_Z end{bmatrix} end{equation*} Lens distortion . However, there is one subtlety alluded to before, which is the impact of radial distortion. Simply, the camera&#39;s lens distorts the rays of light coming in, in a non-linear way. . We can model it using a Taylor series: . begin{equation*} x_c = x(1 + k_1 r + k_2 r^2 + k_3 r^3) end{equation*} begin{equation*} y_c = y(1 + k_1 r + k_2 r^2 + k_3 r^3) end{equation*} In python, we end up with: . r = np.sqrt(np.sum(points_proj**2, axis=1))) r = 1 + k1 times r + k2 * r**2 + k3 * r**3 points_proj *= r[:, np.newaxis] . Putting it all together . def project(points, camera_params): &quot;&quot;&quot;Convert 3-D points to 2-D by projecting onto images.&quot;&quot;&quot; #Rotation rotation_vector = camera_params[:3] R = Rot.from_rotvec(rotation_vector).as_matrix() #Camera Center C = camera_params[3:6].reshape(3,1) IC = np.hstack([np.eye(3),-C]) RIC = np.matmul(R,IC) #Make points Homogeneous points = np.hstack([points,np.ones((points.shape[0],1))]) #Perform Rotation and Translation #(n,k), (k,m) -&gt; (n,m) points_proj = np.matmul(points,RIC.T) #perspective divide points_proj = points_proj[:, :2] / points_proj[:, 2, np.newaxis] f = camera_params[6] k1 = camera_params[7] k2 = camera_params[8] k3 = camera_params[9] c_x = camera_params[10] c_y = camera_params[11] #Radial distortion r = np.sqrt(np.sum(points_proj**2, axis=1)) x = points_proj[:,0] y = points_proj[:,1] points_proj[:,0] = (1 + k1 * r + k2 * r**2 + k3 * r**3)*x points_proj[:,1] = (1 + k1 * r + k2 * r**2 + k3 * r**3)*y #Make points Homogeneous points_proj = np.hstack([points_proj, np.ones((points_proj.shape[0],1))]) K = np.asarray([[f, 0, c_x], [0, f, c_y], [0, 0, 1.0]]) points_proj = np.dot(points_proj,K.T) points_proj = points_proj[:,:2] return(points_proj) . Initial Parameters . Let&#39;s start by providing some hints to the optimiser about what the solution could be like, by putting in some reasonable starting conditions. . We know both the image width and height, and we can assume that the principal point is in the center of the image. . I think the camera is about 10 meters off the ground. . To make the task of optimization easier, let&#39;s rotate the camera so that it&#39;s facing directly down. This means that the points should be in front of/below it. . Let&#39;s also start off by assuming that the camera is centered above the points. It&#39;s obviously not correct, based on what we can see in the image, but it&#39;s not horrifically wrong.  . image_width = 2251 image_height = 1508 estimated_focal_length_px = 2000 camera_params = np.zeros(12) r = Rot.from_euler(&#39;x&#39;, 180, degrees=True).as_rotvec() #Rotation matrix camera_params[0] = r[0] camera_params[1] = r[1] camera_params[2] = r[2] #C camera_params[3] = points_3d[:,0].mean() camera_params[4] = points_3d[:,1].mean() camera_params[5] = 10 #f,k1,k2, camera_params[6] = estimated_focal_length_px camera_params[7] = 0 camera_params[8] = 0 camera_params[9] = 0 #c_x,c_y camera_params[10] = image_width/2.0 camera_params[11] = image_height/2.0 . Optimisation . This section below is really well explained by here. Basically, we are optimizing to minimise a geometric error. It&#39;s the distance between the 2D points we see, and the projection of their 3D counterparts. . Through a process of optimization, we aim to find parameters which result in low error, which means in turn they should represent the real parameters of the camera. . def fun(camera_params, points_2d, points_3d): #Compute residuals. points_proj = project(points_3d, camera_params) return(points_proj - points_2d).ravel() . x0 = camera_params.ravel() optimization_results = least_squares(fun, x0, verbose=1, x_scale=&#39;jac&#39;, ftol=1e-4, method=&#39;lm&#39;, loss=&#39;linear&#39;,args=(points_2d, points_3d)) . `ftol` termination condition is satisfied. Function evaluations 970, initial cost 3.7406e+07, final cost 1.7398e+02, first-order optimality 3.63e+03. . Results . Now let&#39;s go and check out the results of our optimization process. . camera_params = optimization_results.x R_Rodrigues = camera_params[0:3] C = camera_params[3:6] r = Rot.from_rotvec(R_Rodrigues) R_matrix = r.as_matrix() r = Rot.from_matrix(R_matrix.T) R_Quaternion = r.as_quat() print(&#39;Quaternions: X: {:.3f} Y: {:.3f} Z: {:.3f} W: {:.3f} &#39;.format(R_Quaternion[0],R_Quaternion[1],R_Quaternion[2],R_Quaternion[3])) print(&#39;Camera position relative to the origin in (M): X: {:.2f}, Y: {:.2f}, Z: {:.2f}&#39;.format(C[0],C[1],C[2])) focal_length_px = camera_params[6] k1 = camera_params[7] k2 = camera_params[8] k3 = camera_params[9] c_x = camera_params[10] c_y = camera_params[11] print(&#39;Focal length (Pixels): {:.2f}&#39;.format(focal_length_px)) print(&#39;CX, CY: {:.2f} {:.2f}&#39;.format(c_x,c_y)) print(&#39;K_1, K_2, K_3 : {:.6f}, {:.6f}, {:.6f}&#39;.format(k1,k2,k3)) print(&#39;Mean error per point: {:.2f} pixels &#39;.format(optimization_results.cost/points_2d.shape[0])) . Quaternions: X: 0.894 Y: -0.408 Z: 0.084 W: -0.166 Camera position relative to the origin in (M): X: -6.85, Y: -12.92, Z: 2.75 Focal length (Pixels): 1010.93 CX, CY: 1038.58 2663.52 K_1, K_2, K_3 : -0.327041, 0.175031, -0.030751 Mean error per point: 3.41 pixels . Ok, first things first, the mean error per point is 3-4 pixels, which is not great, not terrible. It&#39;s clear that we have found a decent solution, However there are some interesting things going on. . In particular, the principal point lies outside the image, which is curious to say the least. One possibility is that the image was cropped. . Now let&#39;s have a quick look at the errors for each point. . plt.hist(abs(optimization_results.fun),density=True) plt.title(&#39;Histogram of Residuals&#39;) plt.xlabel(&#39;Absolute Residual (Pixels)&#39;) plt.grid() plt.show() . So the histogram looks pretty good, apart from the one point with a high residual, which is probably due to sloppy labeling/annotation. . Now let&#39;s compare the points we annotated, with where they would be projected, using the camera parameters we found: . points_2d_proj = project(points_3d, optimization_results.x) img = plt.imread(&#39;data/2020-02-23-An-Adventure-In-Camera-Calibration/A300.jpg&#39;) plt.imshow(img) plt.scatter(points_2d[:,0],points_2d[:,1],label=&#39;Actual&#39;,c=&#39;r&#39;,alpha=0.5) plt.scatter(points_2d_proj[:,0],points_2d[:,1],label=&#39;Optimised&#39;,c=&#39;k&#39;,alpha=0.5) plt.show() . Again, this looks great. Finally, let&#39;s overlay the hexagons on the floor, to visually build confidence in our solution. . def plot_verticies(row,col): x_vertex = 0.5 * np.array([1,2,1,-1,-2,-1,1]) y_vertex = 0.5 * np.array([np.sqrt(3),0,-np.sqrt(3),-np.sqrt(3),0,np.sqrt(3),np.sqrt(3)]) x = row * 1.5 + x_vertex y = col * 0.5 * np.sqrt(3) + y_vertex x*=1.6 y*=1.6 points_3d = np.vstack([x,y,np.zeros(7)]).T points_2d_proj = project(points_3d, optimization_results.x) return(points_2d_proj) plt.imshow(img) for row in range(0,10,2): for col in range(0,10,2): points_2d_proj = plot_verticies(row,col) plt.plot(points_2d_proj[:,0],points_2d_proj[:,1],color=&#39;B&#39;,alpha=0.25) plt.text(np.mean(points_2d_proj[:,0]), np.mean(points_2d_proj[:,1]), str(row)+&#39;,&#39;+str(col), horizontalalignment=&#39;center&#39;,verticalalignment=&#39;center&#39;) for row in range(1,11,2): for col in range(1,11,2): points_2d_proj = plot_verticies(row,col) plt.plot(points_2d_proj[:,0],points_2d_proj[:,1],color=&#39;R&#39;,alpha=0.25) plt.text(np.mean(points_2d_proj[:,0]), np.mean(points_2d_proj[:,1]), str(row)+&#39;,&#39;+str(col), horizontalalignment=&#39;center&#39;,verticalalignment=&#39;center&#39;) plt.show() . In Conclusion . Awesome, we can see visually that we have found a semi-reasonable solution. . However, I&#39;m worried about the location of the principle point of the image. Normally, with most cameras, this is near the center of the image. In our case, it isn&#39;t. There are a number of reasons why this could be the case, for example, the image might have been cropped, however it&#39;s a little concerning. . I&#39;m also worried about the height of the camera, which is only 2.75M above the ground. The camera looks like it&#39;s at approximately the same height as the roof of the aircraft, which would typically be 7-10m above the ground. . In the future, Let&#39;s look more about how we can extract some more useful information from this image, and understand how confident we can be in our solution. . Thanks to Nikolay Mayorov who created the awesome demo of optimization in Scipy that I built upon, you can find the original code here. . Multiple View Geometry in Computer Vision is an incredible book that I learn more from, each time I read it. in particular, for further information see: . Finite cameras. Page 153, Multiple View Geometry in Computer Vision (Second edition) | Note: Minimizing geometric error. Page 176, Multiple View Geometry in Computer Vision (Second edition) | .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/optimisation/linear%20algebra/2020/02/23/An-Adventure-In-Camera-Calibration.html",
            "relUrl": "/computer%20vision/optimisation/linear%20algebra/2020/02/23/An-Adventure-In-Camera-Calibration.html",
            "date": " • Feb 23, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Three Interesting Papers",
            "content": "Over the past couple of months, 3 incredibly exciting papers have come out, and I want to take the opportunity to share them with you. . The papers, in no particular order are MixMatch, Selfie and Unsupervised Data Augmentation, however let’s first discuss why they are exciting. . In my day to day work, I’m faced with an avalanche of data. Raw data may be cheap, but labelled data is precious, often relying on expensive experiments or busy experts. Even then, when labelled data is available there is an omnipresent, insatiable demand to do more with it. . Semi-supervised learning offers the opportunity to leverage the raw, unlabelled data to improve our models, reducing the barriers to building a model, and democratising AI. . I’m not going to discuss how the papers are actually implemented in detail, but I will say that the papers are very promising, and I hope they will be rapidly implemented and adapted as a standard part of deep learning workflows. . In a sentence, MixMatch uses MixUp, and label sharpening (Fancy way of saying “Artificially boosting your models confidence”) in order to effectively propagate labels. My first impression was “I can’t believe that works”, but then I saw that it decreases error rates 4x with when training with small numbers of samples on CIFAR-10. . Conversely, Selfie is inspired pertaining in BERT, and extends it to CNN’s. At a high level, the pre training task is analagous to removing pieces from a jigsaw puzzle, and asking “what piece should go in each hole?”. Given the power of transfer learning, this is hugely exciting for many problems where the data you want to train on is very different to what is found in ImageNet. . Finally, there is Unsupervised Data Augmentation (UDA), which prosecutes the thesis that “better data augmentation can lead to significantly better semi-supervised learning”. As with Selfie and MixMatch, the techniques used in this paper can be applied to image data. . Deep learning is built on a history of rapidly evolving best practice, including Xavier initialisation, Data Augmentation, One Cycle Policy and MixUp. I hope that adoptions of that MixMatch, Selfie and UDA will soon join this grab bag of best practice. .",
            "url": "https://cgcooke.github.io/Blog/deep%20learning/computer%20vision/2019/06/05/Three-Interesting-Papers.html",
            "relUrl": "/deep%20learning/computer%20vision/2019/06/05/Three-Interesting-Papers.html",
            "date": " • Jun 5, 2019"
        }
        
    
  
    
        ,"post14": {
            "title": "Kalman Filters",
            "content": "Kalman Filters are magic. While they take 5 minutes to explain at a basic level, you can work with them for a career and always be learning more. I think there is something philosophically satisfying about the way that they innately combine what we already believe and what we perceive in order to come to a new belief about the world. . While this sounds somewhat abstract, Kalman Filters provide a concrete mathematical formulation for fusing data from different sources, as well as physical models to provide (potentially) optimum estimates of the state of a system. . For less philosophy, and more maths, I strongly recommend stopping at this point and giving this incredible post a read. Afterwards, let’s talk about Kalman filters in a concrete way, . Creating a Kalman Filter in 7 easy steps: . One of the challenges with Kalman filters is that it’s easy to be initially overwhelmed by the mathematical background, and loose sight of what their implementation looks like in practice. In reality, it’s possible to break the implementation down into a series of discrete steps, which come together to fully describe the filter. . FilterPy is a fantastic Python library for creating Kalman filters, and has an accompanying book, which takes a deep dive into the mathematical theory of Kalman filters. Lets initially discuss the general process for defining a Kalman filter, before applying it to practical application. . x : Our filter state estimate, IE what we want to estimate. If we want to track an object moving an a video, this could be it’s pixel coordinates, as well as it’s velocity in pixels per second. [x,y,vx,vy] . | P : The covariance matrix. Encodes how certain the filter is about it’s estimates, evolves over time. In the object tracking example, how “confident” the filter is about the position of an object and it’s velocity. As the filter receives more measurements, the values in the covariance matrix are “washed out”, and so the the filter tends to be insensitive to the values used. . | Q : The process uncertainty. How large is the error associated with the system doing something unexpected between measurements? I find this the hardest to set, as it requires careful thought about the process. For example, if we are tracking the position and velocity of an object once a second, we would have more uncertainty if we were tracking the position of a fruit fly than an oil tanker. . | R : How uncertain each of our measurements are. This can be determined either through reading sensor datasheets or educated guesses. . | H : How to each measurement is related to the internal state of our system, in addition to scaling measurements. IE, If we have a GPS receiver, it tells us about our position, while an accelerometer tells us about our position. . | F : The state transition matrix. How the system evolves over time. IE, if we know the position and velocity of an object, then in the absence of any error or external influence we can predict it’s next position from it’s current position and velocity. . | B : The control matrix. This matrix allows us to tell the filter about how we expect any inputs we provide the system (u) to update the state of the system. In many cases, especially when we are taking measurements of a system we don’t control, the control matrix is not required. . | At this point, I think it’s worthwhile considering how all of these matrices are related to each other. Tim Babb of Bzarg, has a fantastic diagram, which sets out how information flows through all of the filters mentioned above. If you haven’t already, I strongly recommend you read his post on how Kalman filters work . Looking at the relationships between all of the matrices, . x and P are outputs of the filter, they tell what the filter believes the state of the system to be. . | H, F and B are matrices which control how the filter operates. . | Q, R are closely related, because they both denote uncertainty, in the process as well as the measurements. . | z and u denote inputs to the filter, in the case where we don’t control the system, then u is not required. . | A real world example: . Let’s look at a real world example. In computer vision, object tracking is the process of associating different detections of an object from different images/frames into a single “track”. Many algorithms have been developed for this task (Simple Online and Realtime Tracking)[https://arxiv.org/abs/1602.00763] is particularly elegant. In summary, SORT creates a Kalman filter for each object it wants to track, and then predicts the location and size of each object, in each frame using the filter. . Alex Bewley, one of the creators of SORT has developed a fantastic (implementation)[https://github.com/abewley/sort] of SORT, which uses Filterpy. . Let’s take a look at his implementation, through the lens of what I’ve discussed above: . Quickly defining some nomenclature, . u and v are the x and y pixel coordinates of the center of the bounding box around an object being tracked. . | s and r are tha scale and aspect ratio of the bounding box surrounding the object. . | u˙,v˙ dot u, dot vu˙,v˙ are the x and y velocity of the bounding box. . | s˙ dot ss˙ is the rate at which the scale of the bounding box is changing. . | . kf = KalmanFilter(dim_x=7, dim_z=4) . Our internal state is 7 dimensional: . [u,v,s,r,u˙,v˙,s˙][u, v, s, r, dot u, dot v , dot s][u,v,s,r,u˙,v˙,s˙] . While our input vector is 4 dimensional: . [u,v,s,r][u, v, s, r][u,v,s,r] . kf.F = np.array([[1,0,0,0,1,0,0], [0,1,0,0,0,1,0], [0,0,1,0,0,0,1], [0,0,0,1,0,0,0], [0,0,0,0,1,0,0], [0,0,0,0,0,1,0], [0,0,0,0,0,0,1]]) . The state transition matrix tells us that at each timestep, we update our state as follows: . u=u+u˙u = u + dot uu=u+u˙ . v=v+v˙v = v + dot vv=v+v˙ . s=s+s˙s = s + dot ss=s+s˙ . kf.H = np.array([[1,0,0,0,0,0,0], [0,1,0,0,0,0,0], [0,0,1,0,0,0,0], [0,0,0,1,0,0,0]]) . The sensor matrix tells us that we are directly measuring [u,v,s,r][u, v, s, r][u,v,s,r]. . kf.R = np.array([[ 1, 0, 0, 0], [ 0, 1, 0, 0,], [ 0, 0, 10, 0,], [ 0, 0, 0, 10,]]) . The sensor noise matrix tells us that we can measure uuu and vvv with a much higher degree of certainty than sss and rrr. . kf.P = np.array([[ 10, 0, 0, 0, 0, 0, 0], [ 0, 10, 0, 0, 0, 0, 0], [ 0, 0, 10, 0, 0, 0, 0], [ 0, 0, 0, 10, 0, 0, 0], [ 0, 0, 0, 0, 10000, 0, 0], [ 0, 0, 0, 0, 0, 10000, 0], [ 0, 0, 0, 0, 0, 0, 10000]]) . The covariance matrix tells us that the filter should have a very high initial uncertinty for each of the velocity components. . kf.Q = np.array([[1.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00] [0.e+00, 1.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00] [0.e+00, 0.e+00, 1.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00] [0.e+00, 0.e+00, 0.e+00, 1.e+00, 0.e+00, 0.e+00, 0.e+00] [0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-02, 0.e+00, 0.e+00] [0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-02, 0.e+00] [0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-04]]) . The Process uncertainty matrix tells us how much “uncertainty” there is in each component of the systems behaviour. . Filterpy has a function which can be very useful for generating Q. . filterpy.common.Q_discrete_white_noise . Further reading . Control theory is a broad an intellectually stimulating area, with broad applications. Brian Douglas has an incredible YouTube channel which I strongly recommend. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/2019/03/23/Kalman-Filters.html",
            "relUrl": "/bayesian/2019/03/23/Kalman-Filters.html",
            "date": " • Mar 23, 2019"
        }
        
    
  
    
        ,"post15": {
            "title": "256 Shades Of Grey",
            "content": "On February 22, 2000, after 11 days of measurements, the most comprehensive map ever created of the earth’s topography was complete. The space shuttle Endeavor had just completed the Shuttle Radar Topography Mission, using a specialised radar to image the earths surface. . The Digital Elevation Map (DEM) produced by this mission is in the public domain and provides the measured terrain high at ~90 meter resolution. The mission mapped 99.98% of the area between 60 degrees North and 56 degrees South. . In this post, I will examine how to process the raw DEM so it is more intuitively interpreted, through the use of hillshading,slopeshading &amp; hypsometric tinting. . The process of transforming the raw GeoTIFF into the final imagery product is simple. Much of the grunt work being carried out by GDAL, the Geospatial Data Abstraction Library. . In order, we need to: . Download a DEM as a GeoTIFF | Extract a subsection of the GeoTIFF | Reproject the subsection | Make an image by hillshading | Make an image by coloring the subsection according to altitude | Make an image by coloring the subsection according to slope | Combine the 3 images into a final composite | DEM . Several different DEM’s have been created from the data collected on the SRTM mission, in this post I will use the CGIAR SRTM 90m Digital Elevation Database. Data is provided in 5x5 degree tiles, with each degree of latitude equal to approximately 111Km. . Our first task is to acquire a tile. Tiles can be downloaded from http://data.cgiar-csi.org/srtm/tiles/GeoTIFF/ using wget. . import os import math from PIL import Image, ImageChops, ImageEnhance from matplotlib import cm . def downloadDEMFromCGIAR(lat,lon): &#39;&#39;&#39; Download a DEM from CGIAR FTP repository &#39;&#39;&#39; fileName = lonLatToFileName(lon,lat)+&#39;.zip&#39; &#39;&#39;&#39; Check to see if we have already downloaded the file &#39;&#39;&#39; if fileName not in os.listdir(&#39;.&#39;): os.system(&#39;&#39;&#39;wget --user=data_public --password=&#39;GDdci&#39; http://data.cgiar-csi.org/srtm/tiles/GeoTIFF/&#39;&#39;&#39;+fileName) os.system(&#39;unzip &#39;+fileName) . def lonLatToFileName(lon,lat): &#39;&#39;&#39; Compute the input file name &#39;&#39;&#39; tileX = int(math.ceil((lon+180)/5.0)) tileY = -1*int(math.ceil((lat-65)/5.0)) inputFileName = &#39;srtm_&#39;+str(tileX).zfill(2)+&#39;_&#39;+str(tileY).zfill(2) return(inputFileName) . lon,lat = -123,49 inputFileName = lonLatToFileName(lon,lat) downloadDEMFromCGIAR(lat,lon) . Slicing . The area I have selected covers Washington State and British Columbia, with file name srtm_12_03.tif. . Let’s use GDAL to extract a subsection of the tile.The subsection covers Vancouver Island and the Pacific Ranges stretching from 125ºW - 122ºW &amp; 48ºN - 50ºN. Using gdalwarp: . !! gdalwarp -q -te -125 48 -122 50 -srcnodata -32768 -dstnodata 0 srtm_12_03.tif subset.tif . Our next step is to transform the subsection of the tile to a different projection. The of the points in the subsection are located on a grid 1/1200th of a degree apart. While degrees of latitude are always ~110Km in size, resulting in ~92.5M resolution, degrees of longitude decrease in size, from ~111Km at the equator to 0Km at the poles. A different scale exists between the latitude &amp; longitude axis and a longitude scale that depends on the latitude. . A solution is to project that points so that there is a consistent and equal scale in the X/Y plane. One choice is to use a family of projections called Universal Transverse Mercator. Each UTM projection can map points from longitude &amp; latitude to X &amp; Y coordinates in meters. The UTM projection is useful because it locally preserves both shapes and distances, over a distances of up to several hundred kilometres. . The tradeoff is that several different UTM projections are required for different points on earth, 120 to be precise. Fortunately it is relatively trivial to work out the required projection based on the longitude and latitude. Almost every conceivable projection has been assigned a code by the European Petroleum Survey Group (EPSG). This EPSG code can be used to unambiguously specify the projection being used. With UTM, each code starts with either 327 or 326, depending on the hemisphere of the projection. . utmZone = int((math.floor((lon + 180)/6) % 60) + 1) &#39;&#39;&#39; Check to see if file is in northern or southern hemisphere &#39;&#39;&#39; if lat&lt;0: EPSGCode = &#39;EPSG:327&#39;+str(utmZone) else: EPSGCode = &#39;EPSG:326&#39;+str(utmZone) . Once we have identified the correct EPSG code to use, the process of warping the subset to a new projection is relatively straightforward. . In the following system call to gdalwarp, t_srs denotes the target projection, and tr specifies the resolution in the X and Y plane. The Y resolution is negative because the in the GDAL file uses a row, column based coordinate system. . In this coordinate system, the origin is in the top left hand corner of the file. The row value increases as you move down the file, like an excel spreadsheet, however the UTM Y coordinate decreases. This results in the negative sign in the resolution. . os.system(&#39;gdalwarp -q -t_srs &#39;+EPSGCode+&#39; -tr 100 -100 -r cubic subset.tif warped.tif&#39;) . Hillshading . At this point we can begin to visualise the DEM. One highly effective method is hillshading, which models the way the surface of the DEM would be illuminated by light projected onto it. Shading of the slopes allows the DEM to be more intuitively interpreted than just coloring by height alone. . . !! gdaldem hillshade -q -az 45 -alt 45 warped.tif hillshade.tif . Hypsometric Tinting . Hillshading can also be combined with height information to aid interpretation of the topography. The technical name for the process of coloring a DEM based on height is hypsometric tinting. The process is simple, with GDAL mapping colors to cell heights, using a provided color scheme. . . def createColorMapLUT(minHeight,maxHeight,cmap = cm.YlGn_r,numSteps=256): &#39;&#39;&#39; Create a colormap for visualisation &#39;&#39;&#39; f =open(&#39;color_relief.txt&#39;,&#39;w&#39;) f.write(&#39;-0.1,135,206,250 n&#39;) f.write(&#39;0.1,135,206,250 n&#39;) for i in range(0,numSteps): r,g,b,a= cmap(i/float(numSteps)) height = minHeight + (maxHeight-minHeight)*(i/numSteps) f.write(str(height)+&#39;,&#39;+str(int(255*r))+&#39;,&#39;+str(int(255*g))+&#39;,&#39;+str(int(255*b))+&#39; n&#39;) f.write(str(-1)+&#39;,&#39;+str(int(255*r))+&#39;,&#39;+str(int(255*g))+&#39;,&#39;+str(int(255*b))+&#39; n&#39;) createColorMapLUT(minHeight=10,maxHeight=2658) . !! gdaldem color-relief -q warped.tif color_relief.txt color_relief.tif . Slope Shading . Another technique for visualizing terrain is slopeshading. While hypsometric tinting assigns colors to cells based on elevation, slope shading assigns colors to pixels based on the slope (0º to 90º). In this case, white (255,255,255) is assigned to slopes of 0º and black (0,0,0) is assigned to slopes of 90º, with varying shades of grey for slopes in-between. . . This color scheme is encoded in a txt file for gdaldem as follows: . f = open(&#39;color_slope.txt&#39;,&#39;w&#39;) f.write(&#39;0 255 255 255 n&#39;) f.write(&#39;90 0 0 0 n&#39;) f.close() . The computation of the slope shaded dem takes place over two steps. . The slope of each cell is computed | A shade of grey is assigned to each cell depending on the slope. | !! gdaldem slope -q warped.tif slope.tif !! gdaldem color-relief -q slope.tif color_slope.txt slopeshade.tif . Layer Merging . The final step in producing the final product is to merge the 3 different created images. The python Image Library (PIL) is a quick and dirty way to accomplish this task, with the 3 layers are merged using pixel by pixel multiplication. . One important detail to note is that the pixel by pixel multiplication occurs in the RGB space. From a theoretical perspective, it’s probably better that each pixel is first transformed to the Hue, Saturation, Value (HSV) color space, and the value is then multiplied by the hillshade and slope shade value, before being transformed back into the RGB color space. In practical terms however, the RGB space multiplication is a very reasonable approximation. . In one final tweak, the brightness of the output image is increased by 40%, to offset the average reduction in brightness caused by multiplying the layers together. . . &#39;&#39;&#39; Merge components using Python Image Lib &#39;&#39;&#39; slopeshade = Image.open(&quot;slopeshade.tif&quot;).convert(&#39;L&#39;) hillshade = Image.open(&quot;hillshade.tif&quot;) colorRelief = Image.open(&quot;color_relief.tif&quot;) #Lets just fill in any gaps in the hillshading ref = Image.new(&#39;L&#39;, slopeshade.size,180) hillshade = ImageChops.lighter(hillshade,ref) shading = ImageChops.multiply(slopeshade, hillshade).convert(&#39;RGB&#39;) merged = ImageChops.multiply(shading,colorRelief) &#39;&#39;&#39; Adjust the brightness to take into account the reduction caused by hillshading&#39;&#39;&#39; enhancer = ImageEnhance.Brightness(merged) img_enhanced = enhancer.enhance(1.4) img_enhanced.save(&#39;Merged.png&#39;) . Further reading . I found the following sources to be invaluable in compiling this post: . Creating color relief and slope shading | A workflow for creating beautiful relief shaded DEMs using gdal | Shaded relief map in python | Stamen Design | .",
            "url": "https://cgcooke.github.io/Blog/remote%20sensing/2018/11/18/256-Shades-of-Grey.html",
            "relUrl": "/remote%20sensing/2018/11/18/256-Shades-of-Grey.html",
            "date": " • Nov 18, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Cameron! . I’m passionate about Computer Vision, Robotics, Deep Learning, Bayesian Analysis and Flight. . I’m writing a blog because the first step to better understanding something is to try and explain it to someone else. . Plus I get excited about the incredible world around us, and writing helps me appreciate it all the more. . I currently work with Airbus in Toulouse, France, where I get to learn from some incredible people, while helping to shape the future of the Aerospace industry. . .",
          "url": "https://cgcooke.github.io/Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://cgcooke.github.io/Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}