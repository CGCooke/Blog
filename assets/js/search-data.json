{
  
    
        "post0": {
            "title": "The Log Polar Transform",
            "content": "The Context . As we saw in the last post, we can use FFT Phase Correlation, to find the x &amp; y translation between two different images/arrays. . However, what if there is a rotation between two images/arrays instead? . One solution is the Log-Polar transformation, which transforms the image from x/y space to r/theta space. . In r/theta space, rotation and scaling between the two images manifest as 2d translations. . Once we have identified the rotation between the two images, we can correct for it, before finally finding the 2d translation between the two images. . While understanding this process, I found this really good explanation by Santosh Thoduka, which helped make things much clearer for me. Scikit-image also has a good demo, which I learnt a lot from. . The Log Polar Transform . The log polar transform points from (x,y) to ($ rho$,$ theta$) as follows: . $ rho = ln sqrt{x^2 + y^2}$ . $ theta = atan2(y-y_c,x-x_c)$ . To help visualise this process, let&#39;s do some experiment&#39;s with Mondarian&#39;s Tableau I. . import numpy as np import matplotlib.pyplot as plt import skimage.io import skimage.transform plt.rcParams[&#39;figure.figsize&#39;] = [10, 10] . First, let&#39;s load and plot the image: . mondarin = skimage.io.imread(&#39;data/2020-12-20-FFT-Phase-Correlation-Rotation/mondrian.jpg&#39;) mondarin = mondarin[0:1000,0:1000,:] shape = mondarin.shape plt.title(&#39;Original&#39;) plt.scatter(shape[0]/2,shape[1]/2,color=&#39;r&#39;,label = &#39;Image Center&#39;) plt.imshow(mondarin) plt.legend() plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) plt.show() . Now let&#39;s compute the Log Polar Transform of the image, and visualise that as well: . monarin_warped = skimage.transform.warp_polar(mondarin, radius=700, output_shape=shape, scaling=&#39;linear&#39;, order=0, multichannel=True) plt.title(&#39;Warped&#39;) plt.imshow(monarin_warped) plt.xlabel(&#39;Radius&#39;) plt.ylabel(&#39;Theta&#39;) plt.show() . Now let&#39;s rotate the Mondarin, so that we can better understand power of the log polar transform. . mondarin_rotated = skimage.transform.rotate(mondarin, 90) plt.title(&#39;Rotated&#39;) plt.scatter(shape[0]/2,shape[1]/2,color=&#39;r&#39;,label = &#39;Image Center&#39;) plt.imshow(mondarin_rotated) plt.legend() plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) plt.show() . By comparing the transform of both the original, and the rotated Mondarin, we can see that a rotation in the original domain, corresponds to a translation/shift in the log polar domain. . monarin_rotated_warped = skimage.transform.warp_polar(mondarin_rotated, radius=700, output_shape=shape, scaling=&#39;linear&#39;, order=0, multichannel=True) plt.title(&#39;Side by Side comparison&#39;) plt.imshow(np.hstack([monarin_warped,monarin_rotated_warped])) plt.ylabel(&#39;Theta&#39;) plt.show() . You can read about the Log Polar Transform in more detail here. . Applying theory to practice . from skimage.color import rgb2gray from skimage.filters import window, difference_of_gaussians from scipy.fftpack import fft2, fftshift import scipy plt.rcParams[&#39;figure.figsize&#39;] = [10, 10] . frame = skimage.io.imread(&#39;data/2020-12-20-The-Log-Polar-Transform/Frame.jpg&#39;) image = frame[30:185,35:190] . FileNotFoundError Traceback (most recent call last) &lt;ipython-input-7-78f3a87db317&gt; in &lt;module&gt; -&gt; 1 frame = skimage.io.imread(&#39;data/2020-12-19-FFT-Phase-Correlation/Frame.jpg&#39;) 2 image = frame[30:185,35:190] /opt/anaconda3/envs/OpenCV/lib/python3.8/site-packages/skimage/io/_io.py in imread(fname, as_gray, plugin, **plugin_args) 46 47 with file_or_url_context(fname) as fname: &gt; 48 img = call_plugin(&#39;imread&#39;, fname, plugin=plugin, **plugin_args) 49 50 if not hasattr(img, &#39;ndim&#39;): /opt/anaconda3/envs/OpenCV/lib/python3.8/site-packages/skimage/io/manage_plugins.py in call_plugin(kind, *args, **kwargs) 207 (plugin, kind)) 208 --&gt; 209 return func(*args, **kwargs) 210 211 /opt/anaconda3/envs/OpenCV/lib/python3.8/site-packages/skimage/io/_plugins/imageio_plugin.py in imread(*args, **kwargs) 8 @wraps(imageio_imread) 9 def imread(*args, **kwargs): &gt; 10 return np.asarray(imageio_imread(*args, **kwargs)) /opt/anaconda3/envs/OpenCV/lib/python3.8/site-packages/imageio/core/functions.py in imread(uri, format, **kwargs) 263 264 # Get reader and read first --&gt; 265 reader = read(uri, format, &#34;i&#34;, **kwargs) 266 with reader: 267 return reader.get_data(0) /opt/anaconda3/envs/OpenCV/lib/python3.8/site-packages/imageio/core/functions.py in get_reader(uri, format, mode, **kwargs) 170 171 # Create request object --&gt; 172 request = Request(uri, &#34;r&#34; + mode, **kwargs) 173 174 # Get format /opt/anaconda3/envs/OpenCV/lib/python3.8/site-packages/imageio/core/request.py in __init__(self, uri, mode, **kwargs) 122 123 # Parse what was given --&gt; 124 self._parse_uri(uri) 125 126 # Set extension /opt/anaconda3/envs/OpenCV/lib/python3.8/site-packages/imageio/core/request.py in _parse_uri(self, uri) 258 # Reading: check that the file exists (but is allowed a dir) 259 if not os.path.exists(fn): --&gt; 260 raise FileNotFoundError(&#34;No such file: &#39;%s&#39;&#34; % fn) 261 else: 262 # Writing: check that the directory to write to does exist FileNotFoundError: No such file: &#39;/Users/cooke_c/Documents/GitHub/Blog/_notebooks/data/2020-12-19-FFT-Phase-Correlation/Frame.jpg&#39; . img1 = rgb2gray(frame[30:185,35:190]) . Rotate the image . angle = 30 img2 = rotate(result, angle) . Shift the image . shift_x = 10 shift_y = 20 #Shift the img1 10 pixels to the right and 20 pixels down. shifted = scipy.ndimage.fourier_shift(np.fft.fft2(img1), shift=(shift_y,shift_x)) img2 = np.fft.ifft2(shifted).real . Find Rotation . def fft_phase_correlation(img1, img2): # window images img1_wimage = img1 * window(&#39;hann&#39;, img1.shape) img2_wimage = img2 * window(&#39;hann&#39;, img2.shape) # window images img1_wimage = img1 * window(&#39;hann&#39;, img1.shape) img2_wimage = img2 * window(&#39;hann&#39;, img2.shape) #Compute the corss power spectrum cross_power_spectrum = (img1_fs * np.conjugate(img2_fs)) / np.abs(img1_fs * np.conjugate(img2_fs)) r = np.fft.ifft2(cross_power_spectrum).real r = np.fft.fftshift(r) return(r) . img1_wimage = img1 * window(&#39;hann&#39;, img1.shape) img2_wimage = img2 * window(&#39;hann&#39;, img2.shape) # work with shifted FFT magnitudes img1_fs = np.abs(fftshift(fft2(img1_wimage))) img2_fs = np.abs(fftshift(fft2(img2_wimage))) . shape = img1_fs.shape radius = shape[0] // 8 # only take lower frequencies img1_fs = warp_polar(img1_fs, radius=radius, output_shape=shape, scaling=&#39;log&#39;, order=0) img2_fs = warp_polar(img2_fs, radius=radius, output_shape=shape, scaling=&#39;log&#39;, order=0) . img1_fs = img1_fs[:shape[0] // 2, :] # only use half of FFT img2_fs = img2_fs[:shape[0] // 2, :] #Compute the corss power spectrum cross_power_spectrum = (img1_fs * np.conjugate(img2_fs)) / np.abs(img1_fs * np.conjugate(img2_fs)) r = np.fft.ifft2(cross_power_spectrum).real r = np.fft.fftshift(r) plt.imshow(r) plt.grid() plt.show() . #shiftr, shiftc = shifts[:2] [py,px] = np.argwhere(r==r.max())[0] theta = (360 / shape[0]) * shiftr img2_rotated = rotate(img2, -theta) #Find translation shift, error, phasediff = phase_cross_correlation(img1, img2_rotated, upsample_factor=10) . Align image . rotated = rotate(img2, -theta) #Find translation #shifts, error, phasediff = phase_cross_correlation(img1, rotated, upsample_factor=10) shifts = fft_phase_correlation(img1, img2): img2_rotated = rotate(img2, -theta) print(theta, shifts) plt.imshow(np.dstack([img1,img2,np.zeros(img1.shape)])) plt.show() shiftr = shifts[0] shiftc = shifts[1] result = scipy.ndimage.fourier_shift(np.fft.fft2(img2_rotated), shift=(shiftr,shiftc)) result = np.fft.ifft2(result).real translated = result plt.imshow(np.dstack([img1,translated,np.zeros(translated.shape)])) plt.show() . .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/2020/12/20/The-Log-Polar-Transform.html",
            "relUrl": "/computer%20vision/2020/12/20/The-Log-Polar-Transform.html",
            "date": " • Dec 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "FFT Phase Correlation",
            "content": "The Context . Finding the geometric relationship between two images is common problem in computer vision. . One method, that can be highly effective, if only a translation exists between two images, is to perform a 2D cross correlation. . When there is alignment between the two images, then the correlation value will be at a maximum. While this is simple and effective, it has the potential to be computationally expensive, given the large number of operations required. . However, it&#39;s possible to solve this problem more efficiently using the Fast Fourier Transform (FFT). . The Algorithm . In particular, we can take advantage of convolution theorm. In particular, The Fourier transform of a convolution of two signals is the pointwise product of their Fourier transforms. In other words, convolution in the spatial domain is multiplication in the frequency domain. . Wikipedia has a great page that goes into more details, but let&#39;s step through an implementation in Python. . Creating test data . First off, let&#39;s load in a bunch of libraries. . import numpy as np import matplotlib.pyplot as plt import skimage.io from skimage import filters from skimage.color import rgb2gray from skimage.filters import window, difference_of_gaussians import scipy plt.rcParams[&#39;figure.figsize&#39;] = [10, 10] . As a test, I&#39;m going to be using an image from the popular video game Call of Duty: Black Ops Cold War. . frame = skimage.io.imread(&#39;data/2020-12-19-FFT-Phase-Correlation/Frame.jpg&#39;) plt.imshow(frame) plt.show() . In particular, I&#39;m going to focus on the mini-map, which you can see in the top left of the image above. . img1 = rgb2gray(frame[50:165,55:170,::-1]) plt.title(&#39;Mini Map&#39;) plt.imshow(img1,cmap=&#39;gray&#39;) plt.show() . Let&#39;s create some synthetic data. Using scipy.ndimage.fourier_shift, we can create a new image with a known shift. . Note that, just as we can use the Fast Fourier Transform to find the shift between two images, we can also use it to shift an image. . shift_x = 10 shift_y = 20 #Shift the img1 10 pixels to the right and 20 pixels down. shifted = scipy.ndimage.fourier_shift(np.fft.fft2(img1), shift=(shift_y,shift_x)) img2 = np.fft.ifft2(shifted).real plt.title(&#39;Side by side&#39;) plt.imshow(np.hstack([img1,img2]),cmap=&#39;gray&#39;) plt.show() . Finding the shift . Following the method described here: . Apply a window function (e.g., a Hamming window) on both images to reduce edge effects. . img1_wimage = img1 * window(&#39;hann&#39;, img1.shape) img2_wimage = img2 * window(&#39;hann&#39;, img2.shape) plt.title(&#39;Windowed&#39;) plt.imshow(img1_wimage,cmap=&#39;gray&#39;) plt.show() . Calculate the discrete 2D Fourier transform of both images. . img1_fs = np.fft.fft2(img1_wimage) img2_fs = np.fft.fft2(img2_wimage) . Calculate the cross-power spectrum by taking the complex conjugate of the second result, multiplying the Fourier transforms together elementwise, and normalizing this product elementwise. . cross_power_spectrum = (img1_fs * np.conjugate(img2_fs)) / np.abs(img1_fs * np.conjugate(img2_fs)) . Obtain the normalized cross-correlation by applying the inverse Fourier transform. . r = np.fft.ifft2(cross_power_spectrum).real . Determine the location of the peak in r. . r = np.fft.fftshift(r) plt.title(&#39;Cross Correlation Map&#39;) plt.imshow(r) plt.grid() plt.show() . We can clearly see the peak of the cross correlation at (37,47), normally the peak would not be so well defined. . [py,px] = np.argwhere(r==r.max())[0] cx,cy = 57,57 shift_x = cx - px shift_y = cy - py print(f&#39;Shift measured X:{shift_x}, Y:{shift_y}&#39;) . Shift measured X:10, Y:20 . Voilà! . We can now measure the shift between two images, when there is only a 2D translation between them. . In the next post, I&#39;m going to look at what to do if there is a rotation, as well as a translation. .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/2020/12/19/FFT-Phase-Correlation.html",
            "relUrl": "/computer%20vision/2020/12/19/FFT-Phase-Correlation.html",
            "date": " • Dec 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Training Data From Openexr",
            "content": "Introduction . At the end of the previous post, I had shown how to use Blender to generate depth maps, and semantic segmentation maps. . However, this infomration is in OpenEXR format, and we need to transform it to a form more suitible for training a computer vision model. . While writing this post, I found this post by Tobias Weis to be very helpful. . The Code . import OpenEXR import Imath import array import numpy as np import matplotlib.pyplot as plt from matplotlib.patches import Rectangle . Extracting . Next, we can use some “boilerplate” code to convert the exr file into a Numpy array. . In this case, we will load in the depth map. . def exr2numpy(exr_path): &#39;&#39;&#39; See: https://excamera.com/articles/26/doc/intro.html http://www.tobias-weis.de/groundtruth-data-for-computer-vision-with-blender/ &#39;&#39;&#39; file = OpenEXR.InputFile(exr_path) dw = file.header()[&#39;dataWindow&#39;] size = (dw.max.x - dw.min.x + 1, dw.max.y - dw.min.y + 1) Float_Type = Imath.PixelType(Imath.PixelType.FLOAT) datastr = file.channel(&#39;R&#39;, Float_Type) data = np.fromstring(datastr, dtype = np.float32).reshape(size[1],-1) return(data) depth = exr2numpy(&quot;Metadata/Depth/Image0001.exr&quot;) . Now we can visualise the depth of each pixel from the camera, in meters. . fig = plt.figure() plt.imshow(depth) plt.colorbar() plt.show() . . Creating bounding boxes . Now we can create bounding boxes for each object in the image. Depnding on what we want to do next, we could generate annotations in COCO format. . semantic_index = exr2numpy(&quot;Metadata/Depth/Image0001.exr&quot;) # Create figure and axes fig,ax = plt.subplots(1) # Display the image ax.imshow(semantic_index) for i in np.unique(semantic_index): #index 0 is the background if i!=0: #Find the location of the object mask yi,xi = np.where(semantic_index == i) #Print the index of the object, and it&#39;s bounding box print(i, np.min(xi), np.max(xi), np.min(yi), np.max(yi)) # Create a Rectangle patch rect = Rectangle(np.min(xi), np.min(yi), np.max(xi)-np.min(xi), np.max(yi)-np.min(yi), linewidth=2, edgecolor=&#39;r&#39;, facecolor=&#39;none&#39;, alpha=0.8) # Add the patch to the Axes ax.add_patch(rect) plt.show() . . Conclusion . Our next step, is to generate a number of training examples, and then use this to train a Computer vision model. .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/blender/2020/10/30/Training-Data-From-OpenEXR.html",
            "relUrl": "/computer%20vision/blender/2020/10/30/Training-Data-From-OpenEXR.html",
            "date": " • Oct 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Synthetic Training Data With Blender",
            "content": "Introduction . Supervised learning in computer vision is fundamentally about building a model that can transform an input x, into an output y. . Using Blender, we have seen how we can generate arbitrary scenes, and in this post I will look at how we can create Semantic Segmentation Maps and Depth Maps. . While writing this post, I found this post by Tobias Weis to be really helpful for understanding rendering nodes. . For a more industrial solution, I suggest looking at the bpycv project on Github. . The Code . Let’s start by importing the bpy librarie, and removing a cube that already exists in the scene. . import os import bpy bpy.data.objects.remove(bpy.data.objects[&#39;Cube&#39;], do_unlink = True) . Objects . Let’s start by creating a ground plane, and then placing 3 dragons on it. . I’m using the dragon model from the Stanford scanning dataset. . A key point to note is that I’m assigning each of the dragons a unique index, or identifier. Later during the rendering process, the renderer will tell us the index of the object that makes up each pixel. This will allow us to generate a pixel by pixel semantic map of the image. . def create_dragon(location, rotation, rgba, index):         #Load the mesh         bpy.ops.import_mesh.ply(filepath=os.getcwd()+&quot;/dragon_vrip.ply&quot;)         ob = bpy.context.active_object #Set active object to variable         ob.scale = (10,10,10)         ob.location = location         ob.rotation_euler = rotation         #Assign the object an index, which is used to generate a semantic segmentation map         bpy.context.object.pass_index = index         #Create and add material to the object         mat = create_dragon_material(&#39;Dragon_&#39;+str(index)+&#39;_Material&#39;,rgba=rgba)         ob.data.materials.append(mat) def create_floor():         bpy.ops.mesh.primitive_plane_add(size=1000, enter_editmode=False, align=&#39;WORLD&#39;, location=(0, 0, 0), scale=(100, 100, 1))         mat = create_floor_material(material_name=&#39;Floor&#39;, rgba =  (0.9, 0.9, 0.9, 0))         activeObject = bpy.context.active_object #Set active object to variable         activeObject.data.materials.append(mat) create_floor() create_dragon(location=(0,0.78,-0.56), rotation=(np.radians(90),0,0), rgba=(0.799, 0.125, 0.0423, 1), index=1) create_dragon(location=(-1.5,4.12,-0.56), rotation=(np.radians(90),0,np.radians(227)), rgba=(0.0252, 0.376, 0.799, 1), index=2) create_dragon(location=(1.04,2.7,-0.56), rotation=(np.radians(90),0,np.radians(129)), rgba=(0.133, 0.539, 0.292, 1), index=3) . Materials . For the dragons, I’m going to create a semi-translucent plastic material, with some subsurface scattering, and a reflective coating. . def create_dragon_material(material_name,rgba):         mat = bpy.data.materials.new(name=material_name)         mat.use_nodes = True         nodes = mat.node_tree.nodes         # Base Color         nodes[&quot;Principled BSDF&quot;].inputs[0].default_value = rgba         # Subsurface         nodes[&quot;Principled BSDF&quot;].inputs[1].default_value = 0.5         # Subsurface Color         nodes[&quot;Principled BSDF&quot;].inputs[3].default_value = rgba                 # Clearcoat         nodes[&quot;Principled BSDF&quot;].inputs[12].default_value = 0.5         return(mat) def create_floor_material(material_name,rgba):         mat = bpy.data.materials.new(name=material_name)         mat.use_nodes = True         nodes = mat.node_tree.nodes         # Base Color         nodes[&quot;Principled BSDF&quot;].inputs[0].default_value = rgba         # Clearcoat         nodes[&quot;Principled BSDF&quot;].inputs[12].default_value = 0.5         return(mat) . Light &amp; Camera . def configure_light():         bpy.data.objects[&quot;Light&quot;].data.type = &#39;AREA&#39;         bpy.data.objects[&quot;Light&quot;].scale[0] = 20         bpy.data.objects[&quot;Light&quot;].scale[1] = 20 def configure_camera():         bpy.data.objects[&quot;Camera&quot;].location = (0,-4.96579,2.45831)         bpy.data.objects[&quot;Camera&quot;].rotation_euler = (np.radians(75),0,0) configure_camera() configure_light() . Renderer . Much of the complexity comes in configuring the renderer. . In particular, we need to create 3 different output nodes, then link the relevant output from the render, to each node. . We also need to configure the renderer to record the object index, which we use for building our semantic map. . def configure_render():         bpy.context.scene.render.engine = &#39;CYCLES&#39;         bpy.context.scene.render.filepath = os.getcwd()+&quot;/Metadata&quot;         #Output open exr .exr files         bpy.context.scene.render.image_settings.file_format = &#39;OPEN_EXR&#39;         bpy.context.scene.cycles.samples = 1         # Configure renderer to record object index         bpy.context.scene.view_layers[&quot;View Layer&quot;].use_pass_object_index = True         # Switch on nodes and get reference         bpy.context.scene.use_nodes = True         tree = bpy.context.scene.node_tree         links = tree.links         ## Clear default nodes         for node in tree.nodes:             tree.nodes.remove(node)         # Create a node for outputting the rendered image         image_output_node = tree.nodes.new(type=&quot;CompositorNodeOutputFile&quot;)         image_output_node.label = &quot;Image_Output&quot;         image_output_node.base_path = &quot;Metadata/Image&quot;         image_output_node.location = 400,0         # Create a node for outputting the depth of each pixel from the camera         depth_output_node = tree.nodes.new(type=&quot;CompositorNodeOutputFile&quot;)         depth_output_node.label = &quot;Depth_Output&quot;         depth_output_node.base_path = &quot;Metadata/Depth&quot;         depth_output_node.location = 400,-100         # Create a node for outputting the index of each object         index_output_node = tree.nodes.new(type=&quot;CompositorNodeOutputFile&quot;)         index_output_node.label = &quot;Index_Output&quot;         index_output_node.base_path = &quot;Metadata/Index&quot;         index_output_node.location = 400,-200         # Create a node for the output from the renderer         render_layers_node = tree.nodes.new(type=&quot;CompositorNodeRLayers&quot;)         render_layers_node.location = 0,0         # Link all the nodes together         links.new(render_layers_node.outputs[0], image_output_node.inputs[0])         links.new(render_layers_node.outputs[2], depth_output_node.inputs[0])         links.new(render_layers_node.outputs[3], index_output_node.inputs[0]) configure_render() . This is what it looks like in the Blender compositing interface when the configuration is complete: . . Finally, we can render the scene. . bpy.ops.render.render(write_still=True) . The Results . Image output . The render has generated 3 different outputs in OpenEXR format, An image output, a depth map and a semantic segmentation map. . . Depth map . . Semantic Segmentation map . . In the next post, I will look at how to work with the OpenEXR maps in Python. .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/blender/2020/10/23/Synthetic-Training-Data-With-Blender.html",
            "relUrl": "/computer%20vision/blender/2020/10/23/Synthetic-Training-Data-With-Blender.html",
            "date": " • Oct 23, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Meshes And Materials In Blender",
            "content": "The script . blender --background --python myscript.py . Let’s walk through what myscript.py could look like: . bpy.data.objects.remove(bpy.data.objects[&#39;Cube&#39;], do_unlink = True) . bpy.ops.mesh.primitive_plane_add(size=1000,location=(0, 0, 0), scale=(1, 1, 1)) . Importing Mesh . def create_bunny(): # Load the mesh bpy.ops.import_scene.obj(filepath=os.getcwd()+&quot;/stanford_bunny.obj&quot;) ob = bpy.data.objects[&quot;stanford_bunny&quot;] ob.scale = (10,10,10) ob.location = (1,0,-0.35) ob.name = &#39;Bunny&#39; #Perform subdivision bevel_mod = ob.modifiers.new(&#39;Subsurf&#39;, &#39;SUBSURF&#39;) bevel_mod.render_levels = 3 . Materials . def create_material(object_name,material_name): mat = bpy.data.materials.new(name=material_name) bpy.data.objects[object_name].active_material = mat mat.use_nodes = True nodes = mat.node_tree.nodes return(nodes) def create_ground_plane_material(object_name,material_name): nodes = create_material(object_name,material_name) # nodes[&quot;Principled BSDF&quot;].inputs[0].default_value = (0.7,0.7,0.7,1) # nodes[&quot;Principled BSDF&quot;].inputs[5].default_value = 1 # nodes[&quot;Principled BSDF&quot;].inputs[7].default_value = 0.1 create_ground_plane_material(&quot;Plane&quot;,&quot;Plane_material&quot;) . def create_bunny_material(object_name,material_name): nodes = create_material(object_name,material_name) # Base Color nodes[&quot;Principled BSDF&quot;].inputs[0].default_value = (0.603828, 1, 0.707399, 1) # Roughness nodes[&quot;Principled BSDF&quot;].inputs[7].default_value = 0.1 # IOR (Index of Refraction) nodes[&quot;Principled BSDF&quot;].inputs[14].default_value = 1.5 # Transmission nodes[&quot;Principled BSDF&quot;].inputs[15].default_value = 1 # Transmission Roughness nodes[&quot;Principled BSDF&quot;].inputs[16].default_value = 0.75 create_bunny_material(&quot;Bunny&quot;,&quot;Bunny_Material&quot;) . Lights, Camera, Render! . def configure_light(): bpy.data.objects[&quot;Light&quot;].data.type = &#39;AREA&#39; bpy.data.objects[&quot;Light&quot;].scale = (10,10,1) bpy.data.objects[&quot;Light&quot;].location = (0,0,6) bpy.data.objects[&quot;Light&quot;].rotation_euler = (0,0,0) def configure_camera(): bpy.data.objects[&quot;Camera&quot;].location = (0.7, -4, 3) bpy.data.objects[&quot;Camera&quot;].rotation_euler = (np.radians(60),0,0) def configure_render(): bpy.context.scene.render.engine = &#39;CYCLES&#39; bpy.context.scene.render.filepath = os.getcwd()+&quot;/render.png&quot; bpy.context.scene.render.resolution_x = 1600 bpy.context.scene.render.resolution_y = 1200 bpy.context.scene.cycles.samples = 2560 configure_light() configure_camera() configure_render() bpy.ops.render.render(write_still=True) . The Results . .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/blender/2020/10/16/Meshes-And-Materials-In-Blender.html",
            "relUrl": "/computer%20vision/blender/2020/10/16/Meshes-And-Materials-In-Blender.html",
            "date": " • Oct 16, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Robotic Blender",
            "content": "What is Blender? . Blender is a software application for open source 3D scene creation and rendering. . Why would we want to automate Blender? . Many reasons, but my real focus is on creating synthetic data for training new machine learning algorithms. I’m really interested in trying to do more, with less annotated data, and mixing real data, with synthetic data is a really promising solution. . Using Blender, we can generate arbitrary amounts of synthetic data, where we can precisely control the scene. Because of this, we can generate metadata at the same time, and try to generate data which better covers the available input space.  . A classic example of this, is the CLEVR dataset, used for Visual Reasoning. . How can we automate Blender? . Blender has a comprehensivley documented API. However, I love using Blender’s scripting mode, to experment with. . In short, we can create a python script, which we can then run using Blender. . blender --background --python myscript.py . Hello World . Let’s walk through what myscript.py could look like: . import os import bpy . Objects . When Blender loads, the default scene already contains a cube, called Cube. Let’s adjust it’s position and scale. . cube_scale = 0.5 bpy.data.objects[&quot;Cube&quot;].scale = (cube_scale,cube_scale,cube_scale) bpy.data.objects[&quot;Cube&quot;].location = (0,0,cube_scale) . Now we can alos create a ground plane, and add that to the scene. . bpy.ops.mesh.primitive_plane_add(size=1000, enter_editmode=False, align=&#39;WORLD&#39;, location=(0, 0, 0), scale=(1, 1, 1)) . Our next task is to create materials for the objects we have added into the scene. . Material . def create_material(object_name,material_name, rgba): mat = bpy.data.materials.new(name=material_name) bpy.data.objects[object_name].active_material = mat mat.use_nodes = True nodes = mat.node_tree.nodes # Base Color nodes[&quot;Principled BSDF&quot;].inputs[0].default_value = rgba # Specular nodes[&quot;Principled BSDF&quot;].inputs[5].default_value = 1 # Roughness nodes[&quot;Principled BSDF&quot;].inputs[7].default_value = 0.1 . create_material(&quot;Cube&quot;,&quot;Cube_material&quot;,(3/255.0, 223/255.0, 252/255.0,1)) create_material(&quot;Plane&quot;,&quot;Plane_material&quot;,(252/255.0, 3/255.0, 235/255.0,1)) . Lights . def configure_light(): bpy.data.objects[&quot;Light&quot;].data.type = &#39;AREA&#39; bpy.data.objects[&quot;Light&quot;].scale[0] = 10 bpy.data.objects[&quot;Light&quot;].scale[1] = 10 configure_light() . Camera . Now let’s configure the camera’s position, and orientation/attitude (Using quaternions). . def configure_camera(): bpy.data.objects[&quot;Camera&quot;].location = (5, -5, 4) bpy.data.objects[&quot;Camera&quot;].rotation_mode = &#39;QUATERNION&#39; bpy.data.objects[&quot;Camera&quot;].rotation_quaternion = (0.892399, 0.369644, 0.099046, 0.239118_ configure_camera() . Action! (Renderer) . Finally, let’s configure the renderer. I’ve chosen to use Cycles, which is a physically based renderer/ray tracer. . def configure_render(): bpy.context.scene.render.engine = &#39;CYCLES&#39; bpy.context.scene.render.filepath = os.getcwd()+&quot;/render.png&quot; bpy.context.scene.render.resolution_x = 1920 bpy.context.scene.render.resolution_y = 1080 configure_render() . And we can finish by rendering the image, and writing it out as render.png. . bpy.ops.render.render(write_still=True) . The Results . .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/blender/2020/10/14/Robotic-Blender.html",
            "relUrl": "/computer%20vision/blender/2020/10/14/Robotic-Blender.html",
            "date": " • Oct 14, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Understanding the impact of timing on defaults",
            "content": "The Context . The thesis of this post is actually pretty simple. There is a delay between when customers make a transaction, and when Afterpay realises that they have defaulted. Because of this delay, combined with the rapid growth in the total value of transactions, defaults as a percentage of transaction value may be artificially reduced. . Important: Obviously I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot. . The Model . First off, let&#39;s load in a bunch of libraries. . %matplotlib inline import matplotlib.pyplot as plt import numpy as np from io import StringIO import pandas as pd import scipy.optimize plt.rcParams[&quot;figure.figsize&quot;] = (10,10) from warnings import filterwarnings filterwarnings(&#39;ignore&#39;) . While reading through Afterpay&#39;s releases to the markets, I came across this chart, which appears on page 3 of this release. Let&#39;s use this to build a simple quadratic model of the reported sales. . . Loading the data . csv_data = StringIO(&#39;&#39;&#39;anz_underlying_sales_value,date,month_count 0,FY15,0 37.3,FY16,12 561.2,FY17,24 2184.6,FY18,36 4314.1,FY19,48 6566.9,FY20,60&#39;&#39;&#39;) df = pd.read_csv(csv_data, sep=&quot;,&quot;) . Fitting a curve . Let&#39;s first fit quadratic: . def quadratic(t, a, b, c): y = a * t**2 + b * t + c return y xdata = df.month_count.values ydata = df.anz_underlying_sales_value.values popt, pcov = scipy.optimize.curve_fit(quadratic, xdata, ydata) print(popt) . [ 2.17012649 -17.61639881 -58.725 ] . x = np.linspace(0,60, 61) y = quadratic(x, *popt) plt.plot(xdata, ydata, &#39;o&#39;, label=&#39;data&#39;) plt.plot(x,y, label=&#39;fit&#39;) plt.title(&#39;ANZ Sales by preceding Financial Year ($M AUD)&#39;) plt.xlabel(&#39;Months after launch&#39;) plt.ylabel(&#39;ANZ Sales ($M AUD)&#39;) plt.legend(loc=&#39;best&#39;) plt.show() . Delays in reporting. . So we found that we could model the annual reported sales as: $$2.170 t^2 - 17.61t - 58.725$$ . The instantanious rate of sales, is given by : $$0.1808t^2 + 0.7021t -9.36$$. . Don&#39;t wory how I arrived at this, I will show how in in the appendix of this post. . t = np.linspace(0,60, 61) sales = 0.1808*t**2 + 0.7021* t - 9.36 plt.plot(sales) plt.title(&#39;ANZ Sales by month ($M AUD)&#39;) plt.xlabel(&#39;Months after launch&#39;) plt.ylabel(&#39;ANZ Sales by month ($M AUD)&#39;) plt.show() . Now let&#39;s model a delay of 6 months between when the transaction happens, and when Afterpay finally realised there was a default. . From this we can see there is a potentially significant difference between the true rate at which losses are occurring, and the rate at which we observe them occurring, at any point in time. . delay = 6 #months true_loss_rate = 0.01 losses_true = true_loss_rate*(0.1808*t**2 + 0.7021* t - 9.36) losses_observed = true_loss_rate*(0.1808*(t-delay)**2 + 0.7021* (t-delay) - 9.36) plt.plot(losses_observed,label=&#39;Observed&#39;) plt.plot(losses_true,label=&#39;True&#39;) plt.legend() plt.title(&#39;ANZ losses by month ($M AUD)&#39;) plt.xlabel(&#39;Months after launch&#39;) plt.ylabel(&#39;ANZ losses by month ($M AUD)&#39;) plt.show() . Now let&#39;s integrate by financial year. . def integrate_by_year(y): integrated = np.array([0,np.sum(y[0:12]),np.sum(y[12:24]),np.sum(y[24:36]),np.sum(y[36:48]),np.sum(y[48:60])]) return(integrated) observed_loss_rate = integrate_by_year(losses_true)/integrate_by_year(losses_observed) plt.plot(observed_loss_rate) plt.title(&#39;Ratio of true losses to observed losses&#39;) plt.xlabel(&#39;Years after launch&#39;) plt.ylabel(&#39;Ratio of true losses to observed losses&#39;) plt.ylim(1,2.5) plt.xlim(2,5) plt.grid() plt.show() . Conclusion . In conclusion, we can clearly see the impact of a delay in recognising losses, in situations where there is rapid growth. Even after years of growth, with a 6 month delay in recognising losses, the true losses could be 30-40% higher than reported. . Appendix . Finding an integral . So we found that we could model the annual reported sales as: $$2.170 t^2 - 17.61t - 58.725$$ . Let&#39;s call this function $f(t)$ . We want to find the function $g(t)$, which is the underlying rate of sales, which I claimed was : $$0.1808t^2 + 0.7021t -9.36$$. . This function, when integrated over 12 months, will give us the annual reported sales. . To help us with the algebraic manipulation, we can use Sympy. An alternative is to do the algebraic manipulation by hand, but this is probably faster and more scalable. . import sympy as sym sym.init_printing(use_unicode=True) a,b,c,d,t = sym.symbols(&#39;a b c d t&#39;) . So we are looking for a quadratic function, the definite integral of which is equal to $$2.170 t^2 - 17.61t - 58.725$$. Let&#39;s start by forming the definite integral. . expr = sym.simplify((a*t**3 + b*t**2 + c*t + d) - (a*(t-12)**3 + b*(t-12)**2 + c*(t-12) + d)) . print(sym.collect(expr,t)) . 36*a*t**2 + 1728*a - 144*b + 12*c + t*(-432*a + 24*b) . fitted_quadratic = t**2 * 2.17012649 + t*-17.61639881 -58.725 . Solving for the coefficients . Let&#39;s now form a set of simultaneous equations, and solve for each of the coefficients of $$t$$. . equations = [] for i in [2,1,0]: eq = sym.collect(expr,t).coeff(t, i) coeff = sym.collect(fitted_quadratic,t).coeff(t, i) equations.append(sym.Eq(eq,coeff)) result = sym.solve(equations,(a,b,c)) print(result) . {a: 0.0602812913888889, b: 0.351046627916667, c: -9.36169642500000} . Finding the derivative . Now all that&#39;s left to do, is to find the derivative of the indefinite integral.  . expr = result[a] * t**3 + result[b]*t**2 + result[c]*t print(sym.diff(expr, t)) . 0.180843874166667*t**2 + 0.702093255833333*t - 9.361696425 . Voila! .",
            "url": "https://cgcooke.github.io/Blog/sympy/finance/afterpay/2020/10/03/Afterpay-Customer-Defaults-Part-7.html",
            "relUrl": "/sympy/finance/afterpay/2020/10/03/Afterpay-Customer-Defaults-Part-7.html",
            "date": " • Oct 3, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "How often do Afterpay's customers default? (Part 6)",
            "content": ". Important: Obviously I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot. . Raison d&#39;&#234;tre . &quot;Buy Now, Pay Later&quot; (BNPL) service providers like Afterpay have stormed onto the scene, attracting the attention of both consumers, as well as investors. . I&#39;m intrigued by Afterpay. I understand that a cohort of consumers exists, who may require the cost of a purchase to be spread over 8 weeks, in order to be able to afford it. However, I fail to see the attraction for most creditworthy customers. Unless Afterpay dramatically starts allowing me to buy shares, I&#39;m not really interested. . However, just because I don&#39;t see the value, doesn&#39;t mean that other people might adopt and love the product. It&#39;s clear that millions of people both in Australia, and overseas regularly use Afterpay. . All things being equal, I can see the consumer benefits of Afterpay, vs other more classical forms of consumer credit (Credit cards, payday loans etc). However, Afterpay&#39;s customers don&#39;t live in a vacuum, and It&#39;s entirely possible that some cohort of their customers may be using a medley of products and services to finance their lifestyle. . However, all of these things exist outside the reality of what Afterpay reports to the ASX, and outside of my focus. I wanted to dig deeper, and try and disentangle what was happening, unobserved beneath the surface.It&#39;s critical that Afterpay protects its brand image, so I understand why Afterpay might prefer to talk at a high level about adverse outcomes. . My goal is, to try and reconstruct the clearest picture possible, even though the official published information forces us to look through a glass, darkly. . What are the numbers? . Approximately 10% of purchases incur a late fee*. | If you are late on one payment, there is a moderate chance that you will be late on at least one of the other 3 (Roughly 20%)*. | Late fees are 12-14 AUD on average*. | All together, this is pretty interesting, and I think brings a lot of clarity to any discussion around Afterpay and late fees. However, what we are seeing are these numbers in isolation. We will never know what trade-offs someone may have made, to pay Afterpay one time. . Next steps . Now that we have a clearer picture of how late fees, I want to turn my focus to defaults. I&#39;m interested in how often people just walk away. This is a topic where Afterpay has provided very little information, apart from a headline figure of losses. Hence I think any clarity would be very valuable in helping us understand Afterpay, and it&#39;s impact, for better or worse on society. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/pymc3/finance/monte%20carlo%20simulation/afterpay/2020/08/29/Afterpay-Customer-Defaults-Part-6.html",
            "relUrl": "/bayesian/pymc3/finance/monte%20carlo%20simulation/afterpay/2020/08/29/Afterpay-Customer-Defaults-Part-6.html",
            "date": " • Aug 29, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "How often do Afterpay transactions attract late fees? (Part 5)",
            "content": "Introduction . Now that we have some more data from Afterpay, on the true rate of purchases that attract late fees (±10%), I wanted to go back and compute the average late fees. This post is effectively an adaption of the work I did in this post. . Important: Obviously I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot. . Average Late Fees . %matplotlib inline import pymc3 as pm import matplotlib.pyplot as plt import numpy as np import math import seaborn as sns import scipy.stats as st plt.rcParams[&quot;figure.figsize&quot;] = (10,10) from warnings import filterwarnings filterwarnings(&#39;ignore&#39;) . with pm.Model() as average_late_fee_model: underlying_sales_aud = pm.Uniform(&#39;underlying_sales_aud&#39;, lower=5.24715*10**9, upper=5.247249*10**9) late_fees_rev_aud = pm.Uniform(&#39;late_fees_rev&#39;, lower=46.05 * 10**6, upper=46.149 * 10**6) average_transaction_value_aud = pm.Uniform(&#39;average_transaction_value&#39;, lower=144.50, upper=154.49) late_payment_rate = pm.Uniform(&#39;late_payment_rate&#39;,lower=9.5,upper=10.5)/100.0 number_of_transactions = pm.Deterministic(&#39;number_of_transactions&#39;, underlying_sales_aud / average_transaction_value_aud) late_transactions = pm.Deterministic(&#39;late_transactions&#39;, late_payment_rate * number_of_transactions) average_late_fee_aud = pm.Deterministic(&#39;average_late_fee_aud&#39;, late_fees_rev_aud / late_transactions) pm.model_to_graphviz(model) with average_late_fee_model: samples = pm.sample_prior_predictive(samples = 100_000) . Results . late_fee_samples = samples[&#39;average_late_fee_aud&#39;] plt.title(&#39;Histogram of late fees&#39;) plt.ylabel(&#39;Relative Frequency&#39;) plt.xlabel(&#39;Late fee (AUD)&#39;) sns.distplot(late_fee_samples,kde=False, norm_hist=True) plt.show() . pm.summary(samples[&quot;average_late_fee_aud&quot;]) . arviz.stats.stats_utils - WARNING - Shape validation failed: input_shape: (1, 100000), minimum_shape: (chains=2, draws=4) . mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . x 13.146 | 0.458 | 12.313 | 13.982 | 0.001 | 0.001 | 99393.0 | 99393.0 | 99248.0 | 99302.0 | NaN | . Conclusion . The late fees that Afterpay customers pay are quite low on average, somewhere between 12 AUD and 14 AUD. This fits with what we observed in previous posts, where customers were typically late on only one of the four payments. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/pymc3/finance/monte%20carlo%20simulation/afterpay/2020/08/29/Afterpay-Customer-Defaults-Part-5.html",
            "relUrl": "/bayesian/pymc3/finance/monte%20carlo%20simulation/afterpay/2020/08/29/Afterpay-Customer-Defaults-Part-5.html",
            "date": " • Aug 29, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "How often do Afterpay transactions attract late fees? (Part 4)",
            "content": "Introduction . Afterpay has just released it&#39;s FY20 Annual Results. I&#39;ve had a quick skim through it, and there were two statistics that caught my eye. . . This is exactly the information we tried to infer over the last couple of posts, and now we have some real ground truth to compare to! . At the end of the last post, I finished by saying that: &quot;We find that between 3 and 12% of transactions are attracting late fees. Overall, our best estimate is 7%.&quot; Hence I&#39;m pretty happy that the true value of ±10%, is within the bounds of what we expected. . Now let&#39;s take this new piece of information, and try to infer more about the underlying, unobserved parameters in our model. . Important: Obviously I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot. . Let&#39;s think about this at a high level. . We now know that approximately 10% of purchases had one or more payments that were late. . This 10% of purchases, represents approximately 10% of payments. . Hence, on average, approximately 30% (±5% points) of payments are late within this cohort. . There are 4 payments, at least one of which is late. . The Model . Let&#39;s model this out in code, the variable names get quite verbose! . In the case of both &quot;percentage_of_purchases_incuring_late_fees&quot; and &quot;ercentage_of_all_transactions_incuring_late_fees&quot;, I&#39;m modelling them as a uniform distribution. This is because Afterpay is rounding both of them to the nearest integer. . %matplotlib inline import pymc3 as pm import matplotlib.pyplot as plt import numpy as np import math import seaborn as sns import scipy.stats as st plt.rcParams[&quot;figure.figsize&quot;] = (10,10) from warnings import filterwarnings filterwarnings(&#39;ignore&#39;) . with pm.Model() as model: percentage_of_purchases_incuring_late_fees = pm.Uniform(&#39;percentage_of_purchases_incuring_late_fees&#39;, lower=9.5, upper=10.49) percentage_of_all_transactions_incuring_late_fees = pm.Uniform(&#39;percentage_of_all_transactions_incuring_late_fees&#39;, lower=2.5, upper = 3.49) percentage_of_late_purchase_transactions_incuring_late_fees = pm.Deterministic(&#39;percentage_of_late_purchase_transactions_incuring_late_fees&#39;, 100 * percentage_of_all_transactions_incuring_late_fees / percentage_of_purchases_incuring_late_fees) average_num_late_purchase_transactions_incuring_late_fees = pm.Deterministic(&#39;average_num_late_purchase_transactions_incuring_late_fees&#39;,4 * percentage_of_late_purchase_transactions_incuring_late_fees / 100) . By looking at the graph, we can see that the two pieces of information that we have been provided, are used in turn to create a distribution of &quot;percentage_of_late_purchase_transactions_incuring_late_fees&quot;. . pm.model_to_graphviz(model) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 average_num_late_purchase_transactions_incuring_late_fees average_num_late_purchase_transactions_incuring_late_fees ~ Deterministic late_fees_rev late_fees_rev ~ Uniform percentage_of_late_purchase_transactions_incuring_late_fees percentage_of_late_purchase_transactions_incuring_late_fees ~ Deterministic late_fees_rev&#45;&gt;percentage_of_late_purchase_transactions_incuring_late_fees percentage_of_purchases_incuring_late_fees percentage_of_purchases_incuring_late_fees ~ Uniform percentage_of_purchases_incuring_late_fees&#45;&gt;percentage_of_late_purchase_transactions_incuring_late_fees percentage_of_late_purchase_transactions_incuring_late_fees&#45;&gt;average_num_late_purchase_transactions_incuring_late_fees Now we can draw samples from our model. . with model: samples = pm.sample_prior_predictive(samples=50_000, random_seed=0) . Results . Average late payments per purchase . We can now visualise the distribution of the number of late transactions per purchase, given that there is one or more late fees. We can notice, that because we have taken the rounding into account, our model shows that sometimes there are less than 1 late payment, per purchase. Obviously this is impossible in practice! . samples = samples[&#39;average_num_late_purchase_transactions_incuring_late_fees&#39;] sns.distplot(samples,bins=np.arange(0.75,1.75,0.01),kde=False,norm_hist=True) plt.xlabel(&#39;Average number of late payments per purchase&#39;) plt.ylabel(&#39;Relative Frequency&#39;) plt.show() . Binomial assumption . Finally we can make one last modelling assumption, to help . Given that one of the 4 payments was late, we might assume that each of the 3 equally, and independently likely to be late as well. IE, we are using the Binomial distribution to model late payments. . Binomial($n$,$p$), where $n$ is 3, and we want to find $p$. . Given the mean of this distribution is $n times p$, we can find $p = frac{(samples - 1)}{3}$. . probability_of_late_payment_per_payment = 100 * (samples - 1) /3.0 . Putting this all together, we can now can find a distribution for $p$. . I&#39;m also going to fit a Beta distribution to it, so that I can draw samples from it in the next post. . probability_of_late_payment_per_payment = probability_of_late_payment_per_payment[np.where(probability_of_late_payment_per_payment&gt;0)] a1, b1, loc1, scale1 = st.beta.fit(probability_of_late_payment_per_payment) print(&#39;Alpha: {:0.4f}, Beta: {:0.4f}, Location: {:0.4f}, Scale: {:0.4f}&#39;.format(a1, b1, loc1, scale1 )) . Alpha: 1.3447, Beta: 1.7199, Location: -0.0142, Scale: 15.5965 . Let&#39;s also visualise the distribution and the fitted Beta distribution. . sns.distplot(probability_of_late_payment_per_payment,fit=st.beta, bins=np.arange(0,20),kde=False,norm_hist=True) plt.xlabel(&#39;Probability of late payment for each payment (%)&#39;) plt.ylabel(&#39;Relative Frequency&#39;) plt.show() . pm.summary(probability_of_late_payment_per_payment) . arviz.stats.stats_utils - WARNING - Shape validation failed: input_shape: (1, 48482), minimum_shape: (chains=2, draws=4) . mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . x 6.898 | 3.844 | 0.051 | 12.912 | 0.018 | 0.012 | 47746.0 | 47746.0 | 47675.0 | 48623.0 | NaN | . Conclusion . Based on what Afterpay has publicly released, we now a more concrete perspective on how often people pay late. . What I found quite interesting, was that while there is a ±10% chance of someone having at least one late payment on any given purchase, the probability of any individual subsequent payment being late is quite low. It&#39;s at most ±13%, with a best guess of 7%. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/pymc3/finance/monte%20carlo%20simulation/afterpay/2020/08/27/Afterpay-Customer-Defaults-Part-4.html",
            "relUrl": "/bayesian/pymc3/finance/monte%20carlo%20simulation/afterpay/2020/08/27/Afterpay-Customer-Defaults-Part-4.html",
            "date": " • Aug 27, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "How often do Afterpay transactions attract late fees? (Part 3)",
            "content": "Introduction . There is a lot going on in this post, but our ultimate goal is to conduct a robustnes study. . From the previous post, I looked at how to model customer late payment rates, from a single distribution. Now I want to turn things up a notch, and model it using a range of distributions, from two different families. . To achive this, we need to: . Compute distributions of late fees rapidly. | Compute the average late fee, under a wide range of different assumptions. | Find the distribution of how frequent late payments are. . Important: Obviously I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot. | %matplotlib inline import pymc3 as pm import matplotlib.pyplot as plt import numpy as np import math import seaborn as sns import scipy.stats as st plt.rcParams[&quot;figure.figsize&quot;] = (10,10) from warnings import filterwarnings filterwarnings(&#39;ignore&#39;) . Improving performance . Direct Model . As part of our simulation, we need to know the distribution of late fees, for any given probability of late payment. . We can directly model this in PyMC3, using the Binomial distribution to model the number of times someone makes a late payment. While this model directly models what happens in reality, it takes 3 seconds to generate 100,000 samples. . def create_late_fee_distribution(late_payment_probability, num_samples=100_000): with pm.Model() as fee_model: number_of_delays_of_less_than_1_week = 1 + pm.Binomial(&#39;number_of_delays_of_less_than_1_week&#39;, n=3, p=late_payment_probability) number_of_delays_of_more_than_1_week = pm.Binomial(&#39;number_of_delays_of_more_than_1_week&#39;, n=number_of_delays_of_less_than_1_week, p=late_payment_probability) total_fees = pm.Deterministic(&#39;total_fees&#39;,10 * number_of_delays_of_less_than_1_week + 7 * number_of_delays_of_more_than_1_week) samples = pm.sample_prior_predictive(samples = num_samples, random_seed=0) return(samples) . Now let&#39;s visualise the distribution we would see, if a customer has a 50% chance of defaulting on any given payment. . samples = create_late_fee_distribution(late_payment_probability = 0.5) sns.distplot(samples[&quot;total_fees&quot;],kde=False, norm_hist=True, bins=np.arange(10,70,1)) plt.title(&#39;Histogram of late fees&#39;) plt.ylabel(&#39;Frequency&#39;) plt.xlabel(&#39;Late fee (AUD)&#39;) plt.show() . Categorical Model . One solution to enable us to sample faster, is to capture the output from the direct model, which precisely models the underlying process, and to create a surrogate model. This model uses a categorical distribution to efficently generate samples with the same distribution. . Using this surrogate model, we can generate 100,000 samples in only 60ms, a 50 fold speedup. . def create_late_fee_surrogate_distribution(late_fee_samples): unique, counts = np.unique(late_fee_samples, return_counts=True) categorical_map = {} for i in range(len(unique)): categorical_map[i] = unique[i] with pm.Model() as categorical_fee_model: late_fee_distribution_categorical = pm.Categorical(&#39;late_fee_distribution_categorical&#39;,counts) return(categorical_fee_model, categorical_map) def generate_samples(categorical_fee_model, categorical_map): with categorical_fee_model: samples = pm.sample_prior_predictive(samples=100_000) late_fee_samples = np.zeros_like(samples[&quot;late_fee_distribution_categorical&quot;]) for i in range(len(categorical_map.keys())): late_fee_samples[samples[&quot;late_fee_distribution_categorical&quot;] == i] = categorical_map[i] return(late_fee_samples) categorical_fee_model, categorical_map = create_late_fee_surrogate_distribution(samples[&quot;total_fees&quot;]) late_fee_samples = generate_samples(categorical_fee_model, categorical_map) plt.title(&#39;Histogram of late fees&#39;) plt.ylabel(&#39;Relative Frequency&#39;) plt.xlabel(&#39;Late fee (AUD)&#39;) sns.distplot(late_fee_samples,kde=False, norm_hist=True, bins=np.arange(10,70,1)) plt.show() . Grid Evaluation . Now we can find the average late fee under a range of different combinations of both late payment probability, and average transaction size. . First off, let&#39;s create a dictionary of categorical models, so we can quickly draw new samples. . fee_models = {} for late_payment_probability_percent in range(0, 101,5): samples = create_late_fee_distribution(late_payment_probability = late_payment_probability_percent/100.0) fee_models[late_payment_probability_percent] = create_late_fee_surrogate_distribution(samples[&quot;total_fees&quot;]) . Now it&#39;s time to generate the grid. . def perform_parametric_study(transaction_value_sampling_function): grid = np.zeros((21,20)) for average_transaction_value_aud in range(50,250,10): transaction_value_samples = transaction_value_sampling_function(average_transaction_value_aud) for late_payment_probability_percent in range(0, 101,5): categorical_fee_model, categorical_map = fee_models[late_payment_probability_percent] #Draw a distribution of late fees late_fee_samples = generate_samples(categorical_fee_model,categorical_map) #Limit the late fees to be at at most 25% of the transaction value late_fees = np.minimum(late_fee_samples, 0.25*transaction_value_samples) #And at least $10 AUD late_fees = np.maximum(late_fees,10) mean_late_fee = np.mean(late_fees) grid[int(late_payment_probability_percent/5), int((average_transaction_value_aud-50)/10)] = mean_late_fee return(grid) . def visualise_grid(grid,title): x_axis_labels = range(50,250,10) y_axis_labels = range(0, 101,5) sns.heatmap(grid,xticklabels=x_axis_labels, yticklabels=y_axis_labels,cbar_kws={&#39;label&#39;: &#39;Average late fee (AUD)&#39;},annot=True) plt.xlabel(&#39;Average Transaction Value (AUD)&#39;) plt.ylabel(&#39;Probability of late payment on any transaction (%)&#39;) plt.title(title) plt.show() . Distributions . One of the big unknowns, is the distribution of the transactions where there are late payments. To provide some robustness in our modelling, I&#39;m using 2 different families of distributions. . The Exponential distribution | The Half Normal distribution | For each family of distributions, I will generate distributions with means in the range of 50-250 AUD. . This in turn will help provide a level of conservatism and robustness to our modelling. . Exponential distribution . x = np.linspace(0, 1500,1500) for average_transaction_value_aud in range(50,250,10): pdf = st.expon.pdf(x, scale = average_transaction_value_aud ) plt.plot(x, pdf,color=&#39;k&#39;,alpha=0.5) plt.xlabel(&#39;Transaction Value (AUD&#39;) plt.ylabel(&#39;Relative Frequency&#39;) plt.show() . def create_exponential_transaction_value_samples(average_transaction_value_aud): with pm.Model() as transaction_value_model: transaction_value_aud = pm.Bound(pm.Exponential, upper=1_500.0)(&#39;transaction_value_aud&#39;, lam = 1/average_transaction_value_aud) transaction_value_samples = pm.sample_prior_predictive(samples=100_000) transaction_value_samples = transaction_value_samples[&#39;transaction_value_aud&#39;] return(transaction_value_samples) exponential_grid = perform_parametric_study(create_exponential_transaction_value_samples) visualise_grid(exponential_grid,&#39;Parametric study of late fees&#39;) . Half Normal Distribution . x = np.linspace(0, 1500,1500) for average_transaction_value_aud in range(50,250,10): sigma = average_transaction_value_aud * np.sqrt(np.pi) / np.sqrt(2) pdf = st.halfnorm.pdf(x, scale=sigma) plt.plot(x, pdf,color=&#39;k&#39;,alpha=0.5) plt.xlabel(&#39;Transaction Value (AUD&#39;) plt.ylabel(&#39;Relative Frequency&#39;) plt.show() . def create_halfnormal_transaction_value_samples(average_transaction_value_aud): sigma = average_transaction_value_aud * np.sqrt(np.pi) / np.sqrt(2) with pm.Model() as transaction_value_model: transaction_value_aud = pm.Bound(pm.HalfNormal,lower=0, upper=1_500.0)(&#39;transaction_value_aud&#39;, sigma = sigma) transaction_value_samples = pm.sample_prior_predictive(samples = 100_000) transaction_value_samples = transaction_value_samples[&#39;transaction_value_aud&#39;] return(transaction_value_samples) halfnormal_grid = perform_parametric_study(create_halfnormal_transaction_value_samples) visualise_grid(halfnormal_grid,&#39;Parametric study of late fees&#39;) . Updating our Priors . In a previous post, we developed a model for calculating the distribution of transactions attracting late fees. One of the assumptions that we used, was that the average late fee was uniformally distributed between 10 AUD and 68 AUD. . Given what we now know, we can find a more realistic distribution. I&#39;ve chosen to go with a Half Normal, which looks like it fits quite well. . late_fees_aggregated = np.stack([halfnormal_grid, exponential_grid]).ravel() offset, std = st.halfnorm.fit(late_fees_aggregated) print(&#39;Offset: {:0.2f}, Standard Deviation: {:0.2f}&#39;.format(offset,std)) sns.distplot(late_fees_aggregated,fit=st.halfnorm, kde=False, bins = np.arange(10,68,5)) plt.ylabel(&#39;Relative Frequency&#39;) plt.xlabel(&#39;Average late fee (AUD)&#39;) plt.show() . Offset: 10.00, Standard Deviation: 14.30 . We can also find the average late fee (21.7 AUD). . pm.summary(late_fees_aggregated) . arviz.stats.stats_utils - WARNING - Shape validation failed: input_shape: (1, 840), minimum_shape: (chains=2, draws=4) . mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . x 21.673 | 8.264 | 10.0 | 36.41 | 2.541 | 1.849 | 11.0 | 11.0 | 10.0 | 40.0 | NaN | . Now let&#39;s re-do the model from this post, except now using an updated distribution for the late fee: . with pm.Model() as model: underlying_sales_aud = pm.Uniform(&#39;underlying_sales_aud&#39;, lower=5.24715*10**9, upper=5.247249*10**9) late_fees_rev_aud = pm.Uniform(&#39;late_fees_rev&#39;, lower=46.05 * 10**6, upper=46.149 * 10**6) average_transaction_value_aud = pm.Uniform(&#39;average_transaction_value&#39;, lower=144.50, upper=154.49) #The updated distribution average_late_fee_aud = 10 + pm.HalfNormal(&#39;average_late_fee&#39;,sigma = 14.298) number_of_transactions = pm.Deterministic(&#39;number_of_transactions&#39;, underlying_sales_aud / average_transaction_value_aud) late_payment_rate = pm.Deterministic(&#39;late_payment_rate&#39;,late_fees_rev_aud / (number_of_transactions * average_late_fee_aud)) . We can now sample this distribution, and find an updated distribution. . with model: samples = pm.sample_prior_predictive(samples=50_000, random_seed=0) . sns.distplot(100*samples[&quot;late_payment_rate&quot;], kde=False, norm_hist=True, bins=100) plt.title(&#39;Distribution of frequency of late payements (%)&#39;) plt.xlabel(&#39;Percentage of transactions with late payment (%)&#39;) plt.ylabel(&#39;Relative Frequency&#39;) plt.show() . pm.summary(100*samples[&quot;late_payment_rate&quot;]) . arviz.stats.stats_utils - WARNING - Shape validation failed: input_shape: (1, 50000), minimum_shape: (chains=2, draws=4) . mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . x 7.11 | 2.644 | 3.011 | 12.23 | 0.012 | 0.008 | 50870.0 | 50799.0 | 50628.0 | 49878.0 | NaN | . Conclusion . Based on our analysis, and making some conservative assumptions, we can see that the average late fee is likely to be in the range of 10 to 36 AUD. . If we use the distribution of late fees as an updated input to the model we developed in this post, we find that between 3 and 12% of transactions are attracting late fees. Overall, our best estimate is 7%. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/pymc3/finance/monte%20carlo%20simulation/afterpay/2020/08/22/Afterpay-Customer-Defaults-Part-3.html",
            "relUrl": "/bayesian/pymc3/finance/monte%20carlo%20simulation/afterpay/2020/08/22/Afterpay-Customer-Defaults-Part-3.html",
            "date": " • Aug 22, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "How often do Afterpay transactions attract late fees? (Part 2)",
            "content": "Introduction . From the previous post, I made some high level estimates for how often Afterpay transactions attracted late fees. If we could better understand the real distribution of late fees, then we could in turn better estimate the frequency of late fees. . Important: Obviously I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot. . What do we know? . Late Fees Revenue : 46.1 million AUD | Average Transaction Value : Approximately 150 AUD     Furthermore, we know that the lowest and highest fee that you can be charged for a single transaction is 10 AUD and 68 AUD. Hence, this in turn bounds the average of the late fees. | . Afterpay is pretty transparent about their late fees, I&#39;ve taken the following from here. . . We can model this policy out in code, keeping in mind that the customer pays Afterpay 4 separate times. . There are some pieces of information that we are missing. . The distribution of values for transactions where customers pay late. | How late they make each payment. | It&#39;s at this stage, we have to accept that we probably can&#39;t find an exact solution, but we can settle for a good approximation. . On assumption we could make is that the number of late payments is uncorrelated with the transaction value, and is uniformly distributed, both for delays of less than, and more than 1 week. . Computing late fees . import random def compute_late_fee(transaction_value,probability_of_late_payment = 0.5): total_fees = 0 number_of_delays_of_less_than_1_week = 1 + np.random.binomial(3,probability_of_late_payment,1)[0] if number_of_delays_of_less_than_1_week &gt; 0: number_of_delays_of_more_than_1_week = np.random.binomial(number_of_delays_of_less_than_1_week,probability_of_late_payment,1)[0] total_fees = 10 * number_of_delays_of_less_than_1_week + 7 * number_of_delays_of_more_than_1_week late_fee = min([total_fees,0.25*transaction_value]) late_fee = max([late_fee,10]) return(late_fee) . %matplotlib inline import pymc3 as pm import matplotlib.pyplot as plt import numpy as np import math import seaborn as sns import scipy.stats as st plt.rcParams[&quot;figure.figsize&quot;] = (10,10) from warnings import filterwarnings filterwarnings(&#39;ignore&#39;) . Modelling transaction values . Let&#39;s model the transaction values. . I&#39;m making two assumption: . Transactions where the customer is late in paying, have the same average transaction value as other payments. | Transaction values are Exponentially distributed. | x = np.linspace(0, 1500, 300) pdf = st.expon.pdf(x, scale=150) plt.plot(x, pdf) plt.xlabel(&#39;Transaction value (AUD)&#39;) plt.ylabel(&#39;Relative Frequency&#39;) plt.show() . with pm.Model() as model: average_transaction_value_aud = pm.Uniform(&#39;average_transaction_value_aud&#39;, lower=144.50, upper=154.49) transaction_value_aud = pm.Bound(pm.Exponential, upper=1_500.0)(&#39;transaction_value_aud&#39;, lam = 1/average_transaction_value_aud) . Now that we have instantiated all of the random variables, we will take 50,000 draws from them, in order to perform our Monte carlo simulation. . with model: samples = pm.sample_prior_predictive(samples=50_000, random_seed=0) . sns.distplot(samples[&quot;transaction_value_aud&quot;], kde=False, norm_hist=True, bins=100) plt.title(&#39;Histogram of transaction values&#39;) plt.xlabel(&#39;Transaction value (AUD)&#39;) plt.ylabel(&#39;Relative Frequency&#39;) plt.show() . As we can see, this distribution has a mean of approximately 150 AUD. . pm.summary(samples[&#39;transaction_value_aud&#39;]) . arviz.stats.stats_utils - WARNING - Shape validation failed: input_shape: (1, 50000), minimum_shape: (chains=2, draws=4) . mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . x 150.103 | 150.269 | 0.0 | 422.672 | 0.675 | 0.477 | 49547.0 | 49547.0 | 49322.0 | 49878.0 | NaN | . Modelling late fees . late_fees = [] for transaction_value in samples[&quot;transaction_value_aud&quot;]: late_fee = compute_late_fee(transaction_value) late_fees.append(late_fee) late_fees = np.array(late_fees) . sns.distplot(late_fees, kde=False, norm_hist=True, bins=np.arange(10,70,1)) plt.xlabel(&#39;Average Late Fee (AUD)&#39;) plt.show() . pm.summary(late_fees) . arviz.stats.stats_utils - WARNING - Shape validation failed: input_shape: (1, 50000), minimum_shape: (chains=2, draws=4) . mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . x 22.585 | 12.514 | 10.0 | 44.0 | 0.056 | 0.04 | 49161.0 | 49161.0 | 48824.0 | 49670.0 | NaN | . Conclusion . We now have the ability to compute the expected average late fee, given a distribution of transaction values, and a probability of making each payment on time.  In our case, we found the average late fee was 37.49 AUD. . However, I don&#39;t think we can really conclude anything meaningful, from just a single distribution of transaction values and a single probability of making each payment on time. . Given this, we can try lots of different distributions, and use this to find the distribution of average late fees that we could see. Next post, I will make the way I do the sampling more computationally efficient. This will allow us to draw some meaningful conclusions. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/pymc3/finance/monte%20carlo%20simulation/afterpay/2020/08/22/Afterpay-Customer-Defaults-Part-2.html",
            "relUrl": "/bayesian/pymc3/finance/monte%20carlo%20simulation/afterpay/2020/08/22/Afterpay-Customer-Defaults-Part-2.html",
            "date": " • Aug 22, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "How often do Afterpay transactions attract late fees? (Part 1)",
            "content": "Introduction . I&#39;m interested in what a typical default with Afterpay looks like. I have probably read hundreds of pages of information published by Afterpay, but I&#39;m yet to see them mention the average default size. . Because I&#39;m curious, and I&#39;m looking for a way to entertain myself on a long train ride, I decided to try and work it out myself. . Important: Obviously I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot. . What do we know? . Late Fees Revenue : 46.1 million AUD (Page 54 FY2019 Annual report) | Average Transaction Value : Approximately 150 AUD (Page 25 FY2019 Annual report)     Furthermore, we know that the lowest and highest fee that you can be charged for a single transaction is 10 AUD and 68 AUD. Hence, this in turn bounds the average of the late fees. | . . Let&#39;s spend a minute thinking about the different paths a transaction could take. . The customer makes good on all their payments, on time. . | The customer makes no payments, including late fees. . | The customer is continually late making payment, but in the end makes all the payments required. . | A combination of 2 and 3, where the customer makes some payments, before ultimately defaulting. . | In the case of 2 and 4, there will be a contribution to GROSS LOSS (Afterpay doesn&#39;t get paid what&#39;s owed in full). . In the case of 3 and 4, there will be a contribution to LATE FEES (Afterpay doesn&#39;t get paid on time). . Let&#39;s now use PyMC3 to perform a Monte Carlo simulation, to estimate how often cases 3 and 4 occour.  . %matplotlib inline import pymc3 as pm import matplotlib.pyplot as plt import numpy as np import math import seaborn as sns import scipy plt.rcParams[&quot;figure.figsize&quot;] = (10,10) from warnings import filterwarnings filterwarnings(&#39;ignore&#39;) . While not strictly necessary, I&#39;m modelling &quot;underlying_sales_aud&quot;, &quot;late_fees_rev_aud&quot; and &quot;average_transaction_value_aud&quot; as random variables, so that they show up in the variable graph. . I&#39;m also going to model average_transaction_value_aud, assuming that they have rounded to the nearest 10 AUD. . with pm.Model() as model: underlying_sales_aud = pm.Uniform(&#39;underlying_sales_aud&#39;, lower=5.24715*10**9, upper=5.247249*10**9) late_fees_rev_aud = pm.Uniform(&#39;late_fees_rev&#39;, lower=46.05 * 10**6, upper=46.149 * 10**6) average_transaction_value_aud = pm.Uniform(&#39;average_transaction_value&#39;, lower=144.50, upper=154.49) average_late_fee_aud = pm.Uniform(&#39;average_late_fee&#39;,lower = 10, upper = 68) number_of_transactions = pm.Deterministic(&#39;number_of_transactions&#39;, underlying_sales_aud / average_transaction_value_aud) late_payment_rate = pm.Deterministic(&#39;late_payment_rate&#39;,late_fees_rev_aud / (number_of_transactions * average_late_fee_aud)) . Now that we have instantiated all of the random variables, we will take 50,000 draws from them, in order to perform our Monte carlo simulation. . with model: samples = pm.sample_prior_predictive(samples=50_000, random_seed=0) . Variable Graph . We can graph the relationship between all our variables. From this we can quickly see which variables are key dependencies.  . pm.model_to_graphviz(model) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 late_fees_rev late_fees_rev ~ Uniform late_payment_rate late_payment_rate ~ Deterministic late_fees_rev&#45;&gt;late_payment_rate number_of_transactions number_of_transactions ~ Deterministic number_of_transactions&#45;&gt;late_payment_rate average_transaction_value average_transaction_value ~ Uniform average_transaction_value&#45;&gt;number_of_transactions average_late_fee average_late_fee ~ Uniform average_late_fee&#45;&gt;late_payment_rate underlying_sales_aud underlying_sales_aud ~ Uniform underlying_sales_aud&#45;&gt;number_of_transactions Results . We can now visualise the distribution of possible values for the late payment rate. . sns.distplot(100*samples[&quot;late_payment_rate&quot;], kde=False, norm_hist=True, bins=100) plt.title(&#39;Distribution of frequency of late payements (%)&#39;) plt.xlabel(&#39;Percentage of transactions with late payment (%)&#39;) plt.ylabel(&#39;Relative Frequency&#39;) plt.show() . From this chart, we can see that there is a high likelyhood that the value of this parameter is bounded between 2 and 14%. . Infact, we can find there is a 94% chance it&#39;s between 1.9% and 9.8% using PyMC3&#39;s summary function. . pm.summary(samples[&#39;late_payment_rate&#39;]) . arviz.stats.stats_utils - WARNING - Shape validation failed: input_shape: (1, 50000), minimum_shape: (chains=2, draws=4) . mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . x 0.043 | 0.025 | 0.019 | 0.097 | 0.0 | 0.0 | 48586.0 | 47975.0 | 49350.0 | 46251.0 | NaN | . Conclusion . Based on the assumptions we made, we can get a high level understanding of how common it is for Afterpay customers to be late in payment. Based on our model, and assumptions, it&#39;s approximately 4.3% of the time. However, this is almost certainly wrong  because: . We made a number of implicit assumptions:  1. All payments are the same size.  2. The average late fee is uniformly distributed between 10 AUD and 68 AUD.   In future posts, I want to further refine the model, to build a more accurate distribution, and narrow down it&#39;s bounds, and try and determine a result we can have more confidence in. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/pymc3/finance/monte%20carlo%20simulation/afterpay/2020/08/21/Afterpay-Customer-Defaults-Part-1.html",
            "relUrl": "/bayesian/pymc3/finance/monte%20carlo%20simulation/afterpay/2020/08/21/Afterpay-Customer-Defaults-Part-1.html",
            "date": " • Aug 21, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Modelling Afterpay's customer growth",
            "content": "The Context . Founded in 2014, Afterpay is a wildly successful Australian fintech startup. With a market capitalisation of circa 14 billion USD, it has rapidly grown to be a one of Australia&#39;s largest companies. In summary, Afterpay let&#39;s customers pay for products in 4 separate payments, and charges stores a margin for this. . I&#39;m interested in applying Bayesian analysis to understand more about Afterpay, based on the information that it has provided to the market, plus a little common sense. . Why Bayesian analysis? After all, we could just use least means squares to fit a curve. . In short, Bayesian analysis will give us a better idea to how wrong we could be. . Important: Obviously I need a disclaimer. If you use anything I say as the basis for any decision, financial or otherwise, you are an idiot. . Now, with that out of the way, let&#39;s get started. . Our goal . While reading through Afterpay&#39;s releases to the markets, I came across this chart, which appears on page 3 of this release. . . Given this graph, you might try to answer some questions: . What is the &quot;saturation level&quot; of Afterpay in the Australian market? | How long will it take to get there? | When will it be growing the fastest? | How confident can you be in your answer? | Bayesian analysis can help us answer these questions. . The Model . First off, let&#39;s load in the libraries we will later need. . %matplotlib inline import pymc3 as pm import matplotlib.pyplot as plt import numpy as np from io import StringIO import pandas as pd import math plt.rcParams[&quot;figure.figsize&quot;] = (10,10) from warnings import filterwarnings filterwarnings(&#39;ignore&#39;) . csv_data = StringIO(&#39;&#39;&#39;millions_customers,date,month_count 0.0,30-sep-2015,0 0.0,31-dec-2015,3 0.0,31-mar-2016,6 0.1,30-jun-2016,9 0.2,30-sep-2016,12 0.4,31-dec-2016,15 0.6,31-mar-2017,18 0.8,30-jun-2017,21 1.1,30-sep-2017,24 1.5,31-dec-2017,27&#39;&#39;&#39;) df = pd.read_csv(csv_data, sep=&quot;,&quot;) plt.plot(df.month_count,df.millions_customers,color=&#39;r&#39;) plt.ylabel(&#39;Millions of customers&#39;) plt.xlabel(&#39;Months after launch&#39;) plt.grid() plt.show() . From this, we can see a rapid, accelerating growth in the number of customers over time. . One model we could use is the sigmoidal model. Also known as &quot;The S shaped curve&quot;, it&#39;s a model where growth starts off slow, accelerates, before slowing again. It&#39;s often used in technology adoption, or the introduction of a new product. . $$ frac{1}{1+e^{-x}}$$ . x = np.arange(-10,10,0.01) y = 1/(1+math.e**-x) plt.plot(x,y,color=&#39;k&#39;) plt.grid() plt.show() . The model can be modified, to modify the scale (L), how fast it grows (k), or when the fastest growth occurs (x0). . $$ frac{L}{1+e^{-k(x-x_0)}}$$ . We can fit this model, to the data provided by Afterpay using PyMC3. . We can also utilise priors, to inject things we might know or suspect. . For example, while I am open minded about what proportion of Australians may choose to become customers of Afterpay, I limit the model to a maximum of 25M customers (population of Australia).  I&#39;m using a Uniform distribution, to impute the fact that I have no firm belief about the final number of customers. . I&#39;m also using uniform priors for $k$, the growth rate, and $x_0$, the time, in months at which growth is the fastest. . In general, this isn&#39;t great practice, because I explicitly exclude the possibility that there could be more than 25M customers, and no amount of data will be able to change my mind. . To quote Cromwell : I beseech you, in the bowels of Christ, think it possible that you may be mistaken. . Let&#39;s compose this as a Bayesian Regression problem. . with pm.Model() as model: millions_customers = df.millions_customers.values x = df.month_count.values.astype(np.float64) L = pm.Uniform(&#39;L&#39;, lower = 0, upper = 25) k = pm.Uniform(&#39;k&#39;, lower=0, upper=1) x0 = pm.Uniform(&#39;x0&#39;, lower=0, upper=100) customers_predicted = L/(1+math.e**(-k*(x-x0))) customers = pm.Normal(&#39;customers&#39;, mu = customers_predicted, sigma = 0.1, observed = millions_customers) . with model: trace = pm.sample(draws=10_000,tune=5_000) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [x0, k, L] Sampling 2 chains, 0 divergences: 100%|██████████| 30000/30000 [01:15&lt;00:00, 397.13draws/s] The number of effective samples is smaller than 25% for some parameters. . The Results . From the following plots, we can notice several things. Firstly, and most importantly, our model converged well. Secondly, we can see that according to our model, we expect the number of customers to most likely saturate below 5m people, but it&#39;s not impossible that we might have a lot more customers, even 25m. . Finally, we can see that there is a lot of uncertainty around the timing of the peak of the growth. This ranges from approximately 2 years to 4 years after the initial introduction of Afterpay. . pm.traceplot(trace); . pm.plot_posterior(trace); . pm.summary(trace) . mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . L 9.545 | 7.107 | 1.393 | 22.628 | 0.144 | 0.102 | 2441.0 | 2441.0 | 2226.0 | 3639.0 | 1.0 | . k 0.154 | 0.037 | 0.105 | 0.227 | 0.001 | 0.001 | 2431.0 | 2359.0 | 2876.0 | 2962.0 | 1.0 | . x0 35.685 | 8.967 | 20.444 | 48.898 | 0.185 | 0.131 | 2339.0 | 2339.0 | 2363.0 | 3996.0 | 1.0 | . Posterior Predictive Checks . Let&#39;s see if our model makes sense. By drawing samples from our model, we can further investigate the potential growth rates of Afterpay. . for i in range(0,1_000): x = np.arange(0,30) plt.plot(x,trace[&#39;L&#39;][i]/(1+math.e**(-trace[&#39;k&#39;][i]*(x-trace[&#39;x0&#39;][i]))),color=&#39;k&#39;,alpha=0.01) plt.plot(df.month_count,df.millions_customers,color=&#39;r&#39;,label=&#39;Reported Customers&#39;) plt.legend() plt.ylabel(&#39;Millions of customers&#39;) plt.xlabel(&#39;Months after launch&#39;) plt.show() . Now let&#39;s start forcasting into the future. We can compare what we modelled to the actual customer numbers (3.1M) reported by Afterpay 54 months after they first launched. Page 6 . for i in range(0,10_000): x = np.arange(0,51) plt.plot(x,trace[&#39;L&#39;][i]/(1+math.e**(-trace[&#39;k&#39;][i]*(x-trace[&#39;x0&#39;][i]))),color=&#39;k&#39;,alpha=0.01) plt.plot(df.month_count,df.millions_customers,color=&#39;r&#39;,label=&#39;Reported Customers&#39;) plt.scatter(51,3.1,label=&#39;Most recent report&#39;) plt.legend() plt.ylabel(&#39;Millions of customers&#39;) plt.xlabel(&#39;Months after launch&#39;) plt.show() . We can also generate a histogram, and compare the predictions of our model with the actual true number of 3.1M. . x = 51*np.ones(20000) y = trace[&#39;L&#39;]/(1+math.e**(-trace[&#39;k&#39;]*(x-trace[&#39;x0&#39;]))) plt.hist(y,bins=np.arange(0,25,0.5),alpha=0.5,density=True) plt.vlines(3.1,0,0.175,label=&#39;Most recent report&#39;) plt.xlabel(&#39;Millions of customers&#39;) plt.legend() plt.grid() plt.show() . Conclusion . In conclusion, Bayesian analysis is a powerful tool, because it allows us to understand how confident we are in our predictions. It&#39;s also powerful because we can feed in information that we might already believe. If you are 80% confident that no more than 20% of the Australian population will use Afterpay, then you can feed this in as a prior. . This type of analysis is also important, as the AFR makes clear, Afterpay&#39;s fate intricately linked to their customer growth. . While this is a fairly rough and ready model It&#39;s interesting. We can see clearly that there is a huge amount of uncertainty in the final number of customers we expect Afterpay to have. . We also need to keep in mind that we might have picked an inappropriate model to fit, which might not appropriately fit reality. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/pymc3/finance/monte%20carlo%20simulation/afterpay/2020/08/13/Modelling-Afterpay's-Customer-Growth.html",
            "relUrl": "/bayesian/pymc3/finance/monte%20carlo%20simulation/afterpay/2020/08/13/Modelling-Afterpay's-Customer-Growth.html",
            "date": " • Aug 13, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Bayesian Camera Calibration",
            "content": "The Context . In a previous post, I attempted to reverse-engineer information about a camera, from a photo it had taken of a scene. While I found a solution, I wasn’t sure how confident I could be in the answer. I was also curious if I could improve the solution by injecting prior knowledge from other sources. . I had the idea to apply Bayesian analysis, and try to find a solution using Makov Chain Monte Carlo and PyMC3. After a bit of searching, I also found this paper, which told me that the idea wasn’t completely outlandish. . In this post, we will combine a prior belief (probability distributions) about some of the camera’s parameters, with measured 2D-3D scene correspondances. By combining these two sources of information, we can compute posterior distributions for each camera parameter. . Because we have a probability distribution, we can understand how certain we are about each parameter. . Let’s start by building a model. . Modelling . import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pymc3 as pm import pandas as pd from warnings import filterwarnings filterwarnings(&#39;ignore&#39;) plt.rcParams[&#39;figure.figsize&#39;] = [10,10] . df = pd.read_csv(&#39;data/2020-07-05-Bayesian-Camera-Calibration/points.csv&#39;,sep =&#39; &#39;) px = df.i.values py = df.j.values X_input = df.X.values Y_input = df.Y.values Z_input = df.Z.values number_points = px.shape[0] points3d = np.vstack([X_input,Y_input,Z_input]).T . Ok, so now that we have loaded in our 2D and 3D point correspondences, we can now turn to representing the camera itself. . Quaternions . For reasons of numerical stability, I’m going to use quaternions to represent the camera’s orientation/attitude in 3D space. . def create_rotation_matrix(Q0,Q1,Q2,Q3):     #Create a rotation matrix from a Quaternion representation of an angle.     R =[[Q0**2 + Q1**2 - Q2**2 - Q3**2, 2*(Q1*Q2 - Q0*Q3), 2*(Q0*Q2 + Q1*Q3)],         [2*(Q1*Q2 + Q0*Q3), Q0**2 - Q1**2 + Q2**2 - Q3**2, 2*(Q2*Q3 - Q0*Q1)],         [2*(Q1*Q3 - Q0*Q2), 2*(Q0*Q1 + Q2*Q3), (Q0**2 - Q1**2 - Q2**2 + Q3**2)]]     return(R) def normalize_quaternions(Q0,Q1,Q2,Q3):     norm = pm.math.sqrt(Q0**2 + Q1**2 + Q2**2 + Q3**2)     Q0 /= norm     Q1 /= norm     Q2 /= norm     Q3 /= norm     return(Q0,Q1,Q2,Q3) . In a previous post, I used sets of parallel lines in the image, to find the locations of vanishing points. . By using these vanishing points, I was able to determine both an estimate for the orientation of the camera, as well as of the intrinsic parameters. . Because we are using quaternions to represent the orientation of the camera, we have 4 different components (X,Y,Z,W). . A prior is a probability distribution on a parameter, and I’m using Student’s T to model this distribution. . Extrinsics . Q1 = pm.StudentT(&#39;Xq&#39;, nu = 1.824, mu = 0.706, sigma = 0.015) Q2 = pm.StudentT(&#39;Yq&#39;, nu = 1.694, mu = -0.298, sigma = 0.004) Q3 = pm.StudentT(&#39;Zq&#39;, nu = 2.015, mu = 0.272, sigma = 0.011) Q0 = pm.StudentT(&#39;Wq&#39;, nu = 0.970, mu = 0.590, sigma = 0.019) . To form a prior estimate for the location of the camera, I’m taking the results we found in this post, I’m taking the solution I generated as an initial estimate. However, I’m going to be open minded, and model the position using a normal distribution, with a standard deviation of 10 meters. . As I mentioned at the end of post, While I found a solution for the location of the camera, It was much lower that what I would have guessed. . I can imagine the camera being about 7-10 meters off the ground, so by using a broad prior, we are saying that this outcome wouldn’t be that surprising, if it was supported by the evidence. . # Define  translation priors X_translate = pm.Normal(&#39;X_translate&#39;, mu = -6.85, sigma = 10) Y_translate = pm.Normal(&#39;Y_translate&#39;, mu = -12.92, sigma = 10) Z_translate = pm.Normal(&#39;Z_translate&#39;, mu = 2.75, sigma = 10) . Now we have to Rotate and Translate the points, in 3D space, according to the attitude and the position of the camera in 3D space. . [R∣t][R | t][R∣t] . Where $t$ is: . t=−RCt = −RCt=−RC . In a previous post, I was able to elegantly use Numpy. . #Camera Center C = camera_params[3:6].reshape(3,1) IC = np.hstack([np.eye(3),-C]) RIC = np.matmul(R,IC) #Make points Homogeneous points = np.hstack([points,np.ones((points.shape[0],1))]) #Perform Rotation and Translation #(n,k), (k,m) -&gt; (n,m) points_proj = np.matmul(points,RIC.T) . However, we are a little constrained with PyMC3, so I explicitly (and inelegantly) carry out this as follows. . RIC_0_3 = R[0][0] * -X_translate + R[0][1] * -Y_translate + R[0][2] * -Z_translate RIC_1_3 = R[1][0] * -X_translate + R[1][1] * -Y_translate + R[1][2] * -Z_translate RIC_2_3 = R[2][0] * -X_translate + R[2][1] * -Y_translate + R[2][2] * -Z_translate X_out = X_est * R[0][0] + Y_est * R[0][1] + Z_est * R[0][2] + RIC_0_3 Y_out = X_est * R[1][0] + Y_est * R[1][1] + Z_est * R[1][2] + RIC_1_3 Z_out = X_est * R[2][0] + Y_est * R[2][1] + Z_est * R[2][2] + RIC_2_3 . Now let’s put it all together, . def Rotate_Translate(X_est, Y_est, Z_est):     #Define rotation priors     Q1 = pm.StudentT(&#39;Xq&#39;, nu = 1.824, mu = 0.706, sigma = 0.015)     Q2 = pm.StudentT(&#39;Yq&#39;, nu = 1.694, mu = -0.298, sigma = 0.004)     Q3 = pm.StudentT(&#39;Zq&#39;, nu = 2.015, mu = 0.272, sigma = 0.011)     Q0 = pm.StudentT(&#39;Wq&#39;, nu = 0.970, mu = 0.590, sigma = 0.019)         Q0,Q1,Q2,Q3 = normalize_quaternions(Q0,Q1,Q2,Q3)         R = create_rotation_matrix(Q0,Q1,Q2,Q3)         # Define  translation priors     X_translate = pm.Normal(&#39;X_translate&#39;, mu = -6.85, sigma = 10)     Y_translate = pm.Normal(&#39;Y_translate&#39;, mu = -12.92, sigma = 10)     Z_translate = pm.Normal(&#39;Z_translate&#39;, mu = 2.75, sigma = 10)         RIC_0_3 = R[0][0] * -X_translate + R[0][1] * -Y_translate + R[0][2] * -Z_translate     RIC_1_3 = R[1][0] * -X_translate + R[1][1] * -Y_translate + R[1][2] * -Z_translate     RIC_2_3 = R[2][0] * -X_translate + R[2][1] * -Y_translate + R[2][2] * -Z_translate         X_out = X_est * R[0][0] + Y_est * R[0][1] + Z_est * R[0][2] + RIC_0_3     Y_out = X_est * R[1][0] + Y_est * R[1][1] + Z_est * R[1][2] + RIC_1_3     Z_out = X_est * R[2][0] + Y_est * R[2][1] + Z_est * R[2][2] + RIC_2_3         return(X_out, Y_out, Z_out) . Intrinsics . For the intrinsics, I’m using a mixture of priors from the results of our optimisation, as well as what was identified in this post. . focal_length = pm.Normal(&#39;focal_length&#39;,mu = 2189.49, sigma = 11.74)       k1 = pm.Normal(&#39;k1&#39;, mu = -0.327041, sigma = 0.5 * 0.327041) k2 = pm.Normal(&#39;k2&#39;, mu = 0.175031,  sigma = 0.5 * 0.175031) k3 = pm.Normal(&#39;k3&#39;, mu = -0.030751, sigma = 0.5 * 0.030751) c_x = pm.Normal(&#39;c_x&#39;, mu = 2268/2.0, sigma = 1000) c_y = pm.Normal(&#39;c_y&#39;, mu = 1503/2.0, sigma = 1000) . with pm.Model() as model:     X, Y, Z = Rotate_Translate(points3d[:,0], points3d[:,1], points3d[:,2])         focal_length = pm.Normal(&#39;focal_length&#39;,mu = 2189.49, sigma = 11.74)           k1 = pm.Normal(&#39;k1&#39;, mu = -0.327041, sigma = 0.5 * 0.327041)     k2 = pm.Normal(&#39;k2&#39;, mu = 0.175031,  sigma = 0.5 * 0.175031)     k3 = pm.Normal(&#39;k3&#39;, mu = -0.030751, sigma = 0.5 * 0.030751)         c_x = pm.Normal(&#39;c_x&#39;, mu = 2268/2.0, sigma = 1000)     c_y = pm.Normal(&#39;c_y&#39;, mu = 1503/2.0, sigma = 1000)         px_est = X / Z     py_est = Y / Z         #Radial distortion     r = pm.math.sqrt(px_est**2 + py_est**2)         radial_distortion_factor = (1 + k1 * r + k2 * r**2 + k3 * r**3)     px_est *= radial_distortion_factor     py_est *= radial_distortion_factor         px_est *= focal_length     py_est *= focal_length     px_est += c_x     py_est += c_y         error_scale = 5 #px         delta = pm.math.sqrt((px - px_est)**2 + (py - py_est)**2)         # Define likelihood     likelihood = pm.Normal(&#39;rms_pixel_error&#39;, mu = delta, sigma = error_scale, observed=np.zeros(number_points)) . Finally we can use Markov Chain Monte Carlo (MCMC) to find the posterior distribution of the parameters. . with pm.Model() as model:     # Inference!     trace = pm.sample(draws=10_000, init=&#39;adapt_diag&#39;, cores=4, tune=5_000) . Results . Now that the MCMC sampling has finished, let’s look at the results: . pm.plot_posterior(trace); . . From this we can see two things. In the left hand column, we can see the distribution of potential values for each parameter. In the right hand column, we can see how the MCMC sampler moved through this space over time. . pm.summary(trace) . . A number of things stand out. . Firstly, the Z_centroid of the camera is now 7.44 meters off the ground, with a very small standard deviation (8.5cm). . Secondly, the y coordinate of the inferred principle point (c_y) is much closer to what we expect. Previously we found that it was located at ±2650 pixels, well outside the image. Now we find that it’s somewhere in the broad region of ±1000 pixels. . All things considered, we now have a solution where we understand how confident we are in each parameter, and is a solution that is more reasonable, than what we found in this post. . If we found more data, we could further refine our estimates, by using the posterior results as new priors. This is exciting, as it gives us the framework to evolve and update both our results, and how confident we are, over time. . Additional plots . pm.pairplot(trace, var_names=[&#39;X_translate&#39;,&#39;Y_translate&#39;,&#39;Z_translate&#39;], divergences=True); . . pm.pairplot(trace, var_names=[&#39;k1&#39;, &#39;k2&#39;, &#39;k3&#39;], divergences=True); . . pm.pairplot(trace, var_names=[&#39;c_x&#39;, &#39;c_y&#39;], divergences=True); . . pm.pairplot(trace, var_names=[&#39;Wq&#39;, &#39;Xq&#39;,&#39;Yq&#39;,&#39;Zq&#39;], divergences=True); . . sns.jointplot(trace[:][&#39;X_translate&#39;], trace[:][&#39;Y_translate&#39;], kind=&quot;hex&quot;); . . Conclusion . I’ve been planning to write this post for about a few months, but to get here was an interesting journey, as I needed to first put many of the building blocks in place. . I’m still on my bayesian modelling journey, but I’ve been inspired by others along the way, in particular, this awesome post. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/pymc3/computer%20vision/2020/07/07/Bayesian-Camera-Calibration.html",
            "relUrl": "/bayesian/pymc3/computer%20vision/2020/07/07/Bayesian-Camera-Calibration.html",
            "date": " • Jul 7, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Vanishing points in practice",
            "content": "The Context . In a previous post, I used optimisation to try to calibrate a camera. Ultimately, while we arrived at a plausible solution, I was left with some questions. . In particular, I wasn&#39;t sure how much I could trust this solution. The principle point of the image, which is normally located near the center of the image, was located far outside it. . This in of itself isn&#39;t fatal. The image could have been a crop of a photo, however it&#39;s extremely unusual. . If we want a more accurate answer, we can break this down into a two step process, firstly, computing estimates of the attitude and the camera calibration matrix. . We can then combine these estimates with the points we have measured previously, to produce a new estimate of the position and attitude of the camera. I will cover this in the next post. . We can compute the attitude and camera matrix using the method shown in a couple of previous blog posts. . Finding vanishing points. . Firstly, let&#39;s start by finding the vanishing points in the image. I&#39;ve described how this happens in more detail in this [post] (https://cgcooke.github.io/Blog/computer%20vision/linear%20algebra/monte%20carlo%20simulation/2020/04/10/Finding-Vanishing-Points.html). . Let&#39;s start by loading in the libraries we will need. . from PIL import Image import matplotlib.pyplot as plt import json import numpy as np import scipy.linalg import seaborn as sns import scipy.stats from scipy.spatial.transform import Rotation as Rot plt.rcParams[&#39;figure.figsize&#39;] = [15, 15] np.random.seed(1) . img = Image.open(&#39;data/2020-02-23-An-Adventure-In-Camera-Calibration/A300.jpg&#39;) plt.imshow(img) plt.show() . Maximum Likelihood Estimate . Let&#39;s now load in the annotations I have made to this image. . JSON = json.loads(open(&#39;data/2020-07-06-Vanishing-Points-In-Practice/A300.json&#39;,&#39;r&#39;).read()) . def intersect_multiple_lines(P0,P1): &quot;&quot;&quot;P0 and P1 are NxD arrays defining N lines. D is the dimension of the space. This function returns the least squares intersection of the N lines from the system given by eq. 13 in http://cal.cs.illinois.edu/~johannes/research/LS_line_intersect.pdf. &quot;&quot;&quot; # generate all line direction vectors n = (P1-P0)/np.linalg.norm(P1-P0,axis=1)[:,np.newaxis] # normalized # generate the array of all projectors projs = np.eye(n.shape[1]) - n[:,:,np.newaxis]*n[:,np.newaxis] # I - n*n.T # generate R matrix and q vector R = projs.sum(axis=0) q = (projs @ P0[:,:,np.newaxis]).sum(axis=0) # solve the least squares problem for the # intersection point p: Rp = q p = np.linalg.lstsq(R,q,rcond=None)[0] return(p) . def load_line_data(point_name): P0 = [] P1 = [] for shape in JSON[&#39;shapes&#39;]: points = shape[&#39;points&#39;] if shape[&#39;label&#39;] == point_name: P0.append(points[0]) P1.append(points[1]) P0 = np.array(P0,dtype=np.float64) P1 = np.array(P1,dtype=np.float64) return(P0,P1) def find_vanishing_point(point_name): P0,P1 = load_line_data(point_name) p = intersect_multiple_lines(P0,P1).ravel() return(p) . Using the annotations I manually created of the parallel lines in the images, Let&#39;s compute 3 different vanishing points. In any image, there are an infinite set of parallel lines, and thus vanishing points, however we typically live in a manhattan world, of orthogonal 90 degree angles. Hence in most scenes, there are typically 3 different vanishing points.  . vanishing_points = {} for point_name in [&#39;VP1&#39;,&#39;VP2&#39;,&#39;VP3&#39;]: vanishing_points[point_name] = find_vanishing_point(point_name) plt.imshow(img) for point_name,color in [(&#39;VP1&#39;,&#39;g&#39;),(&#39;VP2&#39;,&#39;r&#39;),(&#39;VP3&#39;,&#39;b&#39;)]: vp = vanishing_points[point_name] p0,p1 = load_line_data(point_name) print(point_name,vp) plt.scatter(vp[0],vp[1],color=color,label=point_name) for i in range(0,p0.shape[0]): plt.plot([p0[i,0],p1[i,0]],[p0[i,1],p1[i,1]], color=color,alpha=0.5) plt.plot([vp[0],p1[i,0]],[vp[1],p1[i,1]], color=color,alpha=0.5) plt.legend() plt.ylim(1500,0) plt.show() . VP1 [2908.86609693 665.60570984] VP2 [-1634.06911245 479.56926814] VP3 [2.45111941e+01 2.60455689e+04] . Monte Carlo Simulation . Ok, so this is a good start, we can find the MLE estimate for the location of the vanishing points. However, I&#39;m also interested in the distribution of possible locations, given that I most likely made mistakes when annotating the points. . I&#39;m not sure of exactly how large the mistakes could be, but let&#39;s start by assuming that the standard deviation of the error was 1 pixel, and then perform a monte carlo simulation. . def monte_carlo_simulation(point_name,num_samples=1_000): P0,P1 = load_line_data(point_name) point_error_magnitude = 1 #px vanishing_points = [] for i in range(0,num_samples): P0_stochastic = P0 + point_error_magnitude * np.random.randn(P0.shape[0],P0.shape[1]) P1_stochastic = P1 + point_error_magnitude * np.random.randn(P1.shape[0],P1.shape[1]) p = intersect_multiple_lines(P0_stochastic,P1_stochastic) vanishing_points.append(p) vanishing_points = np.asarray(vanishing_points) return(vanishing_points) . plt.imshow(img) for point_name,color in [(&#39;VP1&#39;,&#39;g&#39;),(&#39;VP2&#39;,&#39;r&#39;),(&#39;VP3&#39;,&#39;b&#39;)]: vanishing_points = monte_carlo_simulation(point_name) for p in vanishing_points: plt.scatter(p[0],p[1],color=color,alpha=0.1) plt.ylim(1500,0) plt.show() . Finding the Intrinsic Matrix . def generate_A(vanishing_points): A = [] for (point_name_1, point_name_2) in [(&#39;VP1&#39;,&#39;VP2&#39;),(&#39;VP2&#39;,&#39;VP3&#39;),(&#39;VP3&#39;,&#39;VP1&#39;)]: vp1 = vanishing_points[point_name_1] vp2 = vanishing_points[point_name_2] x1,y1 = vp1 x2,y2 = vp2 w1 = x2*x1 + y2*y1 w2 = x2 + x1 w3 = y2 + y1 w4 = 1 A.append([w1,w2,w3,w4]) A = np.array(A) return(A) def compute_K(A): w = scipy.linalg.null_space(A).ravel() w1 = w[0] w2 = w[1] w3 = w[2] w4 = w[3] omega = np.array([[w1,0,w2], [0,w1,w3], [w2,w3,w4]]) K = np.linalg.inv(np.linalg.cholesky(omega)).T K/=K[2,2] return(K) . Finding the Rotation Matrix . def make_homogeneous(x): x_homogeneous = np.array([x[0],x[1],1]) return(x_homogeneous) def compute_R(K,vanishing_points): v_x_h = make_homogeneous(vanishing_points[&#39;VP1&#39;]) v_y_h = make_homogeneous(vanishing_points[&#39;VP2&#39;]) v_z_h = make_homogeneous(vanishing_points[&#39;VP3&#39;]) K_inv = np.linalg.inv(K) R_1 = np.dot(K_inv,v_x_h)/np.linalg.norm(np.dot(K_inv,v_x_h)).T R_2 = np.dot(K_inv,v_y_h)/np.linalg.norm(np.dot(K_inv,v_y_h)).T R_3 = np.cross(R_1,R_2) R = np.vstack([R_1,R_2,R_3]) return(R) . Ok, now let&#39;s visualize the results from 1,000 iterations of the monte carlo simulation. . from mpl_toolkits.mplot3d import Axes3D PTS1 = monte_carlo_simulation(point_name=&#39;VP1&#39;) PTS2 = monte_carlo_simulation(point_name=&#39;VP2&#39;) PTS3 = monte_carlo_simulation(point_name=&#39;VP3&#39;) fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) for i in range(0,1_000): vanishing_points = {} vanishing_points[&#39;VP1&#39;] = (PTS1[i,0,0],PTS1[i,1,0]) vanishing_points[&#39;VP2&#39;] = (PTS2[i,0,0],PTS2[i,1,0]) vanishing_points[&#39;VP3&#39;] = (PTS3[i,0,0],PTS3[i,1,0]) try: A = generate_A(vanishing_points) K = compute_K(A) R = compute_R(K,vanishing_points) ax.plot(xs=[0,R[0,0]], ys=[0,R[1,0]], zs = [0,R[2,0]],color=&#39;r&#39;,alpha=0.01) ax.plot(xs=[0,R[0,1]], ys=[0,R[1,1]], zs = [0,R[2,1]],color=&#39;g&#39;,alpha=0.01) ax.plot(xs=[0,R[0,2]], ys=[0,R[1,2]], zs = [0,R[2,2]],color=&#39;b&#39;,alpha=0.01) except: pass plt.show() . Parameter distributions . Quaternions . From the above visualisation, we can see that we have a distribution of attitudes. Let&#39;s make a subtle change, and find the quaternion representation of the angles. . Quaternions are an elegant way to represent a rotation in 3 dimensions, using 4 numbers. Without going into details, they have many useful properties. If you want to learn more, 3Blue1Brown has done some incredible videos explaining them in more details. . Let&#39;s do a Monte carlo simulation, and compute both the means, and variances of the attitudes. . quats = [] Ks = [] PTS1 = monte_carlo_simulation(point_name=&#39;VP1&#39;,num_samples = 10_000) PTS2 = monte_carlo_simulation(point_name=&#39;VP2&#39;,num_samples = 10_000) PTS3 = monte_carlo_simulation(point_name=&#39;VP3&#39;,num_samples = 10_000) for i in range(0,10_000): vanishing_points = {} vanishing_points[&#39;VP1&#39;] = (PTS1[i,0,0],PTS1[i,1,0]) vanishing_points[&#39;VP2&#39;] = (PTS2[i,0,0],PTS2[i,1,0]) vanishing_points[&#39;VP3&#39;] = (PTS3[i,0,0],PTS3[i,1,0]) try: A = generate_A(vanishing_points) K = compute_K(A) R = compute_R(K,vanishing_points) R = R.T r = Rot.from_matrix(R) Ks.append(K.ravel()) quats.append(r.as_quat()) except: pass Ks = np.vstack(Ks) quats = np.vstack(quats) . sns.distplot(quats[:,0],kde=False,fit=scipy.stats.t,norm_hist=True,bins=100,label=&#39;X&#39;); sns.distplot(quats[:,1],kde=False,fit=scipy.stats.t,norm_hist=True,bins=100,label=&#39;Y&#39;); sns.distplot(quats[:,2],kde=False,fit=scipy.stats.t,norm_hist=True,bins=100,label=&#39;Z&#39;); sns.distplot(quats[:,3],kde=False,fit=scipy.stats.t,norm_hist=True,bins=100,label=&#39;W&#39;); plt.legend() plt.show() . Ok, now let&#39;s fit Student&#39;s T distribution to each quaternion component: . component_names = [&#39;X&#39;,&#39;Y&#39;,&#39;Z&#39;,&#39;W&#39;] for i in range(0,4): nu, mu, sigma = scipy.stats.distributions.t.fit(quats[:,i]) print(&quot;{}: nu: {:.3f}, mu: {:.3f}, sigma: {:.3f}&quot;.format(component_names[i],nu,mu,sigma)) . X: nu: 1.824, mu: 0.706, sigma: 0.015 Y: nu: 1.694, mu: -0.298, sigma: 0.004 Z: nu: 2.015, mu: 0.272, sigma: 0.011 W: nu: 0.970, mu: 0.590, sigma: 0.019 . Focal Length . Now let&#39;s do the same for the focal length: . sns.distplot(Ks[:,0],kde=False,fit=scipy.stats.norm,label=&#39;Focal length&#39;); plt.legend() plt.show() mean, var = scipy.stats.distributions.norm.fit(Ks[:,0]) print(&quot;Mean: {:.2f}, Std: {:.2f}&quot;.format(mean,np.sqrt(var))) . Mean: 2189.49,Std: 11.74 . Principle Point . Finally, let&#39;s see what we think the principle point of the camera might be: . sns.distplot(Ks[:,2],kde=False,fit=scipy.stats.norm,label=&#39;cx&#39;); plt.legend() plt.show() mean, var = scipy.stats.distributions.norm.fit(Ks[:,2]) print(&quot;Mean (CX): {:.2f}, Std: {:.2f}&quot;.format(mean,np.sqrt(var))) sns.distplot(Ks[:,5],kde=False,fit=scipy.stats.norm,label=&#39;cy&#39;); plt.legend() plt.show() mean, var = scipy.stats.distributions.norm.fit(Ks[:,5]) print(&quot;Mean (CY): {:.2f}, Std: {:.2f}&quot;.format(mean,np.sqrt(var))) . Mean (CX): 845.77, Std: 9.73 . Mean (CY): 1032.71, Std: 17.25 . Now let&#39;s plot the joint distribution of th.e principle point in 2D space. . sns.jointplot(Ks[:,2],Ks[:,5], kind=&quot;hex&quot;); .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/linear%20algebra/monte%20carlo%20simulation/2020/07/06/Vanishing-Points-In-Practice.html",
            "relUrl": "/computer%20vision/linear%20algebra/monte%20carlo%20simulation/2020/07/06/Vanishing-Points-In-Practice.html",
            "date": " • Jul 6, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "On Target With PyMC3",
            "content": "The context . Last weekend I was relaxing in the countryside at a colleague&#39;s château, and we decided to take turns shooting at a target with an air rifle. . Unfortunately, we were all terrible shots, and the rifle was uncalibrated, so we didn&#39;t do a particularly good job of hitting the target. . In order to improve our aim, we need to adjust the rifle sights so that the pellet impacts are centered on the middle of the target. . The model . Let&#39;s start modelling this situation in Python. . import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pymc3 as pm from PIL import Image from warnings import filterwarnings filterwarnings(&#39;ignore&#39;) plt.rcParams[&#39;figure.figsize&#39;] = [20,20] np.random.seed(1337) . Ok, now let&#39;s pretend that our rifle is incorrectly sighted/calibrated such that, when we are aiming at exactly at the center of the target, our pellets will land 120 units to the right, and 80 pixels down. . Let&#39;s also pretend that they will be normally distributed around this point, with a standard deviation of 85 pixels in each axis due to the wind/humidity/our terrible aim. . This is our generative model, and we use it to generate some fake data. We will later use the same model construct, and try and infer the x_offset and y_offset. . Let&#39;s go ahead and randomly generate the results of 5 shots at the target. . x_offset = 120 #units/pixels y_offset = -80 #units/pixels standard_deviation = 85 #units/pixels num_samples = 5 x_observed = np.random.normal(x_offset,standard_deviation,5) y_observed = np.random.normal(y_offset,standard_deviation,5) . img = Image.open(&#39;data/2020-07-02-On-Target-With-PyMC3/1000px-10_m_Air_Rifle_target.svg.png&#39;) plt.imshow(img) plt.scatter(x_observed+500,500-y_observed,alpha=0.9,s = 1000) plt.grid() plt.show() . Now, given we have observed this target. What adjustments should we make in order to improve our aim? . As part of Bayesian analysis, we need to provide a prior distribution, which tells us what plausible values of x_offset and y_offset could be. . Based on the impact locations, PyMC3 will try to infer potential values x_offset and y_offset, which are the adjustments we need to make to our rifle. . The beauty of Baysian analysis is that we don&#39;t get a single value as a result, but a distribution of values. This allows us to understand how certain we can be about the results. . with pm.Model() as model: #Set up our model x_offset = pm.Normal(&#39;x_offset&#39;,mu = 0, sigma=250) y_offset = pm.Normal(&#39;y_offset&#39;,mu = 0, sigma=250) standard_deviation = pm.HalfNormal(&#39;standard_deviation&#39;,sigma=200) impact_x = pm.Normal(&#39;impact_x&#39;, mu = x_offset, sigma = standard_deviation, observed = x_observed) impact_y = pm.Normal(&#39;impact_y&#39;, mu = y_offset, sigma = standard_deviation, observed = y_observed) . Now that we have finished setting up our model, we can use Markov Chain Monte-Carlo (MCMC) to infer what x_offset and y_offset could be. . with pm.Model() as model: #The magic line that trace = pm.sample(draws=10_000, tune=1_000) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [standard_deviation, y_offset, x_offset] Sampling 2 chains, 1 divergences: 100%|██████████| 22000/22000 [00:09&lt;00:00, 2309.14draws/s] There was 1 divergence after tuning. Increase `target_accept` or reparameterize. . Results . Now it&#39;s time to look at the results, PyMC3 provides a number of options for understanding them. . Let&#39;s start with a numerical summary. From this we can see that mean value of x_offset is 65.6 and y_offset is -109.7. From this, we can see that our best guess of where we need to aim is 65.6 units to the left, and -109.7 units up. . pm.summary(trace) . mean sd hpd_3% hpd_97% mcse_mean mcse_sd ess_mean ess_sd ess_bulk ess_tail r_hat . x_offset 65.645 | 50.662 | -36.013 | 156.687 | 0.420 | 0.327 | 14566.0 | 12024.0 | 15361.0 | 10949.0 | 1.0 | . y_offset -109.744 | 50.016 | -203.327 | -13.457 | 0.475 | 0.336 | 11107.0 | 11107.0 | 11512.0 | 9532.0 | 1.0 | . standard_deviation 109.028 | 31.722 | 59.562 | 165.361 | 0.324 | 0.235 | 9578.0 | 9085.0 | 10639.0 | 11167.0 | 1.0 | . A traceplot is a useful way to diagnose what is going on. I&#39;m yet to find a really good tutorial on how to interpret it, but here are a few key points that I&#39;ve picked up. . Left Hand Column: Curves should closely overlap. If they don&#39;t, then it means that you can&#39;t rely on the results. . | Right Hand Column: The chart should look like a &quot;Fuzzy Caterpillar&quot;. This means that you are effectively exploring the parameter space. . | . pm.traceplot(trace); . The posterior plot tells you what values your parameters are likely to have. For example, according to our model, there is a 94% chance that x_offset is between -36 and 157.  . pm.plot_posterior(trace); . Finally, because we have two variables, we can plot them together, and understand their joint distribution using Seaborn. . plot = sns.jointplot(trace[:][&#39;x_offset&#39;]+500, trace[:][&#39;y_offset&#39;]+500, kind=&quot;hex&quot;); plt.show() . Putting it all together, we can visualise the potential locations (in red) for the centroid of the actual location of where the pellets will land. With only 5 samples observed, there is a huge amount of uncertainty. As the number of samples increases, this uncertainty would decrease. . We can also see that the green point, which marks the true offset is within this distribution of red potential locations. . plt.imshow(img) plt.scatter(trace[:][&#39;x_offset&#39;]+500,500-trace[:][&#39;y_offset&#39;],alpha=0.5,s = 1,color=&#39;r&#39;) plt.scatter(x_observed+500,500-y_observed,alpha=0.9,s = 1000) plt.scatter(120+500,500--80,alpha=0.9,s = 1000,color=&#39;g&#39;) plt.grid() plt.show() . No handles with labels found to put in legend. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/pymc3/computer%20vision/2020/07/02/On-Target-With-PyMC3.html",
            "relUrl": "/bayesian/pymc3/computer%20vision/2020/07/02/On-Target-With-PyMC3.html",
            "date": " • Jul 2, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Visualising PUBG Deaths with Datashader",
            "content": "While browsing Kaggle, I cam across this interesting dataset, and I thought it would form the basis for some interesting blog posts. . The dataset contains 65M player deaths, from 720,000 different matches, from PlayerUnknown&#39;s Battlegrounds (PUBG), a wildly popular online game. . An Introduction to PUBG . Wikipedia sums up the aim of the game pretty well: . &quot;In the game, up to one hundred players parachute onto an island and scavenge for weapons and equipment to kill others while avoiding getting killed themselves. The available safe area of the game&#39;s map decreases in size over time, directing surviving players into tighter areas to force encounters. The last player or team standing wins the round.&quot; . But for something bit less dry, but just as accurate, there is this video on Youtube. . Data preprocessing . First off, let&#39;s load some of the libraries we will later need. . import glob import pandas as pd import datashader as ds import datashader.transfer_functions as tf import numpy as np import matplotlib.pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [15, 15] . Bad key &#34;text.kerning_factor&#34; on line 4 in /opt/anaconda3/envs/PyMC3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle. You probably need to get an updated matplotlibrc file from https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template or from the matplotlib source distribution . The dataset itself comes in a number of different .csv files, which we will load, and concatentate. . def load_deaths(): li = [] for filename in glob.glob(&quot;/Users/cooke_c/Documents/Blog_Staging/PUBG/9372_13466_bundle_archive/deaths/*.csv&quot;): df = pd.read_csv(filename) df = df.drop([&#39;match_id&#39;,&#39;victim_placement&#39;,&#39;killed_by&#39;,&#39;killer_name&#39;,&#39;killer_placement&#39;,&#39;killer_position_x&#39;,&#39;killer_position_y&#39;,&#39;victim_name&#39;],axis=&#39;columns&#39;) li.append(df) df = pd.concat(li, axis=0, ignore_index=True) return(df) . deaths_df = load_deaths() . Matches in PUBG are limited in time to approximately 32.5 minutes. Let&#39;s create a new categorical variable called &quot;phase&quot;. It will represent which of the following match phases a player died in: . Early (0-10m) (Lime Green points) | Mid Phase (10-25m) (Cyan points) | Late Phase (&gt;25m) (Purple points) | def create_phase_category(deaths_df): conditions = [ (1*60&lt;deaths_df.time) &amp; (deaths_df.time&lt;10*60), (10*60&lt;deaths_df.time) &amp; (deaths_df.time&lt;25*60), (25*60&lt;deaths_df.time)] choices = [&#39;early&#39;, &#39;mid&#39;, &#39;late&#39;] deaths_df[&#39;phase&#39;] = np.select(conditions, choices, default=&#39;very_early&#39;) deaths_df[&#39;phase&#39;] = deaths_df[&#39;phase&#39;].astype(&#39;category&#39;) return(deaths_df) . deaths_df = create_phase_category(deaths_df) . Datashader . Now, this is where the fun begins. . Datashader is a highly efficient Python library for visualising massive amount of data. . Taking Pandas dataframes as inputs, Datashader aggregates the data to form visualisations. . There are 3 key components that we use to generate our visualisation: .  Defining a canvas. It&#39;s going to be 4,000 by 4,000 pixels. The data range we want to visualise is 800,000 by 800,000. | cvs = ds.Canvas(plot_width=4_000, plot_height=4_000, x_range=[0,800_000],y_range=[0,800_000]) . We want to aggregate data from deaths_df, using the &#39;victim_position_x&#39; variable as the x coordinate and &#39;victim_position_y&#39; as the y coordinate. Effectively, we are computing a seperate 2D histogram for each category (game phase). . agg = cvs.points(deaths_df, &#39;victim_position_x&#39;, &#39;victim_position_y&#39;,ds.count_cat(&#39;phase&#39;)) . | We visualise our 2D histogram, coloring each bin/pixel according to our color map. We also use histogram equalization (how=&#39;eq_hist&#39;). . img = tf.shade(agg, color_key=color_key, how=&#39;eq_hist&#39;) . | This post is heavily inspired by this example, which is a lot more detailed about the pipeline involved. . def visualise_with_datashader(deaths_df): color_key = {&#39;very_early&#39;:&#39;black&#39;, &#39;early&#39;:&#39;lime&#39;, &#39;mid&#39;:&#39;aqua&#39;, &#39;late&#39;:&#39;fuchsia&#39;} cvs = ds.Canvas(plot_width=4_000, plot_height=4_000, x_range=[0,800_000],y_range=[0,800_000]) agg = cvs.points(deaths_df,&#39;victim_position_x&#39;,&#39;victim_position_y&#39;,ds.count_cat(&#39;phase&#39;)) img = tf.shade(agg, color_key=color_key, how=&#39;eq_hist&#39;) img = tf.set_background(img,&quot;black&quot;, name=&quot;Black bg&quot;) return(img) . One minor details, is that we need to invert the y coordinates we want to render, to match the coordinate system used for the game maps. . deaths_df.victim_position_y = 800_000 - deaths_df.victim_position_y . Erangel . Early (0-10m) (Lime Green points) | Mid Phase (10-25m) (Cyan points) | Late Phase (&gt;25m) (Purple points) | erangel_df = deaths_df[deaths_df.map==&#39;ERANGEL&#39;] num_points = erangel_df.shape[0] print(f&#39;Total points : {num_points}&#39;) img = visualise_with_datashader(erangel_df) ds.utils.export_image(img=img,filename=&#39;Erangel&#39;, fmt=&quot;.png&quot;); . Total points : 52964245 . . Miramar . Early (0-10m) (Lime Green points) | Mid Phase (10-25m) (Cyan points) | Late Phase (&gt;25m) (Purple points) | miramar_df = deaths_df[deaths_df.map==&#39;MIRAMAR&#39;] num_points = miramar_df.shape[0] print(f&#39;Total points : {num_points}&#39;) img = visualise_with_datashader(miramar_df) ds.utils.export_image(img=img,filename=&#39;Miramar&#39;, fmt=&quot;.png&quot;); . Total points : 11622838 . . Analysis . Let&#39;s finish by taking a closer look at the lower part of the Erangel map. . We can see 3 different phases of the game, the early phase in green, the mid phase in cyan, and the later phase in purple. . I will confess to have had playing a grand total of 2 games of PUBG, before deciding that playing virtual hide and seek wasn&#39;t that fun. Hence I&#39;m far from an expert, but we can see some fairly clear patterns. . In the early phases of the game, deaths are in and around buildings, as players search for supplies and weapons. . In the middle phase, the deaths appear to be more spread over the map, with concentrations on roads and natural chokepoints like bridges. . In the last phase of the game, the decreasing size of the &quot;safe zone&quot; forces the players into a concentrated area for a final stand. The results in the constellation of purple dots spread across the map. . Erangel subsection 1 . Erangel subsection 2 . Miramar subsection .",
            "url": "https://cgcooke.github.io/Blog/datashader/visualisation/pubg/2020/05/31/Visualising-PUBG-Deaths-With-Datashader.html",
            "relUrl": "/datashader/visualisation/pubg/2020/05/31/Visualising-PUBG-Deaths-With-Datashader.html",
            "date": " • May 31, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "How long is the Blacklist? (With PyMC3)",
            "content": "An Introduction to PyMC3 . Last post, I took a first principles approach to computationally solving a Bayesian analysis problem. In practice, problems can be far more complex. Fortunately, sophisticated software libraries already exist that can be leveraged in order to solve equally complex problems. While there are a number of options available, my current favourite is PyMC3. . If you want to be inspired by what&#39;s possible, I strongly suggest checking out the fantastic set of examples on the PyMC3 site.   . Putting it into practice . Let&#39;s start as we did last time, by taking a sample of the first 10 elements from the blacklist. . import numpy as np import matplotlib.pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [15, 15] # E: the data y = np.array([52, 145, 84, 161, 85, 152, 47, 109, 16, 16, 106, 101, 64, 73, 57, 83, 88, 135, 119,120,121, 122, 42, 8, 8, 104, 112, 89, 82, 74, 114, 22, 12, 21, 21, 67, 71, 93, 94, 75, 7, 97, 117, 62, 87, 55, 11, 38, 80, 72, 43, 50, 86, 31, 108, 24, 24, 95, 132, 103, 77, 113, 78, 32, 32, 41, 18, 14, 14, 79, 66, 65, 81, 105, 53, 98, 98, 111, 163, 102, 34, 107, 59, 10, 61, 29, 46, 4, 4, 30, 37, 76, 44, 54, 90, 48, 13, 118, 100, 56, 63, 51, 68, 19, 25, 23, 13, 110, 26, 17, 33, 20, 124, 146, 147, 131, 91, 116, 58, 99, 160, 20, 20, 9, 6, 115, 69, 136, 92, 128, 60, 15, 27, 27, 151, 138, 130, 125, 162, 159, 3, 137, 155, 144, 126, 158, 149, 150]) . y_sample = y[0:10] plt.stem(np.sort(y_sample), use_line_collection=True) plt.show() . Now this is when things start getting intesting. Firstly going to lay out all the code we need to solve this problem, all 7 lines. . import pymc3 as pm model = pm.Model() with model: # prior - P(N): N ~ uniform(max(y), 500) N = pm.DiscreteUniform(&quot;N&quot;, lower=y.max(), upper=500) # likelihood - P(D|N): y ~ uniform(0, N) y_obs = pm.DiscreteUniform(&quot;y_obs&quot;, lower=0, upper=N, observed=y) trace = pm.sample(10_000, start={&quot;N&quot;: y.max()}) pm.plots.plot_posterior(trace) . Multiprocess sampling (2 chains in 2 jobs) Metropolis: [N] Sampling 2 chains, 0 divergences: 100%|██████████| 21000/21000 [00:02&lt;00:00, 9637.66draws/s] The number of effective samples is smaller than 10% for some parameters. . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1121d6f10&gt;], dtype=object) . Ok, so from this we are 94% confident that the answer (the length of the Blacklist) is in the range 163 to 166. . Line by Line. . Let&#39;s step through now line by line, to understand what&#39;s going on. . Import PyMC3. . import pymc3 as pm . | Create a PyMC3 model. . model = pm.Model() . | Create a context manager, so that magic can happen behind the scenes . with model: . | Create a distribution for $N$, our prior belief of the length of the Blacklist ($P(H)$). In layman&#39;s terms, we are saying that $N$ is equally liklely to any integer between the highest index we observe in our sample of data, and 500. The number 500 is arbitrary, ultimately the data we observe will &quot;wash out&quot; any assumptions we have made. . | N = pm.DiscreteUniform(&quot;N&quot;, lower=y.max(), upper=500) . Create our likelyhood function, $P(E|H)$. IE, given the data we have observed, how likely is any value of N? Again, we are using a discrete uniform distribution to model the probability of seeing any observed data point. | y_obs = pm.DiscreteUniform(&quot;y_obs&quot;, lower=0, upper=N, observed=y) . Sample the model. We are computationally finding $P(H|E)$. This is where the real magic occurs. In this case, we generate 10,000 samples, and store them in &#39;trace&#39;. | trace = pm.sample(10_000, start={&quot;N&quot;: y.max()}) . Plot the distribution of the parameters. This is our posterior, ($P(H|E)$).pm.plots.plot_posterior(trace) . | Conclusion . In conclusion, PyMC3 can be used to quickly and efficiently conduct Bayesian analysis. I hope to do some more examples in future posts, looking at other &#39;real world&#39; problems. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/pymc3/2020/05/20/How-Long-Is-The-Blacklist-With-PyMC3.html",
            "relUrl": "/bayesian/pymc3/2020/05/20/How-Long-Is-The-Blacklist-With-PyMC3.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "How long is the Blacklist?",
            "content": "Overview . I&#39;m in the process of learning French, and as part of my language learning journey, I&#39;m slowly working my way through The Blacklist on Netflix. . The premise of the show involves an antihero played by James Spader working his way through a list of nefarious characters, taking them down one at a time. . Out of idle intellectual curiosity, I was curious about how many names are on the list, based on the sample of names that are given. It&#39;s actually an interesting problem we can solve using Bayesian analysis, more commonly known as the &quot;German Tank Problem&quot;, or the &quot;Locomotive Problem&quot;. . It&#39;s also an interesting into into Bayesian analysis. We are trying to infer an unknown quantity (The number of people on the list), while only being able to observe samples from this distribution. . Bayes in Brief . There are many other, far better blog posts which cover this topic, in particular this one. . Now, with that out of the way, let&#39;s talk about the problem at hand, and how we can solve it with Bayesian analysis. . We have some evidence, $E$, which is what we actually obsever. In this case, it&#39;s the index in the black list that we find out each episode. IE, in first episode, it&#39;s entry number $52$. . Then we have what we want to know, an estimate of the length of the Blacklist, given the evidence we have observed. This is the probability of a hypothesis, given the evidence $P(H|E)$. It&#39;s a probability, because we don&#39;t know for sure how long the blacklist is, given what we have observed, but we can work out how confident we can be about each potential option. . Now, given that $P(H|E)$ is what we want, we can use Baye&#39;s Formula to find it. . $P(H|E) = frac{P(E|H) P(H)}{P(E)}$ . We need three more pieces of the puzzle, $P(E|H)$, $P(H)$ and $P(E)$. . Now, $P(E|H)$ is the probability of the Evidence, given the Hypothesis. This term is also known as the Likelyhood, and is the term which I found most confusing while initially learning about Bayesian analysis. In this case, it&#39;s almost confusingly simple. . $P(H)$ is our prior belief about what the possible hypothesis could be. In this case, it&#39;s long long we plausibly think the Blacklist could be. In practice, we know it has to be as long as any of the entries we have observed. Much of the criticism of Baysian analysis comes from the fact we can inject our own beliefs into the process. In this case, I don&#39;t have a strong belief about the length of the Blacklist, but it&#39;s reasonable to assume that it&#39;s probably less than 200 entries long. . Let&#39;s assume that the Blacklist is of length $N$. Let&#39;s also assume that any given entry is as likely to be the subject of each episode as any other. Then the probability of drawing any given number, is $ frac{1}{N}$. This is the likelihood. Also note the subtle point that obviously that N has to be at least as large as the number we have drawn. . Finally, we have $P(E)$, which is the probability of the evidence. This is just a normalisation factor, which is often ignored. . Let&#39;s get down to business, fortunately Wikipedia has all the data we need. . No. overall No. in season Title Blacklist guide Directed by Written by Original air date US viewers(millions) . 1 | 1 | &quot;Pilot&quot; | No. 52 | Joe Carnahan | Jon Bokenkamp | September 23, 2013 | 12.58[10] | . 2 | 2 | &quot;The Freelancer&quot; | No. 145 | Jace Alexander | Jon Bokenkamp | September 30, 2013 | 11.35[11] | . 3 | 3 | &quot;Wujing&quot; | No. 84 | Michael Watkins | Lukas Reiter | October 7, 2013 | 11.18[12] | . 4 | 4 | &quot;The Stewmaker&quot; | No. 161 | Vince Misiano | Patrick Massett &amp; John Zinman | October 14, 2013 | 10.93[13] | . 5 | 5 | &quot;The Courier&quot; | No. 85 | Nick Gomez | John C. Kelley | October 21, 2013 | 10.44[14] | . 6 | 6 | &quot;Gina Zanetakos&quot; | No. 152 | Adam Arkin | Wendy West | October 28, 2013 | 10.51[15] | . 7 | 7 | &quot;Frederick Barnes&quot; | No. 47 | Michael Watkins | J. R. Orci | November 4, 2013 | 10.34[16] | . 8 | 8 | &quot;General Ludd&quot; | No. 109 | Stephen Surjik | Amanda Kate Shuman | November 11, 2013 | 10.69[17] | . 9 | 9 | &quot;Anslo Garrick&quot; | No. 16 | Joe Carnahan | Story by : Joe Carnahan &amp; Jason George Teleplay by : Joe Carnahan | November 25, 2013 | 10.96[18] | . 10 | 10 | &quot;Anslo Garrick Conclusion&quot; | No. 16 | Michael Watkins | Lukas Reiter &amp; J. R. Orci | December 2, 2013 | 11.67[19] | . 11 | 11 | &quot;The Good Samaritan&quot; | No. 106 | Dan Lerner | Brandon Margolis &amp; Brandon Sonnier | January 13, 2014 | 9.35[20] | . 12 | 12 | &quot;The Alchemist&quot; | No. 101 | Vince Misiano | Anthony Sparks | January 20, 2014 | 8.83[21] | . 13 | 13 | &quot;The Cyprus Agency&quot; | No. 64 | Michael Watkins | Lukas Reiter | January 27, 2014 | 10.17[22] | . 14 | 14 | &quot;Madeline Pratt&quot; | No. 73 | Michael Zinberg | Jim Campolongo | February 24, 2014 | 11.18[23] | . 15 | 15 | &quot;The Judge&quot; | No. 57 | Peter Werner | Jonathan Shapiro &amp; Lukas Reiter | March 3, 2014 | 11.01[24] | . 16 | 16 | &quot;Mako Tanida&quot; | No. 83 | Michael Watkins | Story by : Joe Carnahan Teleplay by : John Eisendrath &amp; Jon Bokenkamp &amp; Patrick Massett &amp; John Zinman | March 17, 2014 | 10.97[25] | . 17 | 17 | &quot;Ivan&quot; | No. 88 | Randy Zisk | J.R. Orci &amp; Amanda Kate Shuman | March 24, 2014 | 10.80[26] | . 18 | 18 | &quot;Milton Bobbit&quot; | No. 135 | Steven A. Adelson | Daniel Voll | March 31, 2014 | 11.39[27] | . 19 | 19 | &quot;The Pavlovich Brothers&quot; | Nos. 119-122 | Paul Edwards | Elizabeth Benjamin | April 21, 2014 | 11.24[28] | . 20 | 20 | &quot;The Kingmaker&quot; | No. 42 | Karen Gaviola | J. R. Orci &amp; Lukas Reiter | April 28, 2014 | 10.85[29] | . 21 | 21 | &quot;Berlin&quot; | No. 8 | Michael Zinberg | John Eisendrath &amp; Jon Bokenkamp | May 5, 2014 | 10.47[30] | . 22 | 22 | &quot;Berlin Conclusion&quot; | No. 8 | Michael Watkins | Story by : Richard D&#39;Ovidio Teleplay by : John Eisendrath &amp; Jon Bokenkamp &amp; Lukas Reiter &amp; J. R. Orci | May 12, 2014 | 10.44[31] | . Putting it into practice . Now, let&#39;s look at all the episodes, from the seasons that have been aired: . import numpy as np import matplotlib.pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [15, 15] # E: the data y = np.array([52, 145, 84, 161, 85, 152, 47, 109, 16, 16, 106, 101, 64, 73, 57, 83, 88, 135, 119,120,121, 122, 42, 8, 8, 104, 112, 89, 82, 74, 114, 22, 12, 21, 21, 67, 71, 93, 94, 75, 7, 97, 117, 62, 87, 55, 11, 38, 80, 72, 43, 50, 86, 31, 108, 24, 24, 95, 132, 103, 77, 113, 78, 32, 32, 41, 18, 14, 14, 79, 66, 65, 81, 105, 53, 98, 98, 111, 163, 102, 34, 107, 59, 10, 61, 29, 46, 4, 4, 30, 37, 76, 44, 54, 90, 48, 13, 118, 100, 56, 63, 51, 68, 19, 25, 23, 13, 110, 26, 17, 33, 20, 124, 146, 147, 131, 91, 116, 58, 99, 160, 20, 20, 9, 6, 115, 69, 136, 92, 128, 60, 15, 27, 27, 151, 138, 130, 125, 162, 159, 3, 137, 155, 144, 126, 158, 149, 150]) . Now, potting the episodes in order. If we had to guess now, we would likely guess that the length of the Blacklist, is 163 episodes, or a few more. This is because we have seen entries on the list up to 163, and It looks like nearly every entry on the list has been crossed off. . plt.stem(np.sort(y), use_line_collection=True) plt.show() . Now let&#39;s say that we have a sample of only the first 10 episodes, what can we infer from this sample? . y_sample = y[0:10] plt.stem(np.sort(y_sample), use_line_collection=True) plt.show() . Let&#39;s start by defining our likelyhood function, $P(E|H)$. . def compute_likelyhood(i,observed): if i &lt; observed: likelyhood = 0 else: likelyhood = 1.0/i return(likelyhood) . Now let&#39;s define the prior belif $P(H)$ of what we think the length of the Blacklist could be. . prior = np.ones(200) prior /= prior.sum() . This prior is quite broad, and it&#39;s also uniform. We aren&#39;t really imputing any of our human judgement about how long the list is. . Now, let&#39;s by start by computing the likelyhood for each possilble list length, after we have watched the first episode, and seen person #52 crossed off the list. . posterior = np.zeros(200) observed = 52 for i in range(0,200): posterior[i] = prior[i] * compute_likelyhood(i,observed) posterior /= posterior.sum() plt.stem(posterior) plt.xlabel(&#39;List length&#39;) plt.ylabel(&#39;Confidence&#39;) #plt.xlim(150,175) plt.show() . /Users/cooke_c/.local/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the &#34;use_line_collection&#34; keyword argument to True. import sys . Now we have have have this belief state, having seen the first episode, we can recursively use it as a prior, and update it each time we see an episode: . import matplotlib.pylab as pl posterior = np.zeros(200) episode_number = 1 for observed in y_sample: for i in range(1,200): posterior[i] = prior[i] * compute_likelyhood(i,observed) posterior /= posterior.sum() plt.plot(posterior,alpha=0.35,label=&#39;Episode: &#39;+str(episode_number ), color=pl.cm.jet(episode_number/10.0)) episode_number+=1 prior=posterior plt.legend() plt.xlabel(&#39;List length&#39;) plt.ylabel(&#39;Confidence&#39;) plt.xlim(155,175) plt.show() . From the above, we can see that as we see more data, our confidence in the length of the list increases. Let&#39;s look at the cumulative probability for different list lengths: . plt.plot(np.cumsum(posterior)) plt.grid() plt.xlim(155,185) plt.xlabel(&#39;List length&#39;) plt.ylabel(&#39;Cumulative Confidence&#39;) plt.show() . From this, we can see that we can be approximately 50% confident that the length of Blacklist is between 160 and 170 entries long, after we have seen the first 10 episodes. . Conclusion . We have taken a lighting tour of how Bayesian analysis can allow us to infer from what we can observe, a quantity we are interested in, when there is a random process involved. . In practice, Bayesian analysis isn&#39;t conducted from first principles, but instead uses a purpose built library. Next time, I want to go and demonstrate how to do this analysis using PyMC3. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/2020/04/29/How-Long-Is-The-Blacklist.html",
            "relUrl": "/bayesian/2020/04/29/How-Long-Is-The-Blacklist.html",
            "date": " • Apr 29, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "R From Vanishing points",
            "content": "Ok, so we now have 3 different vanishing points. Using these, let&#39;s try and use them to gain some insights into the camera&#39;s relationship with the scene. . In this post, I&#39;m going to focus on how to determine the camera&#39;s rotation with respect to the scene, using 3 orthogonal vanishing points present in the scene. . Note: Hartley &amp; Zisserman put it best, &quot;Vanishing points are images of points at infinity, and provide orientation (attitude) in- formation in a similar manner to that provided by the fixed stars.&quot; This is a great video to watch for another perspective on this problem. . Ok, now let&#39;s import some useful libraries, and visualise the scene. . from PIL import Image import matplotlib.pyplot as plt import numpy as np np.set_printoptions(precision=3) plt.rcParams[&#39;figure.figsize&#39;] = [15, 15] . img = Image.open(&#39;data/2020-04-10-Finding-Vanishing-Points/csgo-dust2-mid.jpeg&#39;) plt.imshow(img) plt.show() . Now is a good time to pause for a second, and talk about homogeneous coordinates. . Let&#39;s assume we are comfortable with the Euclidean coordinate system (X,Y,Z). Let&#39;s pretend that we have a point, $p$ at: . $X = 5$ | $Y = 7$ | $Z = 4$ | . I.e. . $p = begin{bmatrix} 5 7 4 end{bmatrix}$ . Let&#39;s just go out on a limb. Let&#39;s take $p$, and multiply each component by some amount ($k$). In addition, let&#39;s append $k$ as an extra dimension. . $p = begin{bmatrix} 5k 7k 4k k end{bmatrix}$ . This is the homogeneous representation of $p$. . Let&#39;s now take a point on the $Z$ axis, perhaps $Z=1$ for example: . $p = begin{bmatrix} 0 0 1 end{bmatrix}$ . or . $p = begin{bmatrix} 0k 0k 1k k end{bmatrix}$ . or . If k is 0, then if $Z$ is any other positive value (apart from 0), then it will be a point infinitely far away on the Z axis. This is also the Z vanishing point. . $p = begin{bmatrix} 0 0 1 0 end{bmatrix}$ . From my other post, we can see that we can map from the points in 3D space ($X$) to points in 2D space ($x$) using the matrix $P$. . begin{equation*} x = PX end{equation*}The matrix $P$ in turn, consists of 3 parts. . A intrinsic camera matrix $K$ | A Rotation matrix $R$ | A translation matrix $t$ | begin{equation*} P = K[R | t] end{equation*}Assuming that the camera is free from radial distortion, then the Z vanishing point can be found as follows. . $v_z = K begin{bmatrix} R_1 &amp; R_2 &amp; R_3 &amp; | &amp; t end{bmatrix} begin{bmatrix} 0 0 1 0 end{bmatrix} $ . Looking carefully, we realise that the we can knock out everything but the column $R_3$ of the rotation matrix $R$. . In the previous post, we already found the matrix $K$. . $K = begin{bmatrix} 728 &amp; 0 &amp; 1327 0 &amp; 728 &amp; 706 0 &amp; 0 &amp; 1 end{bmatrix}$ . Now, we can find $R_3$ as follows: . $R_3 = frac{K^{-1} v_z}{|K^{-1} v_z|} $ . Let&#39;s put this into practice. . Now, let&#39;s make some assumptions. . World X axis pointing right (Red arrow) | World Y axis pointing into the scene (Green arrow) | World Z axis pointing up (Blue arrow) | . . . vanishing_points = {&#39;VP1&#39;: [1371.892, 630.421], &#39;VP2&#39;: [-10651.54, 536.681], &#39;VP3&#39;: [1272.225, 7683.02 ]} K = np.array([[7.276e+02,0.000e+00,1.327e+03], [6.236e-14,7.276e+02,7.060e+02], [1.218e-16,0.000e+00,1.000e+00]]) def make_homogeneous(x): x_homogeneous = np.array([x[0],x[1],1]) return(x_homogeneous) v_x_h = make_homogeneous(vanishing_points[&#39;VP2&#39;]) v_y_h = make_homogeneous(vanishing_points[&#39;VP1&#39;]) v_z_h = make_homogeneous(vanishing_points[&#39;VP3&#39;]) K_inv = np.linalg.inv(K) R_2 = np.dot(K_inv,v_y_h)/np.linalg.norm(np.dot(K_inv,v_y_h)).T R_3 = -1 * np.dot(K_inv,v_z_h)/np.linalg.norm(np.dot(K_inv,v_z_h)).T R_1 = np.cross(R_2,R_3) R = np.vstack([R_1,R_2,R_3]) print(R) . [[ 0.998 0.014 -0.06 ] [ 0.061 -0.103 0.993] [ 0.008 -0.995 -0.104]] . This correlates well with what we were expecting. The rotation matrix takes us from the world coordinate system to the camera&#39;s coordinate system. In particular, we can see that the matrix $R$ maps (approximately): . World X axis to Camera X axis | World Y axis to Camera Negative Z axis | World Z axis to Camera Y axis | from mpl_toolkits.mplot3d import Axes3D fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.plot(xs=[0,1], ys=[0,0], zs = [0,0],color=&#39;r&#39;,label=&#39;X World&#39;) ax.plot(xs=[0,0], ys=[0,1], zs = [0,0],color=&#39;g&#39;,label=&#39;Y World&#39;) ax.plot(xs=[0,0], ys=[0,0], zs = [0,1],color=&#39;b&#39;,label=&#39;Z World&#39;) plt.legend() plt.show() fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.plot(xs=[0,R[0,0]], ys=[0,R[1,0]], zs = [0,R[2,0]],color=&#39;r&#39;,label=&#39;X Camera&#39;) ax.plot(xs=[0,R[0,1]], ys=[0,R[1,1]], zs = [0,R[2,1]],color=&#39;g&#39;,label=&#39;Y Camera&#39;) ax.plot(xs=[0,R[0,2]], ys=[0,R[1,2]], zs = [0,R[2,2]],color=&#39;b&#39;,label=&#39;Z Camera&#39;) plt.legend() plt.show() .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/linear%20algebra/2020/04/12/R-From-Vanishing-Points.html",
            "relUrl": "/computer%20vision/linear%20algebra/2020/04/12/R-From-Vanishing-Points.html",
            "date": " • Apr 12, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "K From Vanishing points",
            "content": "from PIL import Image import matplotlib.pyplot as plt import numpy as np import scipy.linalg np.set_printoptions(precision=3) plt.rcParams[&#39;figure.figsize&#39;] = [15, 15] . img = Image.open(&#39;data/2020-04-10-Finding-Vanishing-Points/csgo-dust2-mid.jpeg&#39;) plt.imshow(img) plt.show() . vanishing_points = {&#39;VP1&#39;: [1371.892, 630.421], &#39;VP2&#39;: [-10651.54 , 536.681], &#39;VP3&#39;: [1272.225, 7683.02 ]} . plt.imshow(img) for point_name in [&#39;VP1&#39;,&#39;VP2&#39;,&#39;VP3&#39;]: vp = vanishing_points[point_name] plt.scatter(vp[0],vp[1],label=point_name) plt.legend() plt.show() . Ok, so we now have 3 different vanishing points. Using these, let&#39;s try and use them to gain some insights the cameras&#39;s relationship with the scene. . Hartley &amp; Zisserman put it best, &quot;Vanishing points are images of points at infinity, and provide orientation (attitude) in- formation in a similar manner to that provided by the fixed stars.&quot; . Hartley &amp; Zisserman also provide an algorithm (Example 8.27, page 226) to extract the camera calibration matrix K from 3 mutually orthogonal vanishing points. . Let&#39;s go and implement it in practice. . Some algebra . H&amp;Z propose a matrix $ omega$ (omega) which captures the following relationship between the different vanishing points. . $v^T_i omega v_j = 0$ . Where: . $ omega = begin{bmatrix} w_1 &amp; 0 &amp; w_2 0 &amp; w_1 &amp; w_3 w_2 &amp; w_3 &amp; w_4 end{bmatrix}$ . And: . $ v_j = begin{bmatrix} x_1 y_1 1 end{bmatrix}$ . $v_i = begin{bmatrix} x_2 y_2 1 end{bmatrix}$ . If we can find this matrix $ omega$, then we can find the camera calibration matrix, if we make some assumptions: . Zero Skew | Square Pixels | From H&amp;Z, we have: &quot;K is obtained from $ omega$ by Cholesky factorization of omega, followed by inversion.&quot; . For good practice, we can also normalize the matrix, so that lower right value $K_{22}$ is 1. In python: . K = np.linalg.inv(np.linalg.cholesky(omega)).T K/=K[2,2] . Working backwards, we are faced with the task of finding $ omega$ . Multiplying through, we find that: . $ v^T_i omega v_j = x_2(w_1 x_1 + w_2) + y_2(w_1 y_1 + w_3) + w_2 x_1 + w_3 y_1 + w_4$ . Factorising: . $ v^T_i omega v_j= w_1(x_2 x_1 + y_2 y_1) + w_2(x_2 + x_1) + w_3(y_2 + y_1) + w_4$ . Great, we can now find all the coefficients we need for our matrix, from each pair of vanishing points. . We have 3 pairs of vanishing points: . 1 &amp; 2 | 2 &amp; 3 | 3 &amp; 1 | From each pair we can find a new set of values for $w_1$ to $w_4$. . Stacking them all on top each other, we end up with the matrix $A$. . $A = begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13} &amp; w_{14} w_{21} &amp; w_{22} &amp; w_{23} &amp; w_{24} w_{31} &amp; w_{32} &amp; w_{33} &amp; w_{34} end{bmatrix}$ . In Python: . def generate_A(vanishing_points): A = [] for (point_name_1, point_name_2) in [(&#39;VP1&#39;,&#39;VP2&#39;),(&#39;VP2&#39;,&#39;VP3&#39;),(&#39;VP3&#39;,&#39;VP1&#39;)]: vp1 = vanishing_points[point_name_1] vp2 = vanishing_points[point_name_2] x1,y1 = vp1 x2,y2 = vp2 w1 = x2*x1 + y2*y1 w2 = x2 + x1 w3 = y2 + y1 w4 = 1 A.append([w1,w2,w3,w4]) A = np.array(A) return(A) . def generate_A(vanishing_points): A = [] for (point_name_1, point_name_2) in [(&#39;VP1&#39;,&#39;VP2&#39;),(&#39;VP2&#39;,&#39;VP3&#39;),(&#39;VP3&#39;,&#39;VP1&#39;)]: vp1 = vanishing_points[point_name_1] vp2 = vanishing_points[point_name_2] x1,y1 = vp1 x2,y2 = vp2 w1 = x2*x1 + y2*y1 w2 = x2 + x1 w3 = y2 + y1 w4 = 1 A.append([w1,w2,w3,w4]) A = np.array(A) return(A) def compute_K(A): w = scipy.linalg.null_space(A).ravel() w1 = w[0] w2 = w[1] w3 = w[2] w4 = w[3] omega = np.array([[w1,0,w2], [0,w1,w3], [w2,w3,w4]]) K = np.linalg.inv(np.linalg.cholesky(omega)).T K/=K[2,2] return(K) A = generate_A(vanishing_points) K = compute_K(A) . print(K) . [[7.276e+02 0.000e+00 1.327e+03] [6.236e-14 7.276e+02 7.060e+02] [1.218e-16 0.000e+00 1.000e+00]] . . So now we have the calibration matrix $K$, which gives us 3 seprate pieces of information. . The Focal length in pixels: $K_{11}$ or $K_{22}$ (728) | The x coordinate of the camera optical center: $K_{13}$ (1327) | The y coordinate of the camera optical center $K_{23}$ (706) | $K = begin{bmatrix} 728 &amp; 0 &amp; 1327 0 &amp; 728 &amp; 706 0 &amp; 0 &amp; 1 end{bmatrix}$ . plt.scatter(2560/2.0,1600/2.0,color=&#39;G&#39;) plt.scatter(K[0,2],K[1,2],color=&#39;R&#39;) plt.imshow(img) plt.show() .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/linear%20algebra/2020/04/11/K-From-Vanishing-Points.html",
            "relUrl": "/computer%20vision/linear%20algebra/2020/04/11/K-From-Vanishing-Points.html",
            "date": " • Apr 11, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Finding Vanishing Points",
            "content": "Why are we interested in finding vanishing points? Because they allow us to quickly and easily estimate key parameters about a camera. For example, it&#39;s focal length, optical center and rotation in 3D space. . However, the first step is to be able to identify the location of the vanishing points in an image. . Important: The projection of parallel lines in 3D space, intersect at a vanishing point in 2D space. Sometimes it can be easy to find the location of vanishing point in an image, for example when two objects in the real world are very long and quite close together, for example train tracks. Often it&#39;s a lot more challenging. . Let&#39;s use a semi-realistic image, I&#39;ve chosen a rendered image from a video game (Counter Strike). This means that we can, for the moment ignore some other factors like Radial Distortion. . Let&#39;s start, as always by importing what we will need later. . from PIL import Image import matplotlib.pyplot as plt import json import numpy as np plt.rcParams[&#39;figure.figsize&#39;] = [15, 15] . Now let&#39;s visualise the secene. . img = Image.open(&#39;data/2020-04-10-Finding-Vanishing-Points/csgo-dust2-mid.jpeg&#39;) plt.imshow(img) plt.show() . Maximum Likelihood Estimate . Straight lines in 3D space are mapped to straight lines in 2D space by an ideal camera. We can automate the detection of straight lines using algorithms like a hough transform. However I&#39;ve used an external program to manual annotate the straight lines in the image. I&#39;ve annotated 3 sets of lines that correspond to one of 3 different vanishing points. . Often, we use a manhattan world assumption, that is assuming that there are 3 different sets of orthogonal parallel lines present in the world. We assume that there are 2 orthogonal lines that form the ground plane. In an image, each set of parallel lines that lies on this plane online, which in practice is the horizon. . . Now, let&#39;s use linear algebra and least mean squares (and the magic of Stack Overflow) to find one of the vanishing points.  . JSON = json.loads(open(&#39;data/2020-04-10-Finding-Vanishing-Points/csgo-dust2-mid.json&#39;,&#39;r&#39;).read()) . def intersect_multiple_lines(P0,P1): &quot;&quot;&quot;P0 and P1 are NxD arrays defining N lines. D is the dimension of the space. This function returns the least squares intersection of the N lines from the system given by eq. 13 in http://cal.cs.illinois.edu/~johannes/research/LS_line_intersect.pdf. &quot;&quot;&quot; # generate all line direction vectors n = (P1-P0)/np.linalg.norm(P1-P0,axis=1)[:,np.newaxis] # normalized # generate the array of all projectors projs = np.eye(n.shape[1]) - n[:,:,np.newaxis]*n[:,np.newaxis] # I - n*n.T # generate R matrix and q vector R = projs.sum(axis=0) q = (projs @ P0[:,:,np.newaxis]).sum(axis=0) # solve the least squares problem for the # intersection point p: Rp = q p = np.linalg.lstsq(R,q,rcond=None)[0] return(p) . def load_line_data(point_name): P0 = [] P1 = [] for shape in JSON[&#39;shapes&#39;]: points = shape[&#39;points&#39;] if shape[&#39;label&#39;] == point_name: P0.append(points[0]) P1.append(points[1]) P0 = np.array(P0,dtype=np.float64) P1 = np.array(P1,dtype=np.float64) return(P0,P1) def find_vanishing_point(point_name): P0,P1 = load_line_data(point_name) p = intersect_multiple_lines(P0,P1).ravel() return(p) p = find_vanishing_point(point_name=&#39;VP1&#39;) . Now let&#39;s visualise the location of vanishing point 1. . plt.imshow(img) plt.scatter(p[0],p[1],color=&#39;r&#39;,label=&#39;Vanishing point #1&#39;) plt.legend() plt.xlim(0,2560) plt.ylim(1600,0) plt.show() . Monte Carlo Simulation . Ok, so this is a good start, but I&#39;m interested in how certain we are about the vanishing point. . For example, when I annotated the lines, I most likely made mistakes in the precise location of each point. We would expect that for shorter lines, this would have a bigger impact on the location of the vanishing point than for longer lines. . Let&#39;s do a monte carlo simulation, to find the distribution of possible vanishing points. . def monte_carlo_simulation(point_name): P0,P1 = load_line_data(point_name) point_error_magnitude = 1 vanishing_points = [] for i in range(0,1000): P0_stochastic = P0 + point_error_magnitude*np.random.randn(P0.shape[0],P0.shape[1]) P1_stochastic = P1 + point_error_magnitude*np.random.randn(P1.shape[0],P1.shape[1]) p = intersect_multiple_lines(P0_stochastic,P1_stochastic) vanishing_points.append(p) vanishing_points = np.asarray(vanishing_points) return(vanishing_points) point_name = &#39;VP1&#39; vanishing_points = monte_carlo_simulation(point_name) . Now let&#39;s visualise the distribution of points: . for p in vanishing_points: plt.scatter(p[0],p[1],color=&#39;k&#39;,alpha=0.1) plt.xlim(1350,1390) plt.ylim(640,620) plt.grid() plt.show() . For completeness, we can compute the standard deviation of the points in both the x &amp; y axis. . print(vanishing_points.std(axis=0).ravel()) . [3.5399997 2.03944019] . Voila, we have a standard deviation of ±3.5 pixels in the x direction, and ±2 pixels in the y direction. . Apendix . For the sake of completeness, let&#39;s compute the location of the other 3 vanishing points. . vanishing_points = {} for point_name in [&#39;VP1&#39;,&#39;VP2&#39;,&#39;VP3&#39;]: vanishing_points[point_name]= find_vanishing_point(point_name) plt.imshow(img) for point_name,color in [(&#39;VP1&#39;,&#39;g&#39;),(&#39;VP2&#39;,&#39;r&#39;),(&#39;VP3&#39;,&#39;b&#39;)]: vp = vanishing_points[point_name] print(point_name,vp) plt.scatter(vp[0],vp[1],color=color,label=point_name) plt.legend() plt.show() . VP1 [1371.89171088 630.42051773] VP2 [-10651.53961582 536.68080631] VP3 [1272.22463298 7683.01978252] . plt.imshow(img) for point_name,color in [(&#39;VP1&#39;,&#39;g&#39;),(&#39;VP2&#39;,&#39;r&#39;),(&#39;VP3&#39;,&#39;b&#39;)]: vanishing_points = monte_carlo_simulation(point_name) print(point_name, vanishing_points.std(axis=0).ravel()) for p in vanishing_points: plt.scatter(p[0],p[1],color=color,alpha=0.1) plt.show() . VP1 [3.29072559 2.01278422] VP2 [1706.16058495 32.11479688] VP3 [ 20.31361393 217.79742705] . We can see that there is a significant amount of uncertainty in the x component (±1728) of the 2nd vanishing point. This is because of the lack of good, long parallel lines running left/right across the image. .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/linear%20algebra/monte%20carlo%20simulation/2020/04/10/Finding-Vanishing-Points.html",
            "relUrl": "/computer%20vision/linear%20algebra/monte%20carlo%20simulation/2020/04/10/Finding-Vanishing-Points.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Inverse Radial Distortion",
            "content": "What is radial distortion? . Real cameras, with real optics suffer from radial distortion. Radial distortion is a common type of lens distortion). It&#39;s a deviation away from a perfect pinhole camera, where straight lines in 3D space are mapped to straight lines in 2D space. Let&#39;s define this distortion mathematically. . Defining a forward function . Radial distortion functions map points according to their distance (r) from the optical center of the lens. . We can model the function using a Taylor series: . $ begin{equation*} x_d = x(1 + k_1  r + k_2  r^2 + k_3 r^3) end{equation*}$ . $ begin{equation*} y_d = y(1 + k_1  r + k_2 r^2 + k_3 r^3) end{equation*}$ . IE, the function is described using 3 numbers, k_1, k_2 and k_3. . Firstly, let&#39;s import some things we need. . import numpy as np import matplotlib.pyplot as plt from scipy.optimize import least_squares %matplotlib inline plt.rcParams[&quot;figure.figsize&quot;] = (10,10) . Next let&#39;s use some distortion coefficients from a real lens, and plot how it maps points based on their radius from the optical center of the lens. . k_1 = -0.04436 k_2 = -0.35894 k_3 = 0.14944 r = np.linspace(0,1,1000) r_distorted = r*(1 + k_1 * r + k_2 * r**2 + k_3 * r**3) plt.xlabel(&#39;Initial R&#39;) plt.ylabel(&#39;Distorted R&#39;) plt.plot(r,r_distorted) plt.show() . To help understand this mapping, we can visualise the impact on a grid of straight lines. Note how straight lines are mapped to curves. . def distort_line(x,y,k_1,k_2,k_3): r = np.sqrt(x**2 + y**2) x_distorted = x*(1 + k_1 * r + k_2 * r**2 + k_3 * r**3) y_distorted = y*(1 + k_1 * r + k_2 * r**2 + k_3 * r**3) return(x_distorted,y_distorted) for y in np.linspace(-1,1,10): x = np.linspace(-1,1,1000) x_distorted,y_distorted = distort_line(x,y,k_1,k_2,k_3) plt.plot(x_distorted,y_distorted,color=&#39;k&#39;,alpha=0.8) for x in np.linspace(-1,1,10): y = np.linspace(-1,1,1000) x_distorted,y_distorted = distort_line(x,y,k_1,k_2,k_3) plt.plot(x_distorted,y_distorted,color=&#39;k&#39;,alpha=0.8) plt.xlim(-1,1) plt.ylim(-1,1) plt.show() . Finding an Inverse Function . Now it&#39;s time to find an inverse function, a function that will allow us to take points that have been distorted, and map them back to where they would have been, had the lens been free from distortion. . Unfortunately we can&#39;t algebraically find and an inverse function based on the forward function. However, we can find an inverse function through a process of optimization. . def undistort_point(undistortion_params,r_distorted): undistorted = r_distorted*(1 + undistortion_params[0] * r_distorted + undistortion_params[1] * r_distorted**2 + undistortion_params[2] * r_distorted**3 + undistortion_params[3] * r_distorted**4 + undistortion_params[4] * r_distorted**5) return(undistorted) def fun(undistortion_params,r_distorted): #Compute residuals. undistorted = undistort_point(undistortion_params, r_distorted) return((undistorted - np.linspace(0,1,1000))).ravel() . x0 = np.zeros(5).ravel() res = least_squares(fun, x0, verbose=2, ftol=1e-12,loss=&#39;linear&#39;, args=([r_distorted])) . Iteration Total nfev Cost Cost reduction Step norm Optimality 0 1 5.6524e+00 3.02e+01 1 2 2.3481e-07 5.65e+00 9.82e-01 1.88e-08 2 3 2.3481e-07 1.62e-17 1.58e-06 1.04e-11 `gtol` termination condition is satisfied. Function evaluations 3, initial cost 5.6524e+00, final cost 2.3481e-07, first-order optimality 1.04e-11. . Basically the optimisation process tries to find a set of coefficients that allow us to map the output from the distortion function back to it&#39;s input. . undistorted = undistort_point(res.x,r_distorted) plt.plot(r_distorted,label=&#39;distorted&#39;,alpha=0.5) plt.plot(undistorted,label=&#39;un distorted&#39;,alpha=0.5) plt.plot(np.linspace(0,1,1000),label=&#39;target&#39;,alpha=0.5) plt.legend() plt.show() . print(res.x) . [ 0.04599498 0.32120247 0.22196835 -0.46283148 0.77191211] . Voila, we have found a coefficients of a taylor series that allow us to invert the distortion function. .",
            "url": "https://cgcooke.github.io/Blog/optimisation/computer%20vision/2020/04/05/Inverse-Radial-Distortion.pynb.html",
            "relUrl": "/optimisation/computer%20vision/2020/04/05/Inverse-Radial-Distortion.pynb.html",
            "date": " • Apr 5, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "RQ Decomposition In Practice",
            "content": "Let&#39;s keep things short and sweet. . Given a camera projection matrix, $P$, we can decompose it into a $K$ (Camera Matrix), $R$ (Rotation Matrix) and $C$ (Camera Centroid) matrix. . IE, given we have $P = K[R|-RC]$, We want to find $K$, $R$ and $C$. . The method is simply described in Multiple View Geometry in Computer Vision (Second Edition), on page 163, however let&#39;s turn it into a practical Python implementation. . Let&#39;s follow along with an example from the book. . import numpy as np import scipy.linalg np.set_printoptions(precision=2) . P = np.array([[3.53553e+2, 3.39645e+2, 2.77744e+2, -1.44946e+6], [-1.03528e+2, 2.33212e+1, 4.59607e+2, -6.3252e+5], [7.07107e-1, -3.53553e-1, 6.12372e-1, -9.18559e+2]]) . So, we have: $P = [M | −MC]$ . M can be decomposed as $M=KR$ using the RQ decomposition. . M = P[0:3,0:3] K, R = linalg.rq(M) . So far, so good. . Now things get a little more complex. . We want to find a Camera matrix with a positive diagonal, giving positive focal lengths. . However, in case this doesn&#39;t happen, we can adjust the sign of the column of each column of both the $K$ and $R$ matrix, to &quot;Make it so&quot;. . T = np.diag(np.sign(np.diag(K))) if linalg.det(T) &lt; 0:     T[1,1] *= -1 K = np.dot(K,T) R = np.dot(T,R) .     Finally, we can find the Camera Center ($C$). . We have $P_4$, the 4th column of $P$. . $P_4 = −MC$ . From this, we can find $C = {-M}^{-1} P_4$ . def factorize(P): M = P[:,0:3] K,R = scipy.linalg.rq(M) T = np.diag(np.sign(np.diag(K))) if scipy.linalg.det(T) &lt; 0: T[1,1] *= -1 K = np.dot(K,T) R = np.dot(T,R) C = np.dot(scipy.linalg.inv(-M),P[:,3]) return(K,R,C) K,R,C = factorize(P) print(&#39;K&#39;) print(K) print(&#39;R&#39;) print(R) print(&#39;C&#39;) print(C) . K [[468.16 91.23 300. ] [ 0. 427.2 200. ] [ 0. 0. 1. ]] R [[ 0.41 0.91 0.05] [-0.57 0.22 0.79] [ 0.71 -0.35 0.61]] C [1000.01 2000. 1499.99] . Voila! . This presentation is a great read, and provides a good overview of the RQ and QR decompositions. .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/linear%20algebra/2020/03/13/RQ-Decomposition-In-Practice.html",
            "relUrl": "/computer%20vision/linear%20algebra/2020/03/13/RQ-Decomposition-In-Practice.html",
            "date": " • Mar 13, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "An Adventure in Camera Calibration",
            "content": "The Context . It&#39;s February 1972, the A300 airliner is being unveiled in Toulouse, let&#39;s go on an adventure (In camera calibration!). . . Let&#39;s say we have seen this photo published in a magazine, and we want to try and learn as much about the dimensions of Airbus&#39;s new aircraft as possible. In order to do so, we will need to mathematically reconstruct the camera used to take the photo, as well as the scene itself. . Control points . Now, In this case, we are lucky, because we notice the hexagonal pattern on the floor. In particular, we notice that it&#39;s a tessellating hexagonal pattern, which can only happen if all the hexagons have identical dimensions. . While we don&#39;t know the dimensions of the hexagon, we guess that each side is approximately 1.6m long, based on the height of the people in the photo. If we assume some point on the ground, say the center of a polygon is the point 0,0, we can work out the X &amp; Y location of each other polygon vertex we can see. Furthermore, we could also assume that the factory floor is flat and level. Hence the Z coordinate of each point is 0. . Let&#39;s spend ±5 minutes annotating the image, using an annotation tool like label me. I&#39;ve generated a file, which you can find attached here: . . Firstly, lets load in all of the x and y points: . import json import numpy as np JSON = json.loads(open(&#39;data/2020-02-23-An-Adventure-In-Camera-Calibration/A300.json&#39;,&#39;r&#39;).read()) polygons = {} for shape in JSON[&#39;shapes&#39;]: coords = shape[&#39;label&#39;].split(&#39;,&#39;) x,y = int(coords[0]),int(coords[1]) polygons[x,y] = shape[&#39;points&#39;] . Ok, now doing some maths, and work out the locations of each vertex of our hexagons. . from sklearn.neighbors import KDTree points = [] keys = sorted(polygons.keys()) for key in keys: poly = polygons[key] (pts_x, pts_y) = zip(*poly) pts_x = list(pts_x) pts_y = list(pts_y) #Magic analytic formula for working out the location of each point, based on which vertex, of which polygon it is. x_vertex = 0.5 * np.array([1,2,1,-1,-2,-1]) y_vertex = 0.5 * np.array([np.sqrt(3),0,-np.sqrt(3),-np.sqrt(3),0,np.sqrt(3)]) row,col = key x = row * 1.5 + x_vertex y = col * 0.5 * np.sqrt(3) + y_vertex #From before, we assume the sides of each polygon is 1.6m x*=1.6 #meters y*=1.6 #meters for idx in range(0,6): point = [] i = pts_x[idx] j = pts_y[idx] X = x[idx] Y = y[idx] Z = 0.0 points.append([i,j,X,Y,Z]) . Now we are presented with a minor problem, in many cases, we have annotated the same point up to 3 times, where the vertices of the hexagons meet. So let&#39;s go and find points that are within 10 pixels, and then take their average. If we don&#39;t do this, then we effectively over-weight some points in the image, at the expense of others. . points = np.asarray(points) np.savetxt(&quot;data/2020-02-23-An-Adventure-In-Camera-Calibration/points.csv&quot;, points) tree = KDTree(points[:,0:2], leaf_size=5) merged_indicies = [] unique_points = [] for i in range(0,points.shape[0]): if i not in merged_indicies: dist, ind = tree.query(points[i,0:2].reshape(-1, 2), k=3) indicies_to_merge = [] for j in range(0,3): if dist[0][j]&lt;10: indicies_to_merge.append(ind[0][j]) merged_indicies.append(ind[0][j]) mean_points = np.mean(points[indicies_to_merge,:],axis=0) unique_points.append(mean_points) unique_points = np.asarray(unique_points) . Camera Parameters . So, now we have a bunch of 3D points, and corresponding 2D points in the photo. . Now it&#39;s time to turn to the real magic, bundle adjustment. Basically, our task at hand, is to find a camera, which best fits the data we have measured. . Let&#39;s talk more about cameras. . Important: There are many correct ways to model a camera mathematically. This is one way. . Mathematically, cameras are are composed of two types of parameters, Intrinsic and Extrinsic. The Extrinsic parameters define the position and rotation of the camera, with respect to the origin of the points it&#39;s observing. . The Intrinsic parameters define the parameters of the camera itself, for example the Focal length, the location of the camera&#39;s radial center, as well as distortion induced by the lens. . The Extrinisic parameters are comprised of 6 degrees of freedom, given our world is 3 dimensional, and there are 3 dimensions which to rotate around. . The Intrinsic parameters are more complex. There are a number of great resources, for example Multiple View Geometry in Computer Vision, or the OpenCV documentation. However, In this case, I am assuming that the principal point, the focal length, and the radial parameters are unknown. . Note: To be clear, I&#8217;m building on the shoulders of giants, I&#8217;ve heavily adapted this example from this incredible demo by Nikolay Mayorov which you can find here . Firstly, let&#39;s go and import a bunch of stuff we will need later. . from __future__ import print_function from warnings import filterwarnings filterwarnings(&#39;ignore&#39;) . import numpy as np import matplotlib.pyplot as plt from scipy.optimize import least_squares from scipy.spatial.transform import Rotation as Rot %matplotlib inline plt.rcParams[&quot;figure.figsize&quot;] = (20,20) . points_2d = unique_points[:,0:2] points_3d = unique_points[:,2:5] print(&#39;We have {} unique points&#39;.format(points_2d.shape[0])) . We have 51 unique points . Modelling the Camera . Intrinsic Matrix . Now we come to the real magic. . begin{equation*} x = PX end{equation*}This function models the camera, taking points in 3D space, and converting them into points in 2D space. . There are lots of things going on here. . . Firstly, let&#39;s talk about the camera&#39;s intrinsic matrix. . Basically, it converts points from 3D space to 2D space. . begin{equation*} K = begin{bmatrix} f &amp; 0 &amp; c_{x} 0 &amp; f &amp; c_{y} 0 &amp; 0 &amp; 1 end{bmatrix} end{equation*}We have the focal length, $f$, and the camera optical center $c_x$ and $c_y$. . Extrinsic Matrix . Now let&#39;s talk about the camera&#39;s extrinsic matrix. . These are the 6 degrees of freedom that describe it&#39;s position and orientation within the world. That&#39;s 3 degrees for the position, and 3 for the orientation. At its heart, what we are doing is simple, but confusing. . There are so many ways to represent our setup: . Coordinate systems: 2D and 3D. Left Handed or Right Handed?     | . | Rotations: . Quaternions? | Proper Euler angles (6 different ways)? | Tait–Bryan angles (6 different ways)? | Rodrigues rotation formula? | A rotation matrix? | . | The location of the camera in to the world. (2 Different ways). . | Today we are going to use two different ways to represent the rotations, Firstly a Rodrigues rotation vector representation, and a rotation matrix. The reason why we use two different representations is because it&#39;s easier to optimise when we have 3 degrees of freedom, rather than a naive rotation matrix which uses 9 numbers to represent 3 degrees of freedom. . R represents the orientation of the camera in the World Coordinate Frame (The frame which we use to describe our 3D points). . In python, we can use convert from the Rodrigues rotation vector to the Rotation matrix as follows: . from scipy.spatial.transform import Rotation as Rot rotation_vector = camera_params[:3] R = Rot.from_rotvec(rotation_vector).as_matrix() . begin{equation*} R = begin{bmatrix} R_1 &amp; R_2 &amp; R_3 R_4 &amp; R_5 &amp; R_6 R_7 &amp; R_8 &amp; R_9 end{bmatrix} end{equation*} Now, let&#39;s talk about the Project Matrix $P$ of the camera. This takes the points all the way from their location in 3D world coordinates, to pixel coordinates, assuming we have a camera without radial distortion. There are two main ways this could be formulated. . Firstly: begin{equation*} P = KR[I|−C] end{equation*} . Secondly: begin{equation*} P = K[R | t] end{equation*} . Where $t$ is: begin{equation*} t = −RC end{equation*} . Let&#39;s go with the first method, where C is : . begin{equation*} C = begin{bmatrix} -C_X -C_Y -C_Z end{bmatrix} end{equation*} Lens distortion . However, there is one subtlety alluded to before, which is the impact of radial distortion. Simply, the camera&#39;s lens distorts the rays of light coming in, in a non-linear way. . We can model it using a Taylor series: . begin{equation*} x_c = x(1 + k_1 r + k_2 r^2 + k_3 r^3) end{equation*} begin{equation*} y_c = y(1 + k_1 r + k_2 r^2 + k_3 r^3) end{equation*} In python, we end up with: . r = np.sqrt(np.sum(points_proj**2, axis=1))) r = 1 + k1 times r + k2 * r**2 + k3 * r**3 points_proj *= r[:, np.newaxis] . Putting it all together . def project(points, camera_params): &quot;&quot;&quot;Convert 3-D points to 2-D by projecting onto images.&quot;&quot;&quot; #Rotation rotation_vector = camera_params[:3] R = Rot.from_rotvec(rotation_vector).as_matrix() #Camera Center C = camera_params[3:6].reshape(3,1) IC = np.hstack([np.eye(3),-C]) RIC = np.matmul(R,IC) #Make points Homogeneous points = np.hstack([points,np.ones((points.shape[0],1))]) #Perform Rotation and Translation #(n,k), (k,m) -&gt; (n,m) points_proj = np.matmul(points,RIC.T) #perspective divide points_proj = points_proj[:, :2] / points_proj[:, 2, np.newaxis] f = camera_params[6] k1 = camera_params[7] k2 = camera_params[8] k3 = camera_params[9] c_x = camera_params[10] c_y = camera_params[11] #Radial distortion r = np.sqrt(np.sum(points_proj**2, axis=1)) x = points_proj[:,0] y = points_proj[:,1] points_proj[:,0] = (1 + k1 * r + k2 * r**2 + k3 * r**3)*x points_proj[:,1] = (1 + k1 * r + k2 * r**2 + k3 * r**3)*y #Make points Homogeneous points_proj = np.hstack([points_proj, np.ones((points_proj.shape[0],1))]) K = np.asarray([[f, 0, c_x], [0, f, c_y], [0, 0, 1.0]]) points_proj = np.dot(points_proj,K.T) points_proj = points_proj[:,:2] return(points_proj) . Initial Parameters . Let&#39;s start by providing some hints to the optimiser about what the solution could be like, by putting in some reasonable starting conditions. . We know both the image width and height, and we can assume that the principal point is in the center of the image. . I think the camera is about 10 meters off the ground. . To make the task of optimization easier, let&#39;s rotate the camera so that it&#39;s facing directly down. This means that the points should be in front of/below it. . Let&#39;s also start off by assuming that the camera is centered above the points. It&#39;s obviously not correct, based on what we can see in the image, but it&#39;s not horrifically wrong.  . image_width = 2251 image_height = 1508 estimated_focal_length_px = 2000 camera_params = np.zeros(12) r = Rot.from_euler(&#39;x&#39;, 180, degrees=True).as_rotvec() #Rotation matrix camera_params[0] = r[0] camera_params[1] = r[1] camera_params[2] = r[2] #C camera_params[3] = points_3d[:,0].mean() camera_params[4] = points_3d[:,1].mean() camera_params[5] = 10 #f,k1,k2, camera_params[6] = estimated_focal_length_px camera_params[7] = 0 camera_params[8] = 0 camera_params[9] = 0 #c_x,c_y camera_params[10] = image_width/2.0 camera_params[11] = image_height/2.0 . Optimisation . This section below is really well explained by here. Basically, we are optimizing to minimise a geometric error. It&#39;s the distance between the 2D points we see, and the projection of their 3D counterparts. . Through a process of optimization, we aim to find parameters which result in low error, which means in turn they should represent the real parameters of the camera. . def fun(camera_params, points_2d, points_3d): #Compute residuals. points_proj = project(points_3d, camera_params) return(points_proj - points_2d).ravel() . x0 = camera_params.ravel() optimization_results = least_squares(fun, x0, verbose=1, x_scale=&#39;jac&#39;, ftol=1e-4, method=&#39;lm&#39;, loss=&#39;linear&#39;,args=(points_2d, points_3d)) . `ftol` termination condition is satisfied. Function evaluations 970, initial cost 3.7406e+07, final cost 1.7398e+02, first-order optimality 3.63e+03. . Results . Now let&#39;s go and check out the results of our optimization process. . camera_params = optimization_results.x R_Rodrigues = camera_params[0:3] C = camera_params[3:6] r = Rot.from_rotvec(R_Rodrigues) R_matrix = r.as_matrix() r = Rot.from_matrix(R_matrix.T) R_Quaternion = r.as_quat() print(&#39;Quaternions: X: {:.3f} Y: {:.3f} Z: {:.3f} W: {:.3f} &#39;.format(R_Quaternion[0],R_Quaternion[1],R_Quaternion[2],R_Quaternion[3])) print(&#39;Camera position relative to the origin in (M): X: {:.2f}, Y: {:.2f}, Z: {:.2f}&#39;.format(C[0],C[1],C[2])) focal_length_px = camera_params[6] k1 = camera_params[7] k2 = camera_params[8] k3 = camera_params[9] c_x = camera_params[10] c_y = camera_params[11] print(&#39;Focal length (Pixels): {:.2f}&#39;.format(focal_length_px)) print(&#39;CX, CY: {:.2f} {:.2f}&#39;.format(c_x,c_y)) print(&#39;K_1, K_2, K_3 : {:.6f}, {:.6f}, {:.6f}&#39;.format(k1,k2,k3)) print(&#39;Mean error per point: {:.2f} pixels &#39;.format(optimization_results.cost/points_2d.shape[0])) . Quaternions: X: 0.894 Y: -0.408 Z: 0.084 W: -0.166 Camera position relative to the origin in (M): X: -6.85, Y: -12.92, Z: 2.75 Focal length (Pixels): 1010.93 CX, CY: 1038.58 2663.52 K_1, K_2, K_3 : -0.327041, 0.175031, -0.030751 Mean error per point: 3.41 pixels . Ok, first things first, the mean error per point is 3-4 pixels, which is not great, not terrible. It&#39;s clear that we have found a decent solution, However there are some interesting things going on. . In particular, the principal point lies outside the image, which is curious to say the least. One possibility is that the image was cropped. . Now let&#39;s have a quick look at the errors for each point. . plt.hist(abs(optimization_results.fun),density=True) plt.title(&#39;Histogram of Residuals&#39;) plt.xlabel(&#39;Absolute Residual (Pixels)&#39;) plt.grid() plt.show() . So the histogram looks pretty good, apart from the one point with a high residual, which is probably due to sloppy labeling/annotation. . Now let&#39;s compare the points we annotated, with where they would be projected, using the camera parameters we found: . points_2d_proj = project(points_3d, optimization_results.x) img = plt.imread(&#39;data/2020-02-23-An-Adventure-In-Camera-Calibration/A300.jpg&#39;) plt.imshow(img) plt.scatter(points_2d[:,0],points_2d[:,1],label=&#39;Actual&#39;,c=&#39;r&#39;,alpha=0.5) plt.scatter(points_2d_proj[:,0],points_2d[:,1],label=&#39;Optimised&#39;,c=&#39;k&#39;,alpha=0.5) plt.show() . Again, this looks great. Finally, let&#39;s overlay the hexagons on the floor, to visually build confidence in our solution. . def plot_verticies(row,col): x_vertex = 0.5 * np.array([1,2,1,-1,-2,-1,1]) y_vertex = 0.5 * np.array([np.sqrt(3),0,-np.sqrt(3),-np.sqrt(3),0,np.sqrt(3),np.sqrt(3)]) x = row * 1.5 + x_vertex y = col * 0.5 * np.sqrt(3) + y_vertex x*=1.6 y*=1.6 points_3d = np.vstack([x,y,np.zeros(7)]).T points_2d_proj = project(points_3d, optimization_results.x) return(points_2d_proj) plt.imshow(img) for row in range(0,10,2): for col in range(0,10,2): points_2d_proj = plot_verticies(row,col) plt.plot(points_2d_proj[:,0],points_2d_proj[:,1],color=&#39;B&#39;,alpha=0.25) plt.text(np.mean(points_2d_proj[:,0]), np.mean(points_2d_proj[:,1]), str(row)+&#39;,&#39;+str(col), horizontalalignment=&#39;center&#39;,verticalalignment=&#39;center&#39;) for row in range(1,11,2): for col in range(1,11,2): points_2d_proj = plot_verticies(row,col) plt.plot(points_2d_proj[:,0],points_2d_proj[:,1],color=&#39;R&#39;,alpha=0.25) plt.text(np.mean(points_2d_proj[:,0]), np.mean(points_2d_proj[:,1]), str(row)+&#39;,&#39;+str(col), horizontalalignment=&#39;center&#39;,verticalalignment=&#39;center&#39;) plt.show() . In Conclusion . Awesome, we can see visually that we have found a semi-reasonable solution. . However, I&#39;m worried about the location of the principle point of the image. Normally, with most cameras, this is near the center of the image. In our case, it isn&#39;t. There are a number of reasons why this could be the case, for example, the image might have been cropped, however it&#39;s a little concerning. . I&#39;m also worried about the height of the camera, which is only 2.75M above the ground. The camera looks like it&#39;s at approximately the same height as the roof of the aircraft, which would typically be 7-10m above the ground. . In the future, Let&#39;s look more about how we can extract some more useful information from this image, and understand how confident we can be in our solution. . Thanks to Nikolay Mayorov who created the awesome demo of optimization in Scipy that I built upon, you can find the original code here. . Multiple View Geometry in Computer Vision is an incredible book that I learn more from, each time I read it. in particular, for further information see: . Finite cameras. Page 153, Multiple View Geometry in Computer Vision (Second edition) | Note: Minimizing geometric error. Page 176, Multiple View Geometry in Computer Vision (Second edition) | .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/optimisation/linear%20algebra/2020/02/23/An-Adventure-In-Camera-Calibration.html",
            "relUrl": "/computer%20vision/optimisation/linear%20algebra/2020/02/23/An-Adventure-In-Camera-Calibration.html",
            "date": " • Feb 23, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Three Interesting Papers",
            "content": "Over the past couple of months, 3 incredibly exciting papers have come out, and I want to take the opportunity to share them with you. . The papers, in no particular order are MixMatch, Selfie and Unsupervised Data Augmentation, however let’s first discuss why they are exciting. . In my day to day work, I’m faced with an avalanche of data. Raw data may be cheap, but labelled data is precious, often relying on expensive experiments or busy experts. Even then, when labelled data is available there is an omnipresent, insatiable demand to do more with it. . Semi-supervised learning offers the opportunity to leverage the raw, unlabelled data to improve our models, reducing the barriers to building a model, and democratising AI. . I’m not going to discuss how the papers are actually implemented in detail, but I will say that the papers are very promising, and I hope they will be rapidly implemented and adapted as a standard part of deep learning workflows. . In a sentence, MixMatch uses MixUp, and label sharpening (Fancy way of saying “Artificially boosting your models confidence”) in order to effectively propagate labels. My first impression was “I can’t believe that works”, but then I saw that it decreases error rates 4x with when training with small numbers of samples on CIFAR-10. . Conversely, Selfie is inspired pertaining in BERT, and extends it to CNN’s. At a high level, the pre training task is analagous to removing pieces from a jigsaw puzzle, and asking “what piece should go in each hole?”. Given the power of transfer learning, this is hugely exciting for many problems where the data you want to train on is very different to what is found in ImageNet. . Finally, there is Unsupervised Data Augmentation (UDA), which prosecutes the thesis that “better data augmentation can lead to significantly better semi-supervised learning”. As with Selfie and MixMatch, the techniques used in this paper can be applied to image data. . Deep learning is built on a history of rapidly evolving best practice, including Xavier initialisation, Data Augmentation, One Cycle Policy and MixUp. I hope that adoptions of that MixMatch, Selfie and UDA will soon join this grab bag of best practice. .",
            "url": "https://cgcooke.github.io/Blog/deep%20learning/computer%20vision/2019/06/05/Three-Interesting-Papers.html",
            "relUrl": "/deep%20learning/computer%20vision/2019/06/05/Three-Interesting-Papers.html",
            "date": " • Jun 5, 2019"
        }
        
    
  
    
        ,"post27": {
            "title": "Kalman Filters",
            "content": "Kalman Filters are magic. While they take 5 minutes to explain at a basic level, you can work with them for a career and always be learning more. I think there is something philosophically satisfying about the way that they innately combine what we already believe and what we perceive in order to come to a new belief about the world. . While this sounds somewhat abstract, Kalman Filters provide a concrete mathematical formulation for fusing data from different sources, as well as physical models to provide (potentially) optimum estimates of the state of a system. . For less philosophy, and more maths, I strongly recommend stopping at this point and giving this incredible post a read. Afterwards, let’s talk about Kalman filters in a concrete way, . Creating a Kalman Filter in 7 easy steps: . One of the challenges with Kalman filters is that it’s easy to be initially overwhelmed by the mathematical background, and loose sight of what their implementation looks like in practice. In reality, it’s possible to break the implementation down into a series of discrete steps, which come together to fully describe the filter. . FilterPy is a fantastic Python library for creating Kalman filters, and has an accompanying book, which takes a deep dive into the mathematical theory of Kalman filters. Lets initially discuss the general process for defining a Kalman filter, before applying it to practical application. . x : Our filter state estimate, IE what we want to estimate. If we want to track an object moving an a video, this could be it’s pixel coordinates, as well as it’s velocity in pixels per second. [x,y,vx,vy] . | P : The covariance matrix. Encodes how certain the filter is about it’s estimates, evolves over time. In the object tracking example, how “confident” the filter is about the position of an object and it’s velocity. As the filter receives more measurements, the values in the covariance matrix are “washed out”, and so the the filter tends to be insensitive to the values used. . | Q : The process uncertainty. How large is the error associated with the system doing something unexpected between measurements? I find this the hardest to set, as it requires careful thought about the process. For example, if we are tracking the position and velocity of an object once a second, we would have more uncertainty if we were tracking the position of a fruit fly than an oil tanker. . | R : How uncertain each of our measurements are. This can be determined either through reading sensor datasheets or educated guesses. . | H : How to each measurement is related to the internal state of our system, in addition to scaling measurements. IE, If we have a GPS receiver, it tells us about our position, while an accelerometer tells us about our position. . | F : The state transition matrix. How the system evolves over time. IE, if we know the position and velocity of an object, then in the absence of any error or external influence we can predict it’s next position from it’s current position and velocity. . | B : The control matrix. This matrix allows us to tell the filter about how we expect any inputs we provide the system (u) to update the state of the system. In many cases, especially when we are taking measurements of a system we don’t control, the control matrix is not required. . | At this point, I think it’s worthwhile considering how all of these matrices are related to each other. Tim Babb of Bzarg, has a fantastic diagram, which sets out how information flows through all of the filters mentioned above. If you haven’t already, I strongly recommend you read his post on how Kalman filters work . Looking at the relationships between all of the matrices, . x and P are outputs of the filter, they tell what the filter believes the state of the system to be. . | H, F and B are matrices which control how the filter operates. . | Q, R are closely related, because they both denote uncertainty, in the process as well as the measurements. . | z and u denote inputs to the filter, in the case where we don’t control the system, then u is not required. . | A real world example: . Let’s look at a real world example. In computer vision, object tracking is the process of associating different detections of an object from different images/frames into a single “track”. Many algorithms have been developed for this task (Simple Online and Realtime Tracking)[https://arxiv.org/abs/1602.00763] is particularly elegant. In summary, SORT creates a Kalman filter for each object it wants to track, and then predicts the location and size of each object, in each frame using the filter. . Alex Bewley, one of the creators of SORT has developed a fantastic (implementation)[https://github.com/abewley/sort] of SORT, which uses Filterpy. . Let’s take a look at his implementation, through the lens of what I’ve discussed above: . Quickly defining some nomenclature, . u and v are the x and y pixel coordinates of the center of the bounding box around an object being tracked. . | s and r are tha scale and aspect ratio of the bounding box surrounding the object. . | u˙,v˙ dot u, dot vu˙,v˙ are the x and y velocity of the bounding box. . | s˙ dot ss˙ is the rate at which the scale of the bounding box is changing. . | . kf = KalmanFilter(dim_x=7, dim_z=4) . Our internal state is 7 dimensional: . [u,v,s,r,u˙,v˙,s˙][u, v, s, r, dot u, dot v , dot s][u,v,s,r,u˙,v˙,s˙] . While our input vector is 4 dimensional: . [u,v,s,r][u, v, s, r][u,v,s,r] . kf.F = np.array([[1,0,0,0,1,0,0], [0,1,0,0,0,1,0], [0,0,1,0,0,0,1], [0,0,0,1,0,0,0], [0,0,0,0,1,0,0], [0,0,0,0,0,1,0], [0,0,0,0,0,0,1]]) . The state transition matrix tells us that at each timestep, we update our state as follows: . u=u+u˙u = u + dot uu=u+u˙ . v=v+v˙v = v + dot vv=v+v˙ . s=s+s˙s = s + dot ss=s+s˙ . kf.H = np.array([[1,0,0,0,0,0,0], [0,1,0,0,0,0,0], [0,0,1,0,0,0,0], [0,0,0,1,0,0,0]]) . The sensor matrix tells us that we are directly measuring [u,v,s,r][u, v, s, r][u,v,s,r]. . kf.R = np.array([[ 1, 0, 0, 0], [ 0, 1, 0, 0,], [ 0, 0, 10, 0,], [ 0, 0, 0, 10,]]) . The sensor noise matrix tells us that we can measure uuu and vvv with a much higher degree of certainty than sss and rrr. . kf.P = np.array([[ 10, 0, 0, 0, 0, 0, 0], [ 0, 10, 0, 0, 0, 0, 0], [ 0, 0, 10, 0, 0, 0, 0], [ 0, 0, 0, 10, 0, 0, 0], [ 0, 0, 0, 0, 10000, 0, 0], [ 0, 0, 0, 0, 0, 10000, 0], [ 0, 0, 0, 0, 0, 0, 10000]]) . The covariance matrix tells us that the filter should have a very high initial uncertinty for each of the velocity components. . kf.Q = np.array([[1.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00] [0.e+00, 1.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00] [0.e+00, 0.e+00, 1.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00] [0.e+00, 0.e+00, 0.e+00, 1.e+00, 0.e+00, 0.e+00, 0.e+00] [0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-02, 0.e+00, 0.e+00] [0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-02, 0.e+00] [0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-04]]) . The Process uncertainty matrix tells us how much “uncertainty” there is in each component of the systems behaviour. . Filterpy has a function which can be very useful for generating Q. . filterpy.common.Q_discrete_white_noise . Further reading . Control theory is a broad an intellectually stimulating area, with broad applications. Brian Douglas has an incredible YouTube channel which I strongly recommend. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/2019/03/23/Kalman-Filters.html",
            "relUrl": "/bayesian/2019/03/23/Kalman-Filters.html",
            "date": " • Mar 23, 2019"
        }
        
    
  
    
        ,"post28": {
            "title": "256 Shades Of Grey",
            "content": "On February 22, 2000, after 11 days of measurements, the most comprehensive map ever created of the earth’s topography was complete. The space shuttle Endeavor had just completed the Shuttle Radar Topography Mission, using a specialised radar to image the earths surface. . The Digital Elevation Map (DEM) produced by this mission is in the public domain and provides the measured terrain high at ~90 meter resolution. The mission mapped 99.98% of the area between 60 degrees North and 56 degrees South. . In this post, I will examine how to process the raw DEM so it is more intuitively interpreted, through the use of hillshading,slopeshading &amp; hypsometric tinting. . The process of transforming the raw GeoTIFF into the final imagery product is simple. Much of the grunt work being carried out by GDAL, the Geospatial Data Abstraction Library. . In order, we need to: . Download a DEM as a GeoTIFF | Extract a subsection of the GeoTIFF | Reproject the subsection | Make an image by hillshading | Make an image by coloring the subsection according to altitude | Make an image by coloring the subsection according to slope | Combine the 3 images into a final composite | DEM . Several different DEM’s have been created from the data collected on the SRTM mission, in this post I will use the CGIAR SRTM 90m Digital Elevation Database. Data is provided in 5x5 degree tiles, with each degree of latitude equal to approximately 111Km. . Our first task is to acquire a tile. Tiles can be downloaded from http://data.cgiar-csi.org/srtm/tiles/GeoTIFF/ using wget. . import os import math from PIL import Image, ImageChops, ImageEnhance from matplotlib import cm . def downloadDEMFromCGIAR(lat,lon): &#39;&#39;&#39; Download a DEM from CGIAR FTP repository &#39;&#39;&#39; fileName = lonLatToFileName(lon,lat)+&#39;.zip&#39; &#39;&#39;&#39; Check to see if we have already downloaded the file &#39;&#39;&#39; if fileName not in os.listdir(&#39;.&#39;): os.system(&#39;&#39;&#39;wget --user=data_public --password=&#39;GDdci&#39; http://data.cgiar-csi.org/srtm/tiles/GeoTIFF/&#39;&#39;&#39;+fileName) os.system(&#39;unzip &#39;+fileName) . def lonLatToFileName(lon,lat): &#39;&#39;&#39; Compute the input file name &#39;&#39;&#39; tileX = int(math.ceil((lon+180)/5.0)) tileY = -1*int(math.ceil((lat-65)/5.0)) inputFileName = &#39;srtm_&#39;+str(tileX).zfill(2)+&#39;_&#39;+str(tileY).zfill(2) return(inputFileName) . lon,lat = -123,49 inputFileName = lonLatToFileName(lon,lat) downloadDEMFromCGIAR(lat,lon) . Slicing . The area I have selected covers Washington State and British Columbia, with file name srtm_12_03.tif. . Let’s use GDAL to extract a subsection of the tile.The subsection covers Vancouver Island and the Pacific Ranges stretching from 125ºW - 122ºW &amp; 48ºN - 50ºN. Using gdalwarp: . !! gdalwarp -q -te -125 48 -122 50 -srcnodata -32768 -dstnodata 0 srtm_12_03.tif subset.tif . Our next step is to transform the subsection of the tile to a different projection. The of the points in the subsection are located on a grid 1/1200th of a degree apart. While degrees of latitude are always ~110Km in size, resulting in ~92.5M resolution, degrees of longitude decrease in size, from ~111Km at the equator to 0Km at the poles. A different scale exists between the latitude &amp; longitude axis and a longitude scale that depends on the latitude. . A solution is to project that points so that there is a consistent and equal scale in the X/Y plane. One choice is to use a family of projections called Universal Transverse Mercator. Each UTM projection can map points from longitude &amp; latitude to X &amp; Y coordinates in meters. The UTM projection is useful because it locally preserves both shapes and distances, over a distances of up to several hundred kilometres. . The tradeoff is that several different UTM projections are required for different points on earth, 120 to be precise. Fortunately it is relatively trivial to work out the required projection based on the longitude and latitude. Almost every conceivable projection has been assigned a code by the European Petroleum Survey Group (EPSG). This EPSG code can be used to unambiguously specify the projection being used. With UTM, each code starts with either 327 or 326, depending on the hemisphere of the projection. . utmZone = int((math.floor((lon + 180)/6) % 60) + 1) &#39;&#39;&#39; Check to see if file is in northern or southern hemisphere &#39;&#39;&#39; if lat&lt;0: EPSGCode = &#39;EPSG:327&#39;+str(utmZone) else: EPSGCode = &#39;EPSG:326&#39;+str(utmZone) . Once we have identified the correct EPSG code to use, the process of warping the subset to a new projection is relatively straightforward. . In the following system call to gdalwarp, t_srs denotes the target projection, and tr specifies the resolution in the X and Y plane. The Y resolution is negative because the in the GDAL file uses a row, column based coordinate system. . In this coordinate system, the origin is in the top left hand corner of the file. The row value increases as you move down the file, like an excel spreadsheet, however the UTM Y coordinate decreases. This results in the negative sign in the resolution. . os.system(&#39;gdalwarp -q -t_srs &#39;+EPSGCode+&#39; -tr 100 -100 -r cubic subset.tif warped.tif&#39;) . Hillshading . At this point we can begin to visualise the DEM. One highly effective method is hillshading, which models the way the surface of the DEM would be illuminated by light projected onto it. Shading of the slopes allows the DEM to be more intuitively interpreted than just coloring by height alone. . . !! gdaldem hillshade -q -az 45 -alt 45 warped.tif hillshade.tif . Hypsometric Tinting . Hillshading can also be combined with height information to aid interpretation of the topography. The technical name for the process of coloring a DEM based on height is hypsometric tinting. The process is simple, with GDAL mapping colors to cell heights, using a provided color scheme. . . def createColorMapLUT(minHeight,maxHeight,cmap = cm.YlGn_r,numSteps=256): &#39;&#39;&#39; Create a colormap for visualisation &#39;&#39;&#39; f =open(&#39;color_relief.txt&#39;,&#39;w&#39;) f.write(&#39;-0.1,135,206,250 n&#39;) f.write(&#39;0.1,135,206,250 n&#39;) for i in range(0,numSteps): r,g,b,a= cmap(i/float(numSteps)) height = minHeight + (maxHeight-minHeight)*(i/numSteps) f.write(str(height)+&#39;,&#39;+str(int(255*r))+&#39;,&#39;+str(int(255*g))+&#39;,&#39;+str(int(255*b))+&#39; n&#39;) f.write(str(-1)+&#39;,&#39;+str(int(255*r))+&#39;,&#39;+str(int(255*g))+&#39;,&#39;+str(int(255*b))+&#39; n&#39;) createColorMapLUT(minHeight=10,maxHeight=2658) . !! gdaldem color-relief -q warped.tif color_relief.txt color_relief.tif . Slope Shading . Another technique for visualizing terrain is slopeshading. While hypsometric tinting assigns colors to cells based on elevation, slope shading assigns colors to pixels based on the slope (0º to 90º). In this case, white (255,255,255) is assigned to slopes of 0º and black (0,0,0) is assigned to slopes of 90º, with varying shades of grey for slopes in-between. . . This color scheme is encoded in a txt file for gdaldem as follows: . f = open(&#39;color_slope.txt&#39;,&#39;w&#39;) f.write(&#39;0 255 255 255 n&#39;) f.write(&#39;90 0 0 0 n&#39;) f.close() . The computation of the slope shaded dem takes place over two steps. . The slope of each cell is computed | A shade of grey is assigned to each cell depending on the slope. | !! gdaldem slope -q warped.tif slope.tif !! gdaldem color-relief -q slope.tif color_slope.txt slopeshade.tif . Layer Merging . The final step in producing the final product is to merge the 3 different created images. The python Image Library (PIL) is a quick and dirty way to accomplish this task, with the 3 layers are merged using pixel by pixel multiplication. . One important detail to note is that the pixel by pixel multiplication occurs in the RGB space. From a theoretical perspective, it’s probably better that each pixel is first transformed to the Hue, Saturation, Value (HSV) color space, and the value is then multiplied by the hillshade and slope shade value, before being transformed back into the RGB color space. In practical terms however, the RGB space multiplication is a very reasonable approximation. . In one final tweak, the brightness of the output image is increased by 40%, to offset the average reduction in brightness caused by multiplying the layers together. . . &#39;&#39;&#39; Merge components using Python Image Lib &#39;&#39;&#39; slopeshade = Image.open(&quot;slopeshade.tif&quot;).convert(&#39;L&#39;) hillshade = Image.open(&quot;hillshade.tif&quot;) colorRelief = Image.open(&quot;color_relief.tif&quot;) #Lets just fill in any gaps in the hillshading ref = Image.new(&#39;L&#39;, slopeshade.size,180) hillshade = ImageChops.lighter(hillshade,ref) shading = ImageChops.multiply(slopeshade, hillshade).convert(&#39;RGB&#39;) merged = ImageChops.multiply(shading,colorRelief) &#39;&#39;&#39; Adjust the brightness to take into account the reduction caused by hillshading&#39;&#39;&#39; enhancer = ImageEnhance.Brightness(merged) img_enhanced = enhancer.enhance(1.4) img_enhanced.save(&#39;Merged.png&#39;) . Further reading . I found the following sources to be invaluable in compiling this post: . Creating color relief and slope shading | A workflow for creating beautiful relief shaded DEMs using gdal | Shaded relief map in python | Stamen Design | .",
            "url": "https://cgcooke.github.io/Blog/remote%20sensing/2018/11/18/256-Shades-of-Grey.html",
            "relUrl": "/remote%20sensing/2018/11/18/256-Shades-of-Grey.html",
            "date": " • Nov 18, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Cameron! . I’m passionate about Computer Vision, Robotics, Deep Learning, Bayesian Analysis and Flight. . I’m writing a blog because the first step to better understanding something is to try and explain it to someone else. . I currently work with Airbus in Toulouse, France, where I get to learn from some incredible people, while helping to shape the future of the Aerospace industry. . You can read more about what I do where Airbus here. . .",
          "url": "https://cgcooke.github.io/Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://cgcooke.github.io/Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}