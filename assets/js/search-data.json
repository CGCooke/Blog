{
  
    
        "post0": {
            "title": "Super Fun with Bayesian Magic",
            "content": "In Australia, there have been a number of radical fiscal proposals to fight the economic impacts of COV19. One of these is to allow &quot;Casual&quot; workers (Those without paid leave entitlements) to withdraw money from their Superannuation account. It&#39;s a radical proposal, and one of my friends wanted to understand more about the impacts that this would have. . Their exact question was: . &quot;I want to know what you would estimate the balance of young people to be (Under the age of 34)&quot; . . Note: Superannuation is a type of pension savings account in Australia, where 9.5% (or more) of an employees gross salary is contributed by an employer to an employees account. The account is usually only avalible to the employee when they retire at age 65. While employees have an individual fund, investment decisions are made by a fund manager. With $ $1.5 Trillion USD of total assets, and 15M members, the scheme has an average balance of $ $100,000 USD per member. . Let&#39;s do some analysis, to try and understand more about what the balance of a typical member, in different age and gender brackets looks like. . I&#39;m focussing on a Superannuation fund called Hostplus who describe themselves as follows: . &quot;Hostplus is the industry fund for those that live and love Australian hospitality, tourism, recreation and sport&quot;. . Hostplus is a good fund to analyze, given (65%)[https://www.aph.gov.au/About_Parliament/Parliamentary_Departments/Parliamentary_Library/pubs/rp/rp1718/CasualEmployeesAustralia#_Toc504135061] of employees in the Accommodation and food services sector are casual employees. . So the great news is that the data we need is provided by an Australian goverment body called APRA, and can be found here . In particualr, we are interested in the demographic data which can be found on tab 12. . Ok, so I&#39;ve taken the data, and filtered out some of the blank rows, and columns which are redundant. . import pandas as pd df = pd.read_csv(&#39;data/22-3-2020-Super-Fun-With-Bayesian-Magic/superannuation_demographic_data.csv&#39;, sep=&#39;;&#39;,header=2,skiprows=[3],na_values=&#39;*&#39;,thousands=&#39; xa0&#39;) display(df) . Fund name RSE Regulatory classification Fund type RSE Membership base Fund&#39;s RSE licensee RSE licensee ownership type RSE licensee profit status Total number of member accounts at the end of period Total members&#39; benefits at end of period Number of members accounts : female ... 35 to 44.3 45 to 49.3 50 to 54.3 55 to 59.3 60 to 64.3 65 to 69.3 70 to 74.3 75 to 84.3 85+.3 Age not available.3 . 0 Advance Retirement Suite | Public offer | Retail | General base | BT Funds Management Limited | Financial services corporation ownership | For profit status | 4257.0 | 242016.0 | 2022.0 | ... | 5067.0 | 16385.0 | 25337.0 | 33835.0 | 24265.0 | 14105.0 | 8389.0 | 8522.0 | 2115.0 | NaN | . 1 Alcoa of Australia Retirement Plan | Non public offer | Corporate | Corporate base | Alcoa of Australia Retirement Plan Pty Ltd | Employer sponsor (non-public sector) ownership | Not for profit status | 5575.0 | 2110399.0 | 943.0 | ... | 144855.0 | 199498.0 | 316700.0 | 496428.0 | 368924.0 | 180194.0 | 90768.0 | NaN | NaN | NaN | . 2 AMG Super | Public offer | Retail | General base | Equity Trustees Superannuation Limited | Financial services corporation ownership | For profit status | 26631.0 | 1207910.0 | 16401.0 | ... | 133919.0 | 73901.0 | 77101.0 | 74967.0 | 80670.0 | 80669.0 | 46792.0 | 38600.0 | 6737.0 | NaN | . 3 AMP Eligible Rollover Fund | Public offer | Retail - ERF | General base | AMP Superannuation Limited | Financial services corporation ownership | For profit status | 344498.0 | 1314606.0 | 144799.0 | ... | 123236.0 | 146850.0 | 144130.0 | 157326.0 | 120133.0 | 80376.0 | 21719.0 | 6162.0 | 1443.0 | 2805.0 | . 4 AMP Retirement Trust | Public offer | Retail | General base | AMP Superannuation Limited | Financial services corporation ownership | For profit status | 290026.0 | 16657866.0 | 122790.0 | ... | 1250362.0 | 892699.0 | 958048.0 | 1239739.0 | 1869105.0 | 1726314.0 | 1272513.0 | 553326.0 | 29868.0 | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 166 WA Local Government Superannuation Plan | Public offer | Public Sector | Government base | WA Local Government Superannuation Plan Pty Ltd | Nominating organisation ownership | Not for profit status | 56398.0 | 3916275.0 | NaN | ... | 329344.0 | 286557.0 | 355739.0 | 411157.0 | 384366.0 | 252799.0 | 134972.0 | NaN | NaN | NaN | . 167 Wealth Personal Superannuation and Pension Fund | Public offer | Retail | General base | N. M. Superannuation Proprietary Limited | Financial services corporation ownership | For profit status | 307147.0 | 45164907.0 | NaN | ... | 1962911.0 | 1799016.0 | 2251413.0 | 3384014.0 | 5086797.0 | 5072506.0 | 3528669.0 | 2495078.0 | 218762.0 | NaN | . 168 Westpac Mastertrust - Superannuation Division | Public offer | Retail | General base | Westpac Securities Administration Limited | Financial services corporation ownership | For profit status | 193312.0 | 6293890.0 | 78447.0 | ... | 1042664.0 | 632065.0 | 575985.0 | 527562.0 | 366485.0 | 162305.0 | 85684.0 | 96966.0 | 19493.0 | NaN | . 169 Westpac Personal Superannuation Fund | Public offer | Retail | General base | Westpac Securities Administration Limited | Financial services corporation ownership | For profit status | 41166.0 | 367925.0 | 16487.0 | ... | 4435.0 | 24258.0 | 46543.0 | 68753.0 | 60941.0 | 32357.0 | 15916.0 | NaN | NaN | NaN | . 170 Zurich Master Superannuation Fund | Public offer | Retail | General base | Zurich Australian Superannuation Pty Ltd | Financial services corporation ownership | For profit status | 15694.0 | 1075407.0 | 5123.0 | ... | 44589.0 | 75568.0 | 123446.0 | 198098.0 | 154718.0 | 72766.0 | 34889.0 | 37626.0 | NaN | NaN | . 171 rows × 65 columns . hostplus_row = df[df[&#39;Fund name&#39;] == &#39;HOSTPLUS Superannuation Fund&#39;] display(hostplus_row) . Fund name RSE Regulatory classification Fund type RSE Membership base Fund&#39;s RSE licensee RSE licensee ownership type RSE licensee profit status Total number of member accounts at the end of period Total members&#39; benefits at end of period Number of members accounts : female ... 35 to 44.3 45 to 49.3 50 to 54.3 55 to 59.3 60 to 64.3 65 to 69.3 70 to 74.3 75 to 84.3 85+.3 Age not available.3 . 65 HOSTPLUS Superannuation Fund | Public offer | Industry | Industry base | Host-Plus Pty. Limited | Nominating organisation ownership | Not for profit status | 1193243.0 | 44680088.0 | 612812.0 | ... | 7034224.0 | 3394575.0 | 2750257.0 | 2490451.0 | 1843587.0 | 1008357.0 | 409016.0 | 165163.0 | NaN | NaN | . 1 rows × 65 columns . Ok, Let&#39;s pull the data we need out of the Pandas dataframe. Please forgive me for this horrible horrible hack. . age_brackets = [&#39;&lt;25&#39;,&#39;25 to 34&#39;, &#39;35 to 44&#39;, &#39;45 to 49&#39;, &#39;50 to 54&#39;, &#39;55 to 59&#39;, &#39;60 to 64&#39;, &#39;65 to 69&#39;, &#39;70 to 74&#39;, &#39;75 to 84&#39;, &#39;85+&#39;] members_f_numbers = hostplus_row[[&#39;&lt;25&#39;,&#39;25 to 34&#39;, &#39;35 to 44&#39;, &#39;45 to 49&#39;, &#39;50 to 54&#39;, &#39;55 to 59&#39;, &#39;60 to 64&#39;, &#39;65 to 69&#39;, &#39;70 to 74&#39;, &#39;75 to 84&#39;, &#39;85+&#39;]].values.ravel() members_m_numbers = hostplus_row[[&#39;&lt;25.2&#39;, &#39;25 to 34.2&#39;, &#39;35 to 44.2&#39;,&#39;45 to 49.2&#39;, &#39;50 to 54.2&#39;, &#39;55 to 59.2&#39;, &#39;60 to 64.2&#39;, &#39;65 to 69.2&#39;,&#39;70 to 74.2&#39;, &#39;75 to 84.2&#39;, &#39;85+.2&#39;]].values.ravel() #Total account balance is denominated in 1000&#39;s of dollars members_f_total_account_balance = 1000*hostplus_row[[&#39;&lt;25.1&#39;, &#39;25 to 34.1&#39;, &#39;35 to 44.1&#39;, &#39;45 to 49.1&#39;, &#39;50 to 54.1&#39;, &#39;55 to 59.1&#39;,&#39;60 to 64.1&#39;, &#39;65 to 69.1&#39;, &#39;70 to 74.1&#39;, &#39;75 to 84.1&#39;, &#39;85+.1&#39;]].values.ravel() members_m_total_account_balance = 1000*hostplus_row[[&#39;&lt;25.3&#39;, &#39;25 to 34.3&#39;, &#39;35 to 44.3&#39;, &#39;45 to 49.3&#39;, &#39;50 to 54.3&#39;, &#39;55 to 59.3&#39;, &#39;60 to 64.3&#39;, &#39;65 to 69.3&#39;, &#39;70 to 74.3&#39;, &#39;75 to 84.3&#39;, &#39;85+.3&#39;,]].values.ravel() . Now we have the data, let&#39;s do a quick, visualization of ages of HOSTPLUS&#39;s members. One subtle point to call out, is that the age brackets are not uniform. . import numpy as np import matplotlib.pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [20, 20] . plt.bar(age_brackets,members_f_numbers,label=&#39;Female&#39;,alpha=0.5,width=0.95,color=&#39;#A60628&#39;) plt.bar(age_brackets,members_m_numbers,label=&#39;Male&#39;,alpha=0.5,width=0.95,color=&#39;#348ABD&#39;) plt.grid() plt.legend() plt.title(&#39;Histogram of HOSTPLUS Member Ages&#39;) plt.ylabel(&#39;Number of members&#39;) plt.xlabel(&#39;Age Bracket (Years)&#39;) plt.show() . plt.bar(age_brackets,100*np.cumsum(members_f_numbers)/np.nansum(members_f_numbers),label=&#39;Female&#39;,alpha=0.5,width=0.95,color=&#39;#A60628&#39;) plt.bar(age_brackets,100*np.cumsum(members_m_numbers)/np.nansum(members_m_numbers),label=&#39;Male&#39;,alpha=0.5,width=0.95,color=&#39;#348ABD&#39;) plt.grid() plt.legend() plt.title(&#39;Cumulative Histogram of HOSTPLUS Member Ages&#39;) plt.ylabel(&#39;Number of members&#39;) plt.xlabel(&#39;Age Bracket (Years)&#39;) plt.show() . Ok, so from this, it&#39;s clear that a typical member is in the range of 25-34 years old, with slightly more women than men. Now let&#39;s look at the account balances. . members_f_average_account_balance = members_f_total_account_balance/members_f_numbers members_m_average_account_balance = members_m_total_account_balance/members_m_numbers plt.bar(age_brackets,members_f_average_account_balance,label=&#39;Female&#39;,alpha=0.5,width=0.95,color=&#39;#A60628&#39;) plt.bar(age_brackets,members_m_average_account_balance,label=&#39;Male&#39;,alpha=0.5,width=0.95,color=&#39;#348ABD&#39;) plt.grid() plt.legend() plt.title(&#39;HOSTPLUS average account balance vs age and gender&#39;) plt.ylabel(&#39;Average account balance ($AUD)&#39;) plt.xlabel(&#39;Age Bracket (Years)&#39;) ax = plt.gca() ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: &quot;${:,}&quot;.format(int(x)))) plt.show() . [ 3766.51717258 22381.06381793 59321.49302569 85494.89988666 101093.80628561 117269.43541932 128401.37902215 133610.30873195 130176.95735201 142751.08038029 nan] [ 3979.76456217 20333.99939392 43660.97051091 56223.28455285 64531.66002812 76961.55346693 88071.80594217 107323.93657186 115123.1292517 144849.14841849 nan] . . Note: There is a clear trend which it would be remiss to ignore. Men have significantly larger balances than women, over a large part of the age curve. I&#8217;m not a labour economist, so I don&#8217;t have the right background to correctly apprortion the causes for this. . So I think a logical next question, is what is the gross income of members by age bracket and gender. This question is a little trickier to determine, given we can&#39;t directly observe it. . Let&#39;s step back for a minute, and think about how the superannuation system works. Over time, 9.5%, or more of an employees salary is taken, and invested into the fund. Over the long-run, the value of this invested capital increases. Fortunately, Hostplus has been very effective at investing it&#39;s funds. It&#39;s delivered an average compound average growth rate (CAGR) of [8.6%(https://newsroom.hostplus.com.au/hostplus-best-in-show-with-125-mysuper-return/) over the past 15 years. . Before we do anything fancy, lets keep things really simple. Let&#39;s assume the following: . * Members contribute exactly 9.5% of their salary to the fund * Investment returns are zero * Members earn a constant salary over a 10 year period. . Firstly, let&#39;s create bins of uniform width of 10 years each. . def create_uniform_bins(x): new_array = np.array([x[0],x[1],x[2],0.5*(x[3]+x[4]),0.5*(x[5]+x[6]),0.5*(x[7]+x[8]),x[9]]) return(new_array) members_f_average_account_balance_uniform = create_uniform_bins(members_f_average_account_balance) members_m_average_account_balance_uniform = create_uniform_bins(members_m_average_account_balance) age_brackets_uniform = [&#39;15 to 24&#39;,&#39;25 to 34&#39;,&#39;35 to 44&#39;,&#39;45 to 54&#39;,&#39;55 to 64&#39;,&#39;65 to 74&#39;,&#39;75 to 84&#39;] plt.bar(age_brackets_uniform,members_f_average_account_balance_uniform,label=&#39;Female&#39;,alpha=0.5,width=0.95,color=&#39;#A60628&#39;) plt.bar(age_brackets_uniform,members_m_average_account_balance_uniform,label=&#39;Male&#39;,alpha=0.5,width=0.95,color=&#39;#348ABD&#39;) plt.grid() plt.legend() plt.title(&#39;HOSTPLUS average account balance vs age and gender (Unifrom bins)&#39;) plt.ylabel(&#39;Average account balance ($AUD)&#39;) plt.xlabel(&#39;Age Bracket (Years)&#39;) ax = plt.gca() ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: &quot;${:,}&quot;.format(int(x)))) plt.show() . Ok, so if we are assuming that the superannuation fund balance is just the integral/sum of all contributions, we can find the contributions by taking the derivative of the balance. This will give us the change in the balance from one time period to another. If we divide this by the length, of the time period (10 years), we can find the contribution made each year. . contribution_rate_f = np.diff(members_f_average_account_balance_uniform)/10.0 contribution_rate_m = np.diff(members_m_average_account_balance_uniform)/10.0 plt.bar(age_brackets_uniform[:-1],contribution_rate_f,label=&#39;Female&#39;,alpha=0.5,width=0.95,color=&#39;#A60628&#39;) plt.bar(age_brackets_uniform[:-1],contribution_rate_m,label=&#39;Male&#39;,alpha=0.5,width=0.95,color=&#39;#348ABD&#39;) plt.grid() plt.legend() plt.title(&#39;HOSTPLUS average account contributions p/a vs age and gender&#39;) plt.ylabel(&#39;Contribution p/a ($AUD)&#39;) plt.xlabel(&#39;Age Bracket (Years)&#39;) ax = plt.gca() ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: &quot;${:,}&quot;.format(int(x)))) plt.show() . If we then divide this contribution by 9.5%, we can find the implied salary for each time period. . supperannuation_contribution_rate_percent = 9.5 salary_f = contribution_rate_f / (supperannuation_contribution_rate_percent/100.0) salary_m = contribution_rate_m / (supperannuation_contribution_rate_percent/100.0) plt.bar(age_brackets_uniform[:-1],salary_f,label=&#39;Female&#39;,alpha=0.5,width=0.95,color=&#39;#A60628&#39;) plt.bar(age_brackets_uniform[:-1],salary_m,label=&#39;Male&#39;,alpha=0.5,width=0.95,color=&#39;#348ABD&#39;) plt.grid() plt.legend() plt.title(&#39;HOSTPLUS average gross salary p/a vs age and gender&#39;) plt.ylabel(&#39;Gross salary p/a ($AUD)&#39;) plt.xlabel(&#39;Age Bracket (Years)&#39;) ax = plt.gca() ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: &quot;${:,}&quot;.format(int(x)))) plt.show() . This is incredible. For context, the average fulltime salary for Australian men is approximately $ $ $ $ $$85,000 and $ $ $ $ $$75,000 for women. It&#39;s also why I feel it&#39;s so important to try something simple and dumb, before trying something elegant and clever. . So, what are we missing here? . Well, .",
            "url": "https://cgcooke.github.io/Blog/bayesian/finance/2020/04/03/Super-Fun-With-Bayesian-Magic.html",
            "relUrl": "/bayesian/finance/2020/04/03/Super-Fun-With-Bayesian-Magic.html",
            "date": " • Apr 3, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "RQ Decomposition In Practice",
            "content": "Let&#39;s keep things short and sweet. . Given a camera projection matrix, $P$, we can decompose it into a $K$ (Camera Matrix), $R$ (Rotation Matrix) and $C$ (Camera Centroid) matrix. . IE, given we have $P = K[R|-RC]$, We want to find $K$, $R$ and $C$. . The method is simply described in Multiple View Geometry in Computer Vision (Second Edition), on page 163, however let&#39;s turn it into a practical Python implementation. . Let&#39;s follow along with an example from the book. . import numpy as np import scipy.linalg np.set_printoptions(precision=2) . P = np.array([[3.53553e+2, 3.39645e+2, 2.77744e+2, -1.44946e+6], [-1.03528e+2, 2.33212e+1, 4.59607e+2, -6.3252e+5], [7.07107e-1, -3.53553e-1, 6.12372e-1, -9.18559e+2]]) . So, we have: $P = [M | −MC]$ . M can be decomposed as $M=KR$ using the RQ decomposition. . M = P[0:3,0:3] K, R = linalg.rq(M) . So far, so good. . Now things get a little more complex. . We want to find a Camera matrix with a positive diagonal, giving positive focal lengths. . However, in case this doesn&#39;t happen, we can adjust the sign of the column of each column of both the $K$ and $R$ matrix, to &quot;Make it so&quot;. . T = np.diag(np.sign(np.diag(K))) if linalg.det(T) &lt; 0:     T[1,1] *= -1 K = np.dot(K,T) R = np.dot(T,R) .     Finally, we can find the Camera Center ($C$). . We have $P_4$, the 4th column of $P$. . $P_4 = −MC$ . From this, we can find $C = {-M}^{-1} P_4$ . def factorize(P): M = P[:,0:3] K,R = scipy.linalg.rq(M) T = np.diag(np.sign(np.diag(K))) if scipy.linalg.det(T) &lt; 0: T[1,1] *= -1 K = np.dot(K,T) R = np.dot(T,R) C = np.dot(scipy.linalg.inv(-M),P[:,3]) return(K,R,C) K,R,C = factorize(P) print(&#39;K&#39;) print(K) print(&#39;R&#39;) print(R) print(&#39;C&#39;) print(C) . K [[468.16 91.23 300. ] [ 0. 427.2 200. ] [ 0. 0. 1. ]] R [[ 0.41 0.91 0.05] [-0.57 0.22 0.79] [ 0.71 -0.35 0.61]] C [1000.01 2000. 1499.99] . Voila! . This presentation is a great read, and provides a good overview of the RQ and QR decompositions. .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/linear%20algebra/2020/04/03/RQ-Decomposition-In-Practice.html",
            "relUrl": "/computer%20vision/linear%20algebra/2020/04/03/RQ-Decomposition-In-Practice.html",
            "date": " • Apr 3, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Bayesian Camera Resectioning",
            "content": "import numpy as np import matplotlib.pyplot as plt %matplotlib inline from IPython.display import set_matplotlib_formats set_matplotlib_formats(&#39;svg&#39;) plt.rcParams[&#39;figure.figsize&#39;] = [20, 20] from pymc3 import * . WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named &#39;mkl&#39; . size = 200 true_intercept = 0 true_slope = 2 x = np.linspace(0, 1, size) # y = a + b*x true_regression_line = true_intercept + true_slope * x # add noise y = true_regression_line + np.random.normal(scale=.5, size=size) data = dict(x=x, y=y) . fig = plt.figure(figsize=(7, 7)) ax = fig.add_subplot(111, xlabel=&#39;x&#39;, ylabel=&#39;y&#39;, title=&#39;Generated data and underlying model&#39;) ax.plot(x, y, &#39;x&#39;, label=&#39;sampled data&#39;) ax.plot(x, true_regression_line, label=&#39;true regression line&#39;, lw=2.) plt.legend(loc=0); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; with Model() as model: # model specifications in PyMC3 are wrapped in a with-statement # Define priors sigma = HalfCauchy(&#39;sigma&#39;, beta=10, testval=1.) intercept = Normal(&#39;Intercept&#39;, 0, sigma=20) x_coeff = Normal(&#39;x&#39;, 0, sigma=20) y_est = intercept + x_coeff * x # Define likelihood likelihood = Normal(&#39;y&#39;, mu=y_est, sigma=sigma, observed=y) # Inference! trace = sample(3000, cores=2) # draw 3000 posterior samples using NUTS sampling . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (2 chains in 2 jobs) NUTS: [x, Intercept, sigma] Sampling 2 chains, 0 divergences: 100%|██████████| 7000/7000 [00:04&lt;00:00, 1515.25draws/s] . plt.figure(figsize=(7, 7)) traceplot(trace[100:]) plt.tight_layout(); . /anaconda3/envs/PyMC3/lib/python3.7/site-packages/arviz/plots/backends/matplotlib/distplot.py:38: UserWarning: Argument backend_kwargs has not effect in matplotlib.plot_distSupplied value won&#39;t be used &#34;Argument backend_kwargs has not effect in matplotlib.plot_dist&#34; /anaconda3/envs/PyMC3/lib/python3.7/site-packages/arviz/plots/backends/matplotlib/distplot.py:38: UserWarning: Argument backend_kwargs has not effect in matplotlib.plot_distSupplied value won&#39;t be used &#34;Argument backend_kwargs has not effect in matplotlib.plot_dist&#34; /Users/cooke_c/.local/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: This figure was using constrained_layout==True, but that is incompatible with subplots_adjust and or tight_layout: setting constrained_layout==False. This is separate from the ipykernel package so we can avoid doing imports until . &lt;Figure size 504x504 with 0 Axes&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; plt.figure(figsize=(7, 7)) plt.plot(x, y, &#39;x&#39;, label=&#39;data&#39;) plot_posterior_predictive_glm(trace, samples=100, label=&#39;posterior predictive regression lines&#39;) plt.plot(x, true_regression_line, label=&#39;true regression line&#39;, lw=3., c=&#39;y&#39;) plt.title(&#39;Posterior predictive regression lines&#39;) plt.legend(loc=0) plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;); . KeyError Traceback (most recent call last) &lt;ipython-input-6-de1d4a7bd649&gt; in &lt;module&gt; 2 plt.plot(x, y, &#39;x&#39;, label=&#39;data&#39;) 3 plot_posterior_predictive_glm(trace, samples=100, -&gt; 4 label=&#39;posterior predictive regression lines&#39;) 5 6 plt.plot(x, true_regression_line, label=&#39;true regression line&#39;, lw=3., c=&#39;y&#39;) /anaconda3/envs/PyMC3/lib/python3.7/site-packages/pymc3/plots/posteriorplot.py in plot_posterior_predictive_glm(trace, eval, lm, samples, **kwargs) 35 for rand_loc in np.random.randint(0, len(trace), samples): 36 rand_sample = trace[rand_loc] &gt; 37 plt.plot(eval, lm(eval, rand_sample), **kwargs) 38 # Make sure to not plot label multiple times 39 kwargs.pop(&#39;label&#39;, None) /anaconda3/envs/PyMC3/lib/python3.7/site-packages/pymc3/plots/posteriorplot.py in &lt;lambda&gt;(x, sample) 22 &#34;&#34;&#34; 23 if lm is None: &gt; 24 lm = lambda x, sample: sample[&#39;Intercept&#39;] + sample[&#39;x&#39;] * x 25 26 if eval is None: KeyError: &#39;Intercept&#39; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; #plt.bar(age_brackets,members_f_numbers,label=&#39;Female&#39;,alpha=0.5,width=0.95,color=&#39;#A60628&#39;) #plt.bar(age_brackets,members_m_numbers,label=&#39;Male&#39;,alpha=0.5,width=0.95,color=&#39;#348ABD&#39;) . begin{equation*} x = begin{bmatrix} a b 1 end{bmatrix} end{equation*} begin{equation*} K = begin{bmatrix} f &amp; c_{x} 0 &amp; 1 end{bmatrix} end{equation*} begin{equation*} R = begin{bmatrix} R_1 &amp; R_2 R_3 &amp; R_4 end{bmatrix} end{equation*} begin{equation*} P = KR[I|−C] end{equation*} begin{equation*} C = begin{bmatrix} -C_X -C_Y end{bmatrix} end{equation*} from IPython.display import set_matplotlib_formats %matplotlib inline #set_matplotlib_formats(&#39;svg&#39;) plt.scatter(0,0) for i in range(0,5): x = 5 * (np.random.random()-0.5) y = 5 * (np.random.random()-0.5) + 10 plt.plot([0,x],[0,y],color=&#39;#348ABD&#39;) plt.scatter(x,y,color=&#39;#348ABD&#39;) plt.xlim(-5,5) plt.ylim(-1,15) plt.plot() .",
            "url": "https://cgcooke.github.io/Blog/bayesian/computer%20vision/2020/04/03/Bayesian-Camera-Resectioning.html",
            "relUrl": "/bayesian/computer%20vision/2020/04/03/Bayesian-Camera-Resectioning.html",
            "date": " • Apr 3, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Exodus",
            "content": "There are something like 500,000 international students in Australia, and in the words of Australian Prime minister, Scott Morrison, it&#39;s time for them to go home. . Some of these students are fortunate, and have secured employment in roles where they can continue to work, despite the economic and social impacts of COV19. Many are less fortunate. . Unfortunately for many of these students, returning home is easier said than done. Globally, airlines are grounding their fleets and countries are closing their borders. . While the numbers change on a day to day basis, as I write, the earliest flight from Australia to India is on Sunday the 12th of April, in 9 days time. It costs $1,939 USD ($3,497.23 AUD) and is an epic 58 hours, providing a regional geo-political tour, taking you from Melbourne New Delhi, via Doha, Islamabad and Riyadh. (For geopolitical reasons, you can&#39;t fly from Doha to Riyadh, or from Islamabad to New Delhi). . While I&#39;m not certain of what the future holds, let&#39;s imagine a possible future, and consider what an exodus of Indian and Nepalese students would look like. . Let&#39;s start with the numbers, there are 153,537 Indians and K Nepalese students currently in Australia, on (X) type visas. . Let&#39;s think at a high level for how we can place these numbers in context. 372,000 Indians Indians visited Australia circa 2019. Let&#39;s assume that roughly the same number of Australians visit India. From this we can assume that there are approximately 700,000 seat&#39;s p/a available to/from India, or 350,000 seats available in each direction. This estimate isn&#39;t that far from the truth, with total bi-directional capacity of 676,000. . From this, we can see that under normal operating conditions, assuming that there were no other passengers, it would take ±3.5 months to completely evacuate 100,000 people from Australia to India using the available commercial capacity. . Surprisingly, and exodus of this scale isn&#39;t without precedent in recent history. Take for example the airlift of Indians from Kuait, where 111,711 people were airlifted on 488 flights over 63 days. . . import numpy as np import matplotlib.pyplot as plt %matplotlib inline from IPython.display import set_matplotlib_formats set_matplotlib_formats(&#39;svg&#39;) plt.rcParams[&#39;figure.figsize&#39;] = [20, 20] from pymc3 import * . WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named &#39;mkl&#39; . begin{equation*} x = begin{bmatrix} a b 1 end{bmatrix} end{equation*} begin{equation*} K = begin{bmatrix} f &amp; c_{x} 0 &amp; 1 end{bmatrix} end{equation*} begin{equation*} R = begin{bmatrix} R_1 &amp; R_2 R_3 &amp; R_4 end{bmatrix} end{equation*} begin{equation*} P = KR[I|−C] end{equation*} begin{equation*} C = begin{bmatrix} -C_X -C_Y end{bmatrix} end{equation*}",
            "url": "https://cgcooke.github.io/Blog/aviation/2020/04/03/An-Exodus.html",
            "relUrl": "/aviation/2020/04/03/An-Exodus.html",
            "date": " • Apr 3, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Adventure in Camera Calibration",
            "content": "It&#39;s Febrary 1972, the A300 airliner is being unviled in Toulouse, let&#39;s go on an adventure (In camera calibration!). . . Let&#39;s keep things interesting, and pretend that we work for an aircraft manufacturer, Norton Aircraft, headquartered in Burbank, California. Let&#39;s say we have seen this photo published in a magazine, and we want to try and learn as much about the dimensions of Airbus&#39;s new aircraft as possible. In order to do so, we will need to mathematically reconstruct the camera used to take the photo, as well as the scene itself. . Now, In this case, we are lucky, because we notice the hexagonal pattern on the floor. In particular, we notice that it&#39;s a tessellating hexagonal pattern, which can only happen if all the hexagons have identical dimensions. . While we don&#39;t know the dimensions of the hexagon, we guess that each side is approximately 1.6m long, based on the high of the people in the photo. If we assume some point on the ground, say the center of a polygon is the point 0,0, we can work out the X &amp; Y location of each other polygon vertex we can see. Furthermore, we could also assume that the factory floor is flat and level. Hence the Z coordinate of each point is 0. . Let&#39;s spend ±5 minutes annotating the image, using an annotation tool like label me. I&#39;ve generated a file, which you can find attached here: . . Firstly, lets load in all of the x and y points: . import json import numpy as np JSON = json.loads(open(&#39;Data/2020-2-23-An-Adventure-In-Camera-Calibration/A300.json&#39;,&#39;r&#39;).read()) polygons = {} for shape in JSON[&#39;shapes&#39;]: coords = shape[&#39;label&#39;].split(&#39;,&#39;) x,y = int(coords[0]),int(coords[1]) polygons[x,y] = shape[&#39;points&#39;] . Ok, now doing some maths, and work out the locations of each vertex of our hexagons. . from sklearn.neighbors import KDTree points = [] keys = sorted(polygons.keys()) for key in keys: poly = polygons[key] (pts_x, pts_y) = zip(*poly) pts_x = list(pts_x) pts_y = list(pts_y) #Magic analytic formula for working out the location of each point, based on which vertex, of which polygon it is. x_vertex = 0.5 * np.array([1,2,1,-1,-2,-1]) y_vertex = 0.5 * np.array([np.sqrt(3),0,-np.sqrt(3),-np.sqrt(3),0,np.sqrt(3)]) row,col = key x = row * 1.5 + x_vertex y = col * 0.5 * np.sqrt(3) + y_vertex #From before, we assume the sides of each polygon is 1.6m x*=1.6 #meters y*=1.6 #meters for idx in range(0,6): point = [] i = pts_x[idx] j = pts_y[idx] X = x[idx] Y = y[idx] Z = 0.0 points.append([i,j,X,Y,Z]) . Now we are presented with a minor problem, in many cases, we have annotated the same point up to 3 times, where the vertices of the hexagons meet. So let&#39;s go and find points that are within 10 pixels, and then take their average. If we don&#39;t do this, then we effectively over-weight some points in the image, at the expense of others. . points = np.asarray(points) tree = KDTree(points[:,0:2], leaf_size=5) merged_indicies = [] unique_points = [] for i in range(0,points.shape[0]): if i not in merged_indicies: dist, ind = tree.query(points[i,0:2].reshape(-1, 2), k=3) indicies_to_merge = [] for j in range(0,3): if dist[0][j]&lt;10: indicies_to_merge.append(ind[0][j]) merged_indicies.append(ind[0][j]) mean_points = np.mean(points[indicies_to_merge,:],axis=0) unique_points.append(mean_points) unique_points = np.asarray(unique_points) . So, now we have a bunch of 3D points, and corresponding 2D points in the photo. . Now it&#39;s time to turn to the real magic, bundle adjustment. Basically, our task at hand, is to find a camera, which best fits the data we have measured. . Let&#39;s talk more about cameras. . Important: There are many correct ways to model a camera mathematically. This is one way. . Mathematically, cameras are are composed of two types of parameters, Intrinsic and Extrinsic. The Extrinsic parameters define the position and rotation of the camera, with respect to the origin of the points it&#39;s observing. . The Intrinsic parameters define the parameters of the camera itself, for example the Focal length, the location of the camera&#39;s radial center, as well as distortion induced by the lens. . The Extrinisic parameters are comprised of 6 degrees of freedom, given our world is 3 dimensional, and there are 3 dimensions which to rotate around. . The Intrinsic parameters are more complex. There are a number of great resources, for example Multiple View Geometry in Computer Vision, or the OpenCV documentation. However, In this case, I am assuming that the principal point, the focal length, and the radial parameters are unknown. . Note: To be clear, I&#8217;m building on the shoulders of giants, I&#8217;ve heavily adapted this example from this incredible demo by Nikolay Mayorov which you can find here . Firstly, let&#39;s go and import a bunch of stuff we will need later. . from __future__ import print_function . import numpy as np import matplotlib.pyplot as plt from scipy.optimize import least_squares from scipy.spatial.transform import Rotation as Rot %matplotlib inline plt.rcParams[&quot;figure.figsize&quot;] = (20,20) . points_2d = unique_points[:,0:2] points_3d = unique_points[:,2:5] print(&#39;We have {} unique points&#39;.format(points_2d.shape[0])) . We have 51 unique points . Let&#39;s start by providing some hints to the optimiser about what the solution could be like, by putting in some reasonable starting conditions. . We know both the image width and height, and we can assume that the principal point is in the center of the image. . I think the cameras is about 10 meters off the ground. . To make the task of optimization easier, lets rotate the camera so that it&#39;s facing directly down. This means that the points should be in front of/below it. . Let&#39;s also start off by assuming that the camera is centered above the points. It&#39;s obviously not correct, based on what we can see in the image, but it&#39;s not horrifically wrong. . image_width = 2251 image_height = 1508 estimated_focal_length_px = 2000 camera_params = np.zeros(12) r = Rot.from_euler(&#39;x&#39;, 180, degrees=True).as_rotvec() #Rotation matrix camera_params[0] = r[0] camera_params[1] = r[1] camera_params[2] = r[2] #C camera_params[3] = points_3d[:,0].mean() camera_params[4] = points_3d[:,1].mean() camera_params[5] = 10 #f,k1,k2, camera_params[6] = estimated_focal_length_px camera_params[7] = 0 camera_params[8] = 0 camera_params[9] = 0 #c_x,c_y camera_params[10] = image_width/2.0 camera_params[11] = image_height/2.0 . Now we come to the real magic. This function models the camera, taking points in 3D space, and converting them into points in 2D space. . There are lots of things going on here. . . Firstly, let&#39;s talk about the camera&#39;s intrinsic matrix. . Basically, it converts points from 3D space to 2D space. . begin{equation*} K = begin{bmatrix} f &amp; 0 &amp; c_{x} 0 &amp; f &amp; c_{y} 0 &amp; 0 &amp; 1 end{bmatrix} end{equation*}We have the focal length, $f$, and the camera optical center $c_x$ and $c_y$. . Now let&#39;s talk about the camera&#39;s extrinsic matrix. . These are the 6 degrees of freedom that describe it&#39;s position and orientation within the world. That&#39;s 3 degrees for the position, and 3 for the orientation. At its heart, what we are doing is simple, but confusing. . There are so many ways to represent our setup: . Coordinate systems: 2D and 3D. Left Handed or Right Handed?     | . | Rotations: . Quaternions? | Proper Euler angles (6 different ways)? | Tait–Bryan angles (6 different ways)? | Rodrigues rotation formula? | A rotation matrix? | . | The location of the camera in to the world. (2 Different ways). . | Today we are going to use two different ways to represent the rotations, Firstly a Rodrigues rotation vector representation, and a rotation matrix. The reason why we use two different representations is because it&#39;s easier to optimise when we have 3 degrees of freedom, rather than a naive rotation matrix which uses 9 numbers to represent 3 degrees of freedom. . R represents the orientation of the camera in the World Coordinate Frame (The frame which we use to describe our 3D points). . In python, we can use convert from the Rodrigues rotation vector to the Rotation matrix as follows: . from scipy.spatial.transform import Rotation as Rot rotation_vector = camera_params[:3] R = Rot.from_rotvec(rotation_vector).as_matrix() . begin{equation*} R = begin{bmatrix} R_1 &amp; R_2 &amp; R_3 R_4 &amp; R_5 &amp; R_6 R_7 &amp; R_8 &amp; R_9 end{bmatrix} end{equation*} Now, let&#39;s talk about the Project Matrix $P$ of the camera. This takes the points all the way from their location in 3D world coordinates, to pixel coordinates, assuming we have a camera without radial distortion. There are two main ways this could be formulated. . Firstly: begin{equation*} P = KR[I|−C] end{equation*} . Secondly: begin{equation*} P = K[R | t] end{equation*} . Where $t$ is: begin{equation*} t = −RC end{equation*} . Let&#39;s go with the first method, where C is : . begin{equation*} C = begin{bmatrix} -C_X -C_Y -C_Z end{bmatrix} end{equation*} However, there is one subtlety alluded to before, which is the impact of radial distortion. Simply, the camera&#39;s lens distorts the rays of light coming in, in a non-linear way. . We can model it using a Taylor series: . begin{equation*} x_c = x(1 + k_1 r^2 + k_2 r^4 + k_3 r^6) end{equation*} begin{equation*} y_c = y(1 + k_1 r^2 + k_2 r^4 + k_3 r^6) end{equation*} In python, we end up with: . r = np.sqrt(np.sum(points_proj**2, axis=1))) r = 1 + k1 times r**2 + k2 * r**4 + k3 * r**6 points_proj *= r[:, np.newaxis] . def project(points, camera_params): &quot;&quot;&quot;Convert 3-D points to 2-D by projecting onto images.&quot;&quot;&quot; #Rotation rotation_vector = camera_params[:3] R = Rot.from_rotvec(rotation_vector).as_matrix() #Camera Center C = camera_params[3:6].reshape(3,1) IC = np.hstack([np.eye(3),-C]) RIC = np.matmul(R,IC) #Make points Homogeneous points = np.hstack([points,np.ones((points.shape[0],1))]) #Perform Rotation and Translation #(n,k), (k,m) -&gt; (n,m) points_proj = np.matmul(points,RIC.T) #perspective divide points_proj = points_proj[:, :2] / points_proj[:, 2, np.newaxis] f = camera_params[6] k1 = camera_params[7] k2 = camera_params[8] k3 = camera_params[9] c_x = camera_params[10] c_y = camera_params[11] #Radial distortion r = np.sqrt(np.sum(points_proj**2, axis=1)) x = points_proj[:,0] y = points_proj[:,1] points_proj[:,0] = (1 + k1 * r**2 + k2 * r**4 + k3 * r**6)*x points_proj[:,1] = (1 + k1 * r**2 + k2 * r**4 + k3 * r**6)*y #Make points Homogeneous points_proj = np.hstack([points_proj, np.ones((points_proj.shape[0],1))]) K = np.asarray([[f, 0, c_x], [0, f, c_y], [0, 0, 1.0]]) points_proj = np.dot(points_proj,K.T) points_proj = points_proj[:,:2] return(points_proj) . This section below is really well explained by here. Basically, we are optimizing to minimise a geometric error. It&#39;s the distance between the 2D points we see, and the projection of their 3D counterparts. . Through a process of optimization, we aim to find parameters which result in low error, which means in turn they should represent the real parameters of the camera. . def fun(camera_params, points_2d, points_3d): #Compute residuals. points_proj = project(points_3d, camera_params) return(points_proj - points_2d).ravel() . x0 = camera_params.ravel() optimization_results = least_squares(fun, x0, verbose=1, x_scale=&#39;jac&#39;, ftol=1e-4, method=&#39;lm&#39;, loss=&#39;linear&#39;,args=(points_2d, points_3d)) . `ftol` termination condition is satisfied. Function evaluations 617, initial cost 3.7406e+07, final cost 1.7290e+02, first-order optimality 5.34e+01. . Now let&#39;s go and check out the results of our optimization process. . camera_params = optimization_results.x R_Rodrigues = camera_params[0:3] C = camera_params[3:6] r = Rot.from_rotvec(R_Rodrigues) R_matrix = r.as_matrix() r = Rot.from_matrix(R_matrix.T) R_Quaternion = r.as_quat() print(&#39;Quaternions: X: {:.3f} Y: {:.3f} Z: {:.3f} W: {:.3f} &#39;.format(R_Quaternion[0],R_Quaternion[1],R_Quaternion[2],R_Quaternion[3])) print(&#39;Camera position relative to the origin in (M): X: {:.2f}, Y: {:.2f}, Z: {:.2f}&#39;.format(C[0],C[1],C[2])) focal_length_px = camera_params[6] k1 = camera_params[7] k2 = camera_params[8] k3 = camera_params[9] c_x = camera_params[10] c_y = camera_params[11] print(&#39;Focal length (Pixels): {:.2f}&#39;.format(focal_length_px)) print(&#39;CX, CY: {:.2f} {:.2f}&#39;.format(c_x,c_y)) print(&#39;K_1, K_2, K_3 : {:.6f}, {:.6f}, {:.6f}&#39;.format(k1,k2,k3)) print(&#39;Mean error per point: {:.2f} pixels &#39;.format(optimization_results.cost/points_2d.shape[0])) . Quaternions: X: 0.839 Y: -0.382 Z: 0.174 W: -0.345 Camera position relative to the origin in (M): X: -8.20, Y: -14.01, Z: 5.40 Focal length (Pixels): 1598.01 CX, CY: 1056.75 2153.95 K_1, K_2, K_3 : -0.086846, 0.195150, -0.128968 Mean error per point: 3.39 pixels . Ok, first things first, the mean error per point is approximately 3.6 pixels, which is not great, not terrible. It&#39;s clear that we have found a decent solution, However there are some interesting things going on. . In particular, the principal point lies outside the image, which is curious to say the least. One possibility is that the image was cropped. . Now let&#39;s have a quick look at the errors for each point. . plt.hist(abs(optimization_results.fun),density=True) plt.title(&#39;Histogram of Residuals&#39;) plt.xlabel(&#39;Absolute Residual (Pixels)&#39;) plt.grid() plt.show() . So the histogram looks pretty good, apart from the one point with a high residual, which is probably due to sloppy labeling/annotation. . Now let&#39;s compare the points we annotated, with where they would be projected, using the camera parameters we found: . points_2d_proj = project(points_3d, optimization_results.x) img = plt.imread(&#39;data/2020-2-23-An-Adventure-In-Camera-Calibration/A300.jpg&#39;) plt.imshow(img) plt.scatter(points_2d[:,0],points_2d[:,1],label=&#39;Actual&#39;,c=&#39;r&#39;,alpha=0.5) plt.scatter(points_2d_proj[:,0],points_2d[:,1],label=&#39;Optimised&#39;,c=&#39;k&#39;,alpha=0.5) plt.show() . Again, this looks great. Finally, let&#39;s overlay the hexagons on the floor, to visually build confidence in our solution. . def plot_verticies(row,col): x_vertex = 0.5 * np.array([1,2,1,-1,-2,-1,1]) y_vertex = 0.5 * np.array([np.sqrt(3),0,-np.sqrt(3),-np.sqrt(3),0,np.sqrt(3),np.sqrt(3)]) x = row * 1.5 + x_vertex y = col * 0.5 * np.sqrt(3) + y_vertex x*=1.6 y*=1.6 points_3d = np.vstack([x,y,np.zeros(7)]).T points_2d_proj = project(points_3d, optimization_results.x) return(points_2d_proj) plt.imshow(img) for row in range(0,10,2): for col in range(0,10,2): points_2d_proj = plot_verticies(row,col) plt.plot(points_2d_proj[:,0],points_2d_proj[:,1],color=&#39;B&#39;,alpha=0.25) plt.text(np.mean(points_2d_proj[:,0]), np.mean(points_2d_proj[:,1]), str(row)+&#39;,&#39;+str(col), horizontalalignment=&#39;center&#39;,verticalalignment=&#39;center&#39;) for row in range(1,11,2): for col in range(1,11,2): points_2d_proj = plot_verticies(row,col) plt.plot(points_2d_proj[:,0],points_2d_proj[:,1],color=&#39;R&#39;,alpha=0.25) plt.text(np.mean(points_2d_proj[:,0]), np.mean(points_2d_proj[:,1]), str(row)+&#39;,&#39;+str(col), horizontalalignment=&#39;center&#39;,verticalalignment=&#39;center&#39;) plt.show() . /Users/cooke_c/.local/lib/python3.7/site-packages/IPython/core/pylabtools.py:128: MatplotlibDeprecationWarning: Support for uppercase single-letter colors is deprecated since Matplotlib 3.1 and will be removed in 3.3; please use lowercase instead. fig.canvas.print_figure(bytes_io, **kw) . Awesome, we can see visually that we have found a reasonable solution . Thanks for reading all the way to the end, . In the future, Let&#39;s look more about how we can extract useful information from this image, and understand how confident we can be in our solution. . Thanks to Nikolay Mayorov who created the awesome demo of optimization in Scipy that I built upon, please find the original code here. . Multiple View Geometry in Computer Vision is an incredible book, that I learn more from, each time I read it. in particular, for further information see: . Finite cameras. Page 153, Multiple View Geometry in Computer Vision (Second edition) | Note: Minimizing geometric error. Page 176, Multiple View Geometry in Computer Vision (Second edition) | .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/optimization/linear%20algebra/2020/04/03/An-Adventure-In-Camera-Calibration.html",
            "relUrl": "/computer%20vision/optimization/linear%20algebra/2020/04/03/An-Adventure-In-Camera-Calibration.html",
            "date": " • Apr 3, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Title",
            "content": "%matplotlib inline import pymc3 as pm import matplotlib.pyplot as plt import numpy as np import theano plt.rcParams[&quot;figure.figsize&quot;] = (10,10) . WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named &#39;mkl&#39; . with pm.Model() as model: merchant_revenue = 2.185 * 10**9 total_late_fees = 28.4 * 10**6 avg_late_fee = pm.Uniform(&#39;avg_late_fee&#39;,lower=10,upper=68) avg_transaction_size = pm.Normal(&#39;avg_transaction_size&#39;,mu=150,sigma=10) num_transactions = pm.Deterministic(&#39;num_transactions&#39;,(merchant_revenue / avg_transaction_size)) #loss_rate = pm.Beta(&#39;loss_rate&#39;,alpha=1,beta=3) late_fee_rate = pm.Beta(&#39;loss_rate&#39;,alpha=1,beta=3) total_late_fees = pm.Deterministic(&#39;total_loss&#39;, num_transactions * late_fee_rate * avg_late_fee) #total_loss = pm.Deterministic(&#39;total_loss&#39;, num_transactions * loss_rate * avg_transaction_size) #total_rev = pm.Deterministic(&#39;total_rev&#39;,num_transactions * avg_transaction_size) obs_1 = pm.Normal(&#39;total_late_fees&#39;, mu=total_loss, sigma=10**5, observed=[total_late_fees]) #obs_2 = pm.Normal(&#39;merchant_revenue_obs&#39;, mu=total_rev, sigma=10**7, observed=[merchant_revenue]) #obs_3 = pm.Exponential(&#39;avg_transaction_size_obs&#39;,lam = 1/avg_transaction_size, observed=[150]) . NameError Traceback (most recent call last) &lt;ipython-input-2-b39b732e4909&gt; in &lt;module&gt; 20 21 &gt; 22 obs_1 = pm.Normal(&#39;total_late_fees&#39;, mu=total_loss, sigma=10**5, observed=[total_late_fees]) 23 24 #obs_2 = pm.Normal(&#39;merchant_revenue_obs&#39;, mu=total_rev, sigma=10**7, observed=[merchant_revenue]) NameError: name &#39;total_loss&#39; is not defined . with model: trace = pm.sample(1_000) . pm.traceplot(trace); . pm.plot_posterior(trace); . pm.model_to_graphviz(model) .",
            "url": "https://cgcooke.github.io/Blog/2020/04/03/APT.html",
            "relUrl": "/2020/04/03/APT.html",
            "date": " • Apr 3, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Inverse Radial Distortion",
            "content": "summary . from __future__ import print_function . import time import numpy as np import pandas as pd import matplotlib.pyplot as plt from scipy.optimize import least_squares from scipy.sparse import lil_matrix from scipy.spatial.transform import Rotation as Rot %matplotlib inline plt.rcParams[&quot;figure.figsize&quot;] = (20,20) . k1 = -0.0443637091577041 k2 = -0.35894053523797786 k3 = 0.14944647701164057 r = np.linspace(0,1,1000) r_distorted = r*(1 + k1 * r + k2 * r**2 + k3 * r**3) plt.xlabel(&#39;Initial R&#39;) plt.ylabel(&#39;Distorted R&#39;) plt.plot(r,r_distorted) plt.show() . . def distort_point(distortion_params,r_distorted): undistorted = r_distorted*(1 + distortion_params[0] * r_distorted + distortion_params[1] * r_distorted**2 + distortion_params[2] * r_distorted**3 + distortion_params[3] * r_distorted**4 + distortion_params[4] * r_distorted**5) return(undistorted) def fun(distortion_params,r_distorted): #Compute residuals. undistorted = distort_point(distortion_params,r_distorted) return((undistorted - np.linspace(0,1,1000))).ravel() . x0 = np.zeros(5).ravel() res = least_squares(fun, x0, verbose=2, ftol=1e-12,loss=&#39;linear&#39;, args=([r_distorted])) . Iteration Total nfev Cost Cost reduction Step norm Optimality 0 1 5.6524e+00 3.02e+01 1 2 2.3534e-07 5.65e+00 9.81e-01 3.04e-08 2 3 2.3534e-07 1.81e-18 2.83e-06 1.11e-11 `gtol` termination condition is satisfied. Function evaluations 3, initial cost 5.6524e+00, final cost 2.3534e-07, first-order optimality 1.11e-11. . undistorted = distort_point(res.x,r_distorted) plt.plot(r_distorted,label=&#39;distorted&#39;) plt.plot(undistorted,label=&#39;un distorted&#39;) plt.plot(np.linspace(0,1,1000),label=&#39;target&#39;) plt.legend() plt.show() . . print(res.x) . [ 0.04598032 0.32141486 0.2211217 -0.46139385 0.77101275] .",
            "url": "https://cgcooke.github.io/Blog/computer%20vision/optimisation/2020/02/29/Inverse-Radial-Distortion.html",
            "relUrl": "/computer%20vision/optimisation/2020/02/29/Inverse-Radial-Distortion.html",
            "date": " • Feb 29, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Three Interesting Papers",
            "content": "Over the past couple of months, 3 incredibly exciting papers have come out, and I want to take the opportunity to share them with you. . The papers, in no particular order are MixMatch, Selfie and Unsupervised Data Augmentation, however let’s first discuss why they are exciting. . In my day to day work, I’m faced with an avalanche of data. Raw data may be cheap, but labelled data is precious, often relying on expensive experiments or busy experts. Even then, when labelled data is available there is an omnipresent, insatiable demand to do more with it. . Semi-supervised learning offers the opportunity to leverage the raw, unlabelled data to improve our models, reducing the barriers to building a model, and democratising AI. . I’m not going to discuss how the papers are actually implemented in detail, but I will say that the papers are very promising, and I hope they will be rapidly implemented and adapted as a standard part of deep learning workflows. . In a sentence, MixMatch uses MixUp, and label sharpening (Fancy way of saying “Artificially boosting your models confidence”) in order to effectively propagate labels. My first impression was “I can’t believe that works”, but then I saw that it decreases error rates 4x with when training with small numbers of samples on CIFAR-10. . Conversely, Selfie is inspired pertaining in BERT, and extends it to CNN’s. At a high level, the pre training task is analagous to removing pieces from a jigsaw puzzle, and asking “what piece should go in each hole?”. Given the power of transfer learning, this is hugely exciting for many problems where the data you want to train on is very different to what is found in ImageNet. . Finally, there is Unsupervised Data Augmentation (UDA), which prosecutes the thesis that “better data augmentation can lead to significantly better semi-supervised learning”. As with Selfie and MixMatch, the techniques used in this paper can be applied to image data. . Deep learning is built on a history of rapidly evolving best practice, including Xavier initialisation, Data Augmentation, One Cycle Policy and MixUp. I hope that adoptions of that MixMatch, Selfie and UDA will soon join this grab bag of best practice. .",
            "url": "https://cgcooke.github.io/Blog/deep%20learning/computer%20vision/2019/06/05/Three-Interesting-Papers.html",
            "relUrl": "/deep%20learning/computer%20vision/2019/06/05/Three-Interesting-Papers.html",
            "date": " • Jun 5, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "The Differentiable Airliner",
            "content": "It’s hard to conceive of something more carefully designed than an aircraft. Millions of parts, Hundreds of kilometers of wiring, all designed with by thousands of engineers. . What I find particularly fascinating however, is that the core design principles of an aircraft can be elicited through a relatively simple thought experiment, . At a steady state in cruise, we can make two assumptions. Firstly, the lift (L) generated by wings equals the weight (W) of the aircraft, secondly the thrust (T) generated by the engines equals the drag (D). Obviously these two cases are true, otherwise the aircraft would be accelerating either vertically or horizontally. . Thrust is generated by burning fuel in turbofan engine, and minimising fuel consumption is a key design goal. Reducing fuel consumption directly reduces the direct operating cost of an aircraft, while simultaneously increasing it’s potential range. . L=WL=WL=W . T=DT=DT=D . From this, we can also derive: . LD=WT frac{L}{D} = frac{W}{T}DL​=TW​ . T=WDLT = W frac{D}{L}T=WLD​ . So, from this we can see that we can reduce the thrust required, by . Reducing the aircraft weight. . | Reducing the drag. . | Improving the lift to drag ratio. . | Lets look at each of these aspects in a little more detail, . Reducing the aircraft weight . One of the current trends is the increasing use of composite materials in the design of aircraft. The higher the strength to weight ratio, allowing the same structural outcomes to be achieved, at a lower weight. . From this, we can see that fuel consumption is intricately linked to both the percentage of the aircraft which can be constructed from composite materials, as well as their strength. . One other important consideration is mission optimisation. An aircraft which is capable of long range flight needs to be engineered to structurally support the weight of the fuel required for the flight. If the aircraft operates a shorter route, then additional structure, and it’s associated weight increases fuel consumption, compared to an aircraft designed for shorter routes. . Reducing the drag . Fundamentally there are two sources of drag, Parasitic drag, and Lift induced drag. . Parasitic drag is a byproduct of the motion of the aircraft through the air, . Conversely, Lift induced drag is a byproduct of generating lift using the wings of the aircraft. . Together they sum to form total drag: . DTotal=DParasitic+DLiftInducedD_{Total} = D_{Parasitic } + D_{Lift Induced}DTotal​=DParasitic​+DLiftInduced​ . Minimising the Parasitic drag in turn reduces the total drag, minimising fuel burn. . Improving the lift to drag ratio. . Finally, maximising the lift to drag ratio, effectively means that aircraft wing can generate the required amount of lift, while minimising drag. . To provide a practical worked example, . Taking the case of an A330, weighing 200,000 kg, with an L/D ratio of approximately 18, then: . DLiftInduced=2×106×9.8118D_{Lift Induced} = frac{2 times10^{6} times 9.81}{18}DLiftInduced​=182×106×9.81​ . If the L/D ratio could be doubled, then the thrust, and fuel burn required to support the weight of the aircraft would halve. . Unconstrained Optimisation: The Virgin Atlantic GlobalFlyer . Optimising purely for minimum thrust, and thus fuel consumption, we end up with an aircraft with a fuselage made entirely out of composite materials, with minimal drag, and long slender wings, in order to maximise the L/D ratio. This is the design used by the Virgin Atlantic GlobalFlyer, with a L/D ratio of 37, which holds the FAI record for longest flight by an aircraft ever at 41,466 km. . Obviously the GlobalFlyer represents optimisation towards a singular goal, without consideration for other goals, like passenger comfort, however that doesn’t stop me loving it for it’s pure design. . .",
            "url": "https://cgcooke.github.io/Blog/aerospace/2019/03/30/The-Differentiable-Airliner.html",
            "relUrl": "/aerospace/2019/03/30/The-Differentiable-Airliner.html",
            "date": " • Mar 30, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "Kalman Filters",
            "content": "Kalman Filters are magic. While they take 5 minutes to explain at a basic level, you can work with them for a career and always be learning more. I think there is something philosophically satisfying about the way that they innately combine what we already believe and what we perceive in order to come to a new belief about the world. . While this sounds somewhat abstract, Kalman Filters provide a concrete mathematical formulation for fusing data from different sources, as well as physical models to provide (potentially) optimum estimates of the state of a system. . For less philosophy, and more maths, I strongly recommend stopping at this point and giving this incredible post a read. Afterwards, let’s talk about Kalman filters in a concrete way, . Creating a Kalman Filter in 7 easy steps: . One of the challenges with Kalman filters is that it’s easy to be initially overwhelmed by the mathematical background, and loose sight of what their implementation looks like in practice. In reality, it’s possible to break the implementation down into a series of discrete steps, which come together to fully describe the filter. . FilterPy is a fantastic Python library for creating Kalman filters, and has an accompanying book, which takes a deep dive into the mathematical theory of Kalman filters. Lets initially discuss the general process for defining a Kalman filter, before applying it to practical application. . x : Our filter state estimate, IE what we want to estimate. If we want to track an object moving an a video, this could be it’s pixel coordinates, as well as it’s velocity in pixels per second. [x,y,vx,vy] . | P : The covariance matrix. Encodes how certain the filter is about it’s estimates, evolves over time. In the object tracking example, how “confident” the filter is about the position of an object and it’s velocity. As the filter receives more measurements, the values in the covariance matrix are “washed out”, and so the the filter tends to be insensitive to the values used. . | Q : The process uncertainty. How large is the error associated with the system doing something unexpected between measurements? I find this the hardest to set, as it requires careful thought about the process. For example, if we are tracking the position and velocity of an object once a second, we would have more uncertainty if we were tracking the position of a fruit fly than an oil tanker. . | R : How uncertain each of our measurements are. This can be determined either through reading sensor datasheets or educated guesses. . | H : How to each measurement is related to the internal state of our system, in addition to scaling measurements. IE, If we have a GPS receiver, it tells us about our position, while an accelerometer tells us about our position. . | F : The state transition matrix. How the system evolves over time. IE, if we know the position and velocity of an object, then in the absence of any error or external influence we can predict it’s next position from it’s current position and velocity. . | B : The control matrix. This matrix allows us to tell the filter about how we expect any inputs we provide the system (u) to update the state of the system. In many cases, especially when we are taking measurements of a system we don’t control, the control matrix is not required. . | At this point, I think it’s worthwhile considering how all of these matrices are related to each other. Tim Babb of Bzarg, has a fantastic diagram, which sets out how information flows through all of the filters mentioned above. If you haven’t already, I strongly recommend you read his post on how Kalman filters work . Looking at the relationships between all of the matrices, . x and P are outputs of the filter, they tell what the filter believes the state of the system to be. . | H, F and B are matrices which control how the filter operates. . | Q, R are closely related, because they both denote uncertainty, in the process as well as the measurements. . | z and u denote inputs to the filter, in the case where we don’t control the system, then u is not required. . | A real world example: . Let’s look at a real world example. In computer vision, object tracking is the process of associating different detections of an object from different images/frames into a single “track”. Many algorithms have been developed for this task (Simple Online and Realtime Tracking)[https://arxiv.org/abs/1602.00763] is particularly elegant. In summary, SORT creates a Kalman filter for each object it wants to track, and then predicts the location and size of each object, in each frame using the filter. . Alex Bewley, one of the creators of SORT has developed a fantastic (implementation)[https://github.com/abewley/sort] of SORT, which uses Filterpy. . Let’s take a look at his implementation, through the lens of what I’ve discussed above: . Quickly defining some nomenclature, . u and v are the x and y pixel coordinates of the center of the bounding box around an object being tracked. . | s and r are tha scale and aspect ratio of the bounding box surrounding the object. . | u˙,v˙ dot u, dot vu˙,v˙ are the x and y velocity of the bounding box. . | s˙ dot ss˙ is the rate at which the scale of the bounding box is changing. . | . kf = KalmanFilter(dim_x=7, dim_z=4) . Our internal state is 7 dimensional: . [u,v,s,r,u˙,v˙,s˙][u, v, s, r, dot u, dot v , dot s][u,v,s,r,u˙,v˙,s˙] . While our input vector is 4 dimensional: . [u,v,s,r][u, v, s, r][u,v,s,r] . kf.F = np.array([[1,0,0,0,1,0,0], [0,1,0,0,0,1,0], [0,0,1,0,0,0,1], [0,0,0,1,0,0,0], [0,0,0,0,1,0,0], [0,0,0,0,0,1,0], [0,0,0,0,0,0,1]]) . The state transition matrix tells us that at each timestep, we update our state as follows: . u=u+u˙u = u + dot uu=u+u˙ . v=v+v˙v = v + dot vv=v+v˙ . s=s+s˙s = s + dot ss=s+s˙ . kf.H = np.array([[1,0,0,0,0,0,0], [0,1,0,0,0,0,0], [0,0,1,0,0,0,0], [0,0,0,1,0,0,0]]) . The sensor matrix tells us that we are directly measuring [u,v,s,r][u, v, s, r][u,v,s,r]. . kf.R = np.array([[ 1, 0, 0, 0], [ 0, 1, 0, 0,], [ 0, 0, 10, 0,], [ 0, 0, 0, 10,]]) . The sensor noise matrix tells us that we can measure uuu and vvv with a much higher degree of certainty than sss and rrr. . kf.P = np.array([[ 10, 0, 0, 0, 0, 0, 0], [ 0, 10, 0, 0, 0, 0, 0], [ 0, 0, 10, 0, 0, 0, 0], [ 0, 0, 0, 10, 0, 0, 0], [ 0, 0, 0, 0, 10000, 0, 0], [ 0, 0, 0, 0, 0, 10000, 0], [ 0, 0, 0, 0, 0, 0, 10000]]) . The covariance matrix tells us that the filter should have a very high initial uncertinty for each of the velocity components. . kf.Q = np.array([[1.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00] [0.e+00, 1.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00] [0.e+00, 0.e+00, 1.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00] [0.e+00, 0.e+00, 0.e+00, 1.e+00, 0.e+00, 0.e+00, 0.e+00] [0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-02, 0.e+00, 0.e+00] [0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-02, 0.e+00] [0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 1.e-04]]) . The Process uncertainty matrix tells us how much “uncertainty” there is in each component of the systems behaviour. . Filterpy has a function which can be very useful for generating Q. . filterpy.common.Q_discrete_white_noise . Further reading . Control theory is a broad an intellectually stimulating area, with broad applications. Brian Douglas has an incredible YouTube channel which I strongly recommend. .",
            "url": "https://cgcooke.github.io/Blog/bayesian/2019/03/23/Kalman-Filters.html",
            "relUrl": "/bayesian/2019/03/23/Kalman-Filters.html",
            "date": " • Mar 23, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "Scattered Interpolation",
            "content": "Geospatial data often comes in the forms of samples taken at different points, when this occours a common next step is interpolation. An example of this is Airbourne LiDAR, where a laser range finder is scanned over the ground underneath the aircraft. The produces a cloud of scattered points, and often the next step is build a Digital Elevation Model (DEM) from these scattered points. There are a number of different options for interpolation in python, the correct choice of method is often task specific, so its good to have some options at your disposal. . import numpy as np import math import os from PIL import Image import gdal import matplotlib.pyplot as plt from scipy import interpolate . The Data . def downloadDEMFromCGIAR(lat,lon): &#39;&#39;&#39; Download a DEM from CGIAR FTP repository &#39;&#39;&#39; fileName = lonLatToFileName(lon,lat)+&#39;.zip&#39; &#39;&#39;&#39; Check to see if we have already downloaded the file &#39;&#39;&#39; if fileName not in os.listdir(&#39;.&#39;): os.system(&#39;&#39;&#39;wget --user=data_public --password=&#39;GDdci&#39; http://data.cgiar-csi.org/srtm/tiles/GeoTIFF/&#39;&#39;&#39;+fileName) os.system(&#39;unzip &#39;+fileName) def lonLatToFileName(lon,lat): &#39;&#39;&#39; Compute the input file name &#39;&#39;&#39; tileX = int(math.ceil((lon+180)/5.0)) tileY = -1*int(math.ceil((lat-65)/5.0)) inputFileName = &#39;srtm_&#39;+str(tileX).zfill(2)+&#39;_&#39;+str(tileY).zfill(2) return(inputFileName) lon,lat = -155.48647818,19.6662251 inputFileName = lonLatToFileName(lon,lat) #downloadDEMFromCGIAR(lat,lon) utmZone = int((math.floor((lon + 180)/6) % 60) + 1) &#39;&#39;&#39; Check to see if file is in northern or southern hemisphere &#39;&#39;&#39; if lat&lt;0: EPSGCode = &#39;EPSG:327&#39;+str(utmZone) else: EPSGCode = &#39;EPSG:326&#39;+str(utmZone) !! gdalwarp -q -te -155.75 19.25 -155.25 19.75 -srcnodata -32768 -dstnodata 0 srtm_05_09.tif subset.tif os.system(&#39;gdalwarp -q -t_srs &#39;+EPSGCode+&#39; -tr 100 -100 -r cubic subset.tif warped.tif&#39;) . data = np.asarray(Image.open(&#39;warped.tif&#39;)) plt.imshow(data) plt.colorbar() plt.show() y_range = data.shape[0] x_range = data.shape[1] num_samples = 10**5 xArray = np.zeros(num_samples) yArray = np.zeros(num_samples) heightArray = np.zeros(num_samples) for sample_num in range(0,num_samples): x_idx = np.random.randint(0,x_range) y_idx = np.random.randint(0,y_range) xArray[sample_num] = x_idx yArray[sample_num] = y_idx heightArray[sample_num] = data[y_idx,x_idx] . plt.scatter(xArray,yArray,c=heightArray,alpha=0.75) plt.show() . Interpolation . Ultimately, there is no universally correct choice of interpolation method to use. However it is useful to know the mechanics associated with going from scattered points to an interpolated array for a number of different methods. . numIndexes = 500 xi = np.linspace(np.min(xArray), np.max(xArray),numIndexes) yi = np.linspace(np.min(yArray), np.max(yArray),numIndexes) . One option is linear interpolation, which can be performed using scipy.interpolate. . XI, YI = np.meshgrid(xi, yi) points = np.vstack((xArray,yArray)).T values = np.asarray(heightArray) points = np.asarray(points) DEM = interpolate.griddata(points, values, (XI,YI), method=&#39;linear&#39;) . Contour plots . Contour plots are useful for visualzing Digital Elevation Models, Our first step is to create the contour lines, with a 25m contour interval as follows : We can map the array to an image using imshow, an alternative is to use plt.contourf, to “fill in” the contours. . levels = np.arange(0,4000,100) plt.contour(DEM, levels,linewidths=0.2,colors=&#39;k&#39;) plt.imshow(DEM,origin=&#39;lower&#39;) plt.colorbar() plt.show() .",
            "url": "https://cgcooke.github.io/Blog/remote%20sensing/2018/11/18/Scattered-Interpolation.html",
            "relUrl": "/remote%20sensing/2018/11/18/Scattered-Interpolation.html",
            "date": " • Nov 18, 2018"
        }
        
    
  
    
        ,"post11": {
            "title": "256 Shades Of Grey",
            "content": "On February 22, 2000, after 11 days of measurements, the most comprehensive map ever created of the earth’s topography was complete. The space shuttle Endeavor had just completed the Shuttle Radar Topography Mission, using a specialised radar to image the earths surface. . The Digital Elevation Map (DEM) produced by this mission is in the public domain and provides the measured terrain high at ~90 meter resolution. The mission mapped 99.98% of the area between 60 degrees North and 56 degrees South. . In this post, I will examine how to process the raw DEM so it is more intuitively interpreted, through the use of hillshading,slopeshading &amp; hypsometric tinting. . The process of transforming the raw GeoTIFF into the final imagery product is simple. Much of the grunt work being carried out by GDAL, the Geospatial Data Abstraction Library. . In order, we need to: . Download a DEM as a GeoTIFF | Extract a subsection of the GeoTIFF | Reproject the subsection | Make an image by hillshading | Make an image by coloring the subsection according to altitude | Make an image by coloring the subsection according to slope | Combine the 3 images into a final composite | DEM . Several different DEM’s have been created from the data collected on the SRTM mission, in this post I will use the CGIAR SRTM 90m Digital Elevation Database. Data is provided in 5x5 degree tiles, with each degree of latitude equal to approximately 111Km. . Our first task is to acquire a tile. Tiles can be downloaded from http://data.cgiar-csi.org/srtm/tiles/GeoTIFF/ using wget. . import os import math from PIL import Image, ImageChops, ImageEnhance from matplotlib import cm . def downloadDEMFromCGIAR(lat,lon): &#39;&#39;&#39; Download a DEM from CGIAR FTP repository &#39;&#39;&#39; fileName = lonLatToFileName(lon,lat)+&#39;.zip&#39; &#39;&#39;&#39; Check to see if we have already downloaded the file &#39;&#39;&#39; if fileName not in os.listdir(&#39;.&#39;): os.system(&#39;&#39;&#39;wget --user=data_public --password=&#39;GDdci&#39; http://data.cgiar-csi.org/srtm/tiles/GeoTIFF/&#39;&#39;&#39;+fileName) os.system(&#39;unzip &#39;+fileName) . def lonLatToFileName(lon,lat): &#39;&#39;&#39; Compute the input file name &#39;&#39;&#39; tileX = int(math.ceil((lon+180)/5.0)) tileY = -1*int(math.ceil((lat-65)/5.0)) inputFileName = &#39;srtm_&#39;+str(tileX).zfill(2)+&#39;_&#39;+str(tileY).zfill(2) return(inputFileName) . lon,lat = -123,49 inputFileName = lonLatToFileName(lon,lat) downloadDEMFromCGIAR(lat,lon) . Slicing . The area I have selected covers Washington State and British Columbia, with file name srtm_12_03.tif. . Let’s use GDAL to extract a subsection of the tile.The subsection covers Vancouver Island and the Pacific Ranges stretching from 125ºW - 122ºW &amp; 48ºN - 50ºN. Using gdalwarp: . !! gdalwarp -q -te -125 48 -122 50 -srcnodata -32768 -dstnodata 0 srtm_12_03.tif subset.tif . Our next step is to transform the subsection of the tile to a different projection. The of the points in the subsection are located on a grid 1/1200th of a degree apart. While degrees of latitude are always ~110Km in size, resulting in ~92.5M resolution, degrees of longitude decrease in size, from ~111Km at the equator to 0Km at the poles. A different scale exists between the latitude &amp; longitude axis and a longitude scale that depends on the latitude. . A solution is to project that points so that there is a consistent and equal scale in the X/Y plane. One choice is to use a family of projections called Universal Transverse Mercator. Each UTM projection can map points from longitude &amp; latitude to X &amp; Y coordinates in meters. The UTM projection is useful because it locally preserves both shapes and distances, over a distances of up to several hundred kilometres. . The tradeoff is that several different UTM projections are required for different points on earth, 120 to be precise. Fortunately it is relatively trivial to work out the required projection based on the longitude and latitude. Almost every conceivable projection has been assigned a code by the European Petroleum Survey Group (EPSG). This EPSG code can be used to unambiguously specify the projection being used. With UTM, each code starts with either 327 or 326, depending on the hemisphere of the projection. . utmZone = int((math.floor((lon + 180)/6) % 60) + 1) &#39;&#39;&#39; Check to see if file is in northern or southern hemisphere &#39;&#39;&#39; if lat&lt;0: EPSGCode = &#39;EPSG:327&#39;+str(utmZone) else: EPSGCode = &#39;EPSG:326&#39;+str(utmZone) . Once we have identified the correct EPSG code to use, the process of warping the subset to a new projection is relatively straightforward. . In the following system call to gdalwarp, t_srs denotes the target projection, and tr specifies the resolution in the X and Y plane. The Y resolution is negative because the in the GDAL file uses a row, column based coordinate system. . In this coordinate system, the origin is in the top left hand corner of the file. The row value increases as you move down the file, like an excel spreadsheet, however the UTM Y coordinate decreases. This results in the negative sign in the resolution. . os.system(&#39;gdalwarp -q -t_srs &#39;+EPSGCode+&#39; -tr 100 -100 -r cubic subset.tif warped.tif&#39;) . Hillshading . At this point we can begin to visualise the DEM. One highly effective method is hillshading, which models the way the surface of the DEM would be illuminated by light projected onto it. Shading of the slopes allows the DEM to be more intuitively interpreted than just coloring by height alone. . . !! gdaldem hillshade -q -az 45 -alt 45 warped.tif hillshade.tif . Hypsometric Tinting . Hillshading can also be combined with height information to aid interpretation of the topography. The technical name for the process of coloring a DEM based on height is hypsometric tinting. The process is simple, with GDAL mapping colors to cell heights, using a provided color scheme. . . def createColorMapLUT(minHeight,maxHeight,cmap = cm.YlGn_r,numSteps=256): &#39;&#39;&#39; Create a colormap for visualisation &#39;&#39;&#39; f =open(&#39;color_relief.txt&#39;,&#39;w&#39;) f.write(&#39;-0.1,135,206,250 n&#39;) f.write(&#39;0.1,135,206,250 n&#39;) for i in range(0,numSteps): r,g,b,a= cmap(i/float(numSteps)) height = minHeight + (maxHeight-minHeight)*(i/numSteps) f.write(str(height)+&#39;,&#39;+str(int(255*r))+&#39;,&#39;+str(int(255*g))+&#39;,&#39;+str(int(255*b))+&#39; n&#39;) f.write(str(-1)+&#39;,&#39;+str(int(255*r))+&#39;,&#39;+str(int(255*g))+&#39;,&#39;+str(int(255*b))+&#39; n&#39;) createColorMapLUT(minHeight=10,maxHeight=2658) . !! gdaldem color-relief -q warped.tif color_relief.txt color_relief.tif . Slope Shading . Another technique for visualizing terrain is slopeshading. While hypsometric tinting assigns colors to cells based on elevation, slope shading assigns colors to pixels based on the slope (0º to 90º). In this case, white (255,255,255) is assigned to slopes of 0º and black (0,0,0) is assigned to slopes of 90º, with varying shades of grey for slopes in-between. . . This color scheme is encoded in a txt file for gdaldem as follows: . f = open(&#39;color_slope.txt&#39;,&#39;w&#39;) f.write(&#39;0 255 255 255 n&#39;) f.write(&#39;90 0 0 0 n&#39;) f.close() . The computation of the slope shaded dem takes place over two steps. . The slope of each cell is computed | A shade of grey is assigned to each cell depending on the slope. | !! gdaldem slope -q warped.tif slope.tif !! gdaldem color-relief -q slope.tif color_slope.txt slopeshade.tif . Layer Merging . The final step in producing the final product is to merge the 3 different created images. The python Image Library (PIL) is a quick and dirty way to accomplish this task, with the 3 layers are merged using pixel by pixel multiplication. . One important detail to note is that the pixel by pixel multiplication occurs in the RGB space. From a theoretical perspective, it’s probably better that each pixel is first transformed to the Hue, Saturation, Value (HSV) color space, and the value is then multiplied by the hillshade and slope shade value, before being transformed back into the RGB color space. In practical terms however, the RGB space multiplication is a very reasonable approximation. . In one final tweak, the brightness of the output image is increased by 40%, to offset the average reduction in brightness caused by multiplying the layers together. . . &#39;&#39;&#39; Merge components using Python Image Lib &#39;&#39;&#39; slopeshade = Image.open(&quot;slopeshade.tif&quot;).convert(&#39;L&#39;) hillshade = Image.open(&quot;hillshade.tif&quot;) colorRelief = Image.open(&quot;color_relief.tif&quot;) #Lets just fill in any gaps in the hillshading ref = Image.new(&#39;L&#39;, slopeshade.size,180) hillshade = ImageChops.lighter(hillshade,ref) shading = ImageChops.multiply(slopeshade, hillshade).convert(&#39;RGB&#39;) merged = ImageChops.multiply(shading,colorRelief) &#39;&#39;&#39; Adjust the brightness to take into account the reduction caused by hillshading&#39;&#39;&#39; enhancer = ImageEnhance.Brightness(merged) img_enhanced = enhancer.enhance(1.4) img_enhanced.save(&#39;Merged.png&#39;) . Further reading . I found the following sources to be invaluable in compiling this post: . Creating color relief and slope shading | A workflow for creating beautiful relief shaded DEMs using gdal | Shaded relief map in python | Stamen Design | .",
            "url": "https://cgcooke.github.io/Blog/remote%20sensing/2018/11/18/256-Shades-of-Grey.html",
            "relUrl": "/remote%20sensing/2018/11/18/256-Shades-of-Grey.html",
            "date": " • Nov 18, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Cameron, . I’m passionate about AI (Computer Vision in particular), Robotics and the future of Aerospace. . I’m writing a blog because the first step to truly understanding something is being able to explain it to someone else. . Plus I get excited about the incredible world around us, and can’t wait to tell everyone about it! . Until next time. . Cameron . .",
          "url": "https://cgcooke.github.io/Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}